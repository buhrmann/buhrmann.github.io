<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>datawerk</title><link>https://synergenz.github.io/</link><description></description><atom:link href="https://synergenz.github.io/feeds/thomas-buhrmann.rss.xml" rel="self"></atom:link><lastBuildDate>Thu, 23 Oct 2014 13:19:01 +0200</lastBuildDate><item><title>Interest rates on P2P loans</title><link>https://synergenz.github.io/p2p-loans.html</link><description>&lt;p&gt;Like other peer-to-peer services, the Lending Club [1] aims to directly connect producers and consumers, or in this case borrowers and lenders, by cutting out the middleman. Borrowers apply for loans online and provide details about the desired loan as well their financial status (such as &lt;span class="caps"&gt;FICO&lt;/span&gt; score). Lenders use the information provided to choose which loans to invest in. The Lending Club, finally, uses a proprietary algorithm to determine the interest charged on an applicant’s loan [2]. Given the secret nature of this process, a borrower or lender might be interested in which variables, beside the obvious &lt;span class="caps"&gt;FICO&lt;/span&gt; credit score, influence the final interest rate and how strong this influence is. It is the aim of this analysis to identify such&amp;nbsp;associations.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/p2ploans/lendingclub.jpg" alt="Fico score analysis"/&gt;
&lt;/figure&gt;

&lt;h3&gt;Methods&lt;/h3&gt;
&lt;p&gt;Data about peer-to-peer loans issued through the Lending Club was provided by the Data Analysis class on Coursera [3]. The set used in this analysis was downloaded on the 17th of February, 2013. After accounting for missing values in the data, an exploratory analysis was performed to identify variables that required transformation prior to statistical modeling (boxplots, histograms etc.), and to find a subset of variables to be used in a regression model relating interest rate to an applicant’s &lt;span class="caps"&gt;FICO&lt;/span&gt; score (using correlation analysis and &lt;span class="caps"&gt;PCA&lt;/span&gt;). The statistical model itself was a simple linear multivariate regression [4]. Since most variables were not normally distributed, results were also compared to robust estimation&amp;nbsp;techniques.&lt;/p&gt;
&lt;p&gt;To reproduce the results of this report the complementary R script (provided on github) can be run with the corresponding data&amp;nbsp;file.&lt;/p&gt;
&lt;h3&gt;Summary of&amp;nbsp;data&lt;/h3&gt;
&lt;p&gt;Besides a loan’s interest rate, the data set analyzed here contains information about the amount requested and the amount eventually funded by investors (1000$-35000$), the length of the loan (36 or 60 months), and its purpose (the majority went towards debt consolidation or to pay off credit cards). Information about the applicant included his or her duration of employment, state of residence, information about home ownership (the majority either renting or having a mortgage), debt to income ratio, monthly income, &lt;span class="caps"&gt;FICO&lt;/span&gt; score, number of open credit lines, revolving credit balance, and the number of inquiries made in the last 6 months. In total the set included information about 2500 loans, out of which 79 contained missing data (most in employment length). Given this relatively small number, and the relatively large set, the corresponding data was simply&amp;nbsp;discarded.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;FICO&lt;/span&gt; scores were transformed from a 38-level factor (the lowest being [640-644] and the highest [830-834]) to mean values for each range. The number of recent loan inquiries showed a Poisson- or exponential-like distribution. Since the kinds of analyses performed on the data —such as linear regression—might be sensitive to data not being normally distributed, we created a new factor variable with only two levels: 0 inquiries (1213 data points) and 1-9 inquiries (1208 data&amp;nbsp;points).&lt;/p&gt;
&lt;p&gt;Histograms of quantitative variables indicated that the distributions of monthly incomes, &lt;span class="caps"&gt;FICO&lt;/span&gt; score, revolving credit balance and number of open credit lines were not normal, and more specifically right-skewed to different degrees. While log10 transformation generally brought the distributions closer to normal (at least visually), the results of a Shapiro test [5] indicated that the null-hypothesis of normal distribution still needed to be rejected. Inspection of normal &lt;span class="caps"&gt;QQ&lt;/span&gt;-plots, however, indicated fairly normal distributions across a wide range centered at the means of most variables. While removal of outliers (e.g. monthly incomes greater than 4 or 5 standard deviations from mean) improved normality, in the remaining analysis the whole data set was&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;The histogram of the interest rate variable showed a bimodal distribution, suggestive of a superposition of two separate distributions. While none of the factors separated these, when interest rate histograms were plotted for different ranges of the &lt;span class="caps"&gt;FICO&lt;/span&gt; variable (by cutting the latter at quantiles) normal-like distributions could be identified. The shift in interest rate mean as a function of &lt;span class="caps"&gt;FICO&lt;/span&gt; score indicates that for very high &lt;span class="caps"&gt;FICO&lt;/span&gt; scores, relatively low interest rates become disproportionally more&amp;nbsp;probable.&lt;/p&gt;
&lt;h3&gt;Exploratory&amp;nbsp;analysis&lt;/h3&gt;
&lt;p&gt;Since intuitively we know that interest rate should correlate strongly with &lt;span class="caps"&gt;FICO&lt;/span&gt; scores, we examined the associations between the two, as well as those between either and third&amp;nbsp;variables.&lt;/p&gt;
&lt;p&gt;Box plots of interest rate and &lt;span class="caps"&gt;FICO&lt;/span&gt; scores by factor variables showed that only two factors seemed to influence these variables. Loan length had a significant effect on interest rate (p-value of t-test &amp;lt; 2.2e-16, effect size of 4.24%), but not on &lt;span class="caps"&gt;FICO&lt;/span&gt; scores (p-value=0.41). The number of inquiries (two-level factor) had significant effects on the means of both variables (p &amp;lt; 2.2e-16 and p = 1.6e-5), but the effect size was interesting only in the case of interest rate&amp;nbsp;(1.6%).&lt;/p&gt;
&lt;p&gt;Using their correlation matrix, as well as pair-wise scatter plots with linear models fitted, we aimed to reduce the set of quantitative variables by discarding those (except interest rate and &lt;span class="caps"&gt;FICO&lt;/span&gt; score) that showed high correlations amongst&amp;nbsp;themselves.&lt;/p&gt;
&lt;p&gt;First we eliminated the amount funded by investors. This is justified as a) we are interested here only in the resulting interest rate, not the size of the eventual loans, and b) for most applications the eventual loan equalled the amount requested, i.e. a strong linear relationship (with slope 1) existed between the two (linear regression resulted in adjusted coefficient of determination R2=0.94, i.e. approx. 94% variance&amp;nbsp;explained).&lt;/p&gt;
&lt;p&gt;The correlation matrix further showed an association between monthly income and loan amount requested (Pearson correlation R = 0.47; linear regression with adj. R^2 = 0.23 and p &amp;lt; 2.2e-16). This is not surprising as we would expect people with higher incomes to be able to afford larger loans. To avoid confounders we also rejected monthly income as an independent variable in further&amp;nbsp;models.&lt;/p&gt;
&lt;p&gt;Of the remaining four covariates (excluding &lt;span class="caps"&gt;FICO&lt;/span&gt;), three are related to the applicants current debt. In particular, correlation analysis revealed that the number of open credit lines correlates (relatively weakly) with both debt to income ratio (R = 0.38) and revolving credit balance (0.34). We therefore use only the first to stand in for the overall debt&amp;nbsp;burden.&lt;/p&gt;
&lt;p&gt;In summary, we consider as quantitative covariates in the statistical model only the amount requested, open credit lines and the &lt;span class="caps"&gt;FICO&lt;/span&gt; score. A quick &lt;span class="caps"&gt;PCA&lt;/span&gt; analysis (visually via R’s biplot function) confirms that these are indeed relatively independent. The variable for open credit lines is aligned mostly with the first principal component, &lt;span class="caps"&gt;FICO&lt;/span&gt; score with the second, and the amount requested falls in between the other two (i.e. they are not orthogonal, but far from parallel in the space of the principal&amp;nbsp;components).&lt;/p&gt;
&lt;p&gt;The exploratory analysis can be summarised by the following relationships between interest rate and retained&amp;nbsp;covariates:&lt;/p&gt;
&lt;p&gt;Factors:&lt;ul&gt;
&lt;li&gt;The longer the loan, the higher the&amp;nbsp;interest.&lt;/li&gt;
&lt;li&gt;The more often an applicant has recently inquired about a loan, the higher its&amp;nbsp;interest.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
&lt;p&gt;Quantitative:&lt;ul&gt;
&lt;li&gt;The larger the loan, the higher its&amp;nbsp;interest.&lt;/li&gt;
&lt;li&gt;The smaller the &lt;span class="caps"&gt;FICO&lt;/span&gt; score, the higher the&amp;nbsp;interest.&lt;/li&gt;
&lt;li&gt;The higher the number of open credit lines, the higher the&amp;nbsp;interest.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
&lt;p&gt;In the following section we aim to quantify these associations in more detail using statistical&amp;nbsp;modeling.&lt;/p&gt;
&lt;h3&gt;Statistical&amp;nbsp;Modeling&lt;/h3&gt;
&lt;p&gt;As a first test, a simple linear regression was performed relating interest rate to log10- transformed &lt;span class="caps"&gt;FICO&lt;/span&gt; scores only, as these two variables exhibited the greatest correlation (R=0.71). Analysis of the residuals showed non-random patterns as a function of both the requested amount and loan length, but not the number of open credit lines or number of recent inquiries. We therefore chose to include the first two as potential confounders in a more complicated&amp;nbsp;model:&lt;/p&gt;
&lt;script type="math/tex; mode=display"&gt;
IR = LL_{36} + b_1 log_{10}(FICO) + b_2(Amount) + b_3(LL_{60}) + e
&lt;/script&gt;

&lt;p&gt;where &lt;span class="caps"&gt;IR&lt;/span&gt; is the interest rate; the intercept &lt;span class="caps"&gt;LL&lt;/span&gt;&lt;sub&gt;36&lt;/sub&gt; corresponds to the estimated mean of interest rates for loans over a 36 months period given a &lt;span class="caps"&gt;FICO&lt;/span&gt; score of 1 (log&lt;sub&gt;10&lt;/sub&gt;(1)=0); b&lt;sub&gt;1&lt;/sub&gt; represents the change in interest rate associated with a change of 1 unit in log&lt;sub&gt;10&lt;/sub&gt; &lt;span class="caps"&gt;FICO&lt;/span&gt; score for loans of the same amount; b&lt;sub&gt;2&lt;/sub&gt; captures the change in interest rate as a function of the loan amount requested; b&lt;sub&gt;3&lt;/sub&gt; the increase in interest rate of loans lasting 60 rather than 36 months; and e are unmodelled random variations. A scatterplot matrix illustrates the relationship between these covariates (Figure 1, left&amp;nbsp;panel).&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/p2ploans/fico-figure.jpg" alt="Fico score analysis"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 1: Covariates used in final regression model (left panel) and its residuals (right panel). Left: scatter plot matrix of covariates color-coded according to loan length (gray: 36 months; black: 60 months). Longer loans tend to have higher interest rates and correspond to larger loans. Green lines indicate univariate regression lines, supporting the observed trends. Above the diagonal, Pearson correlations are shown. Besides the association between interest rate and &lt;span class="caps"&gt;FICO&lt;/span&gt; score (here log10-transformed) as well as amount requested, it can be seen that the latter two are not associated with each other (R=0.091). Right: residuals after fitting a univariate regression using only &lt;span class="caps"&gt;FICO&lt;/span&gt; score (top row), and residuals when using the full model (bottom row). In the left column residuals are color-coded with respect to the levels of the loan length factor, and in the right column according to four different levels of the loan amount variable. The non-random patterns of the univariate model vanish in the full model.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As can be be seen in the right panel of Figure 1, the full model largely removes the patterns in residuals observed in the simple model. Further analysis also revealed that the residuals were approximately normal in distribution. Whereas the simple model accounted for only 50.65% of the variance in the data, the full model captured 75.18% (R^2=0.75, the significance of associations being p &amp;lt; 2.2e-16). The added covariates did not however change the sign nor much the value of the resulting coefficients (&lt;span class="caps"&gt;LL&lt;/span&gt;&lt;sub&gt;36&lt;/sub&gt;=427.3, b&lt;sub&gt;1&lt;/sub&gt;=-146.2, b&lt;sub&gt;2&lt;/sub&gt;=0.00014, b&lt;sub&gt;3&lt;/sub&gt;=3.3). We also checked whether inclusion of the number of open credit lines or the number of recent inquiries in the model would improve the result even further, but the change in explained variance was small (+2.2%). Allowing for interactions between covariates in the full model also did not result in better fit. Since the distributions of most variables in the data set were not perfectly normal even after transforming (see histograms in Figure 1), we also tested whether non-linear and robust forms of regression would perform better. But neither a generalized linear model (glm in R [5]) nor a robust regression using an M-estimator (rlm in R [6]) produced significantly different&amp;nbsp;coefficients.&lt;/p&gt;
&lt;p&gt;In the final model, a change of one unit in log&lt;sub&gt;10&lt;/sub&gt; &lt;span class="caps"&gt;FICO&lt;/span&gt; score corresponds to a change of -146.2% (95% confidence interval: -142,-152) in interest rate over the base rate of 427.3% (&lt;span class="caps"&gt;CI&lt;/span&gt;: 416, 438). So for example, the interest rate for a 10000$ loan over 36 months would be 12.1% for the average reported &lt;span class="caps"&gt;FICO&lt;/span&gt; score of 707, and would decrease by 0.89% for an additional 10 points of the &lt;span class="caps"&gt;FICO&lt;/span&gt; score. An increase in the size of the loan by 1000$ corresponds to an increase of 0.14% in interest rate (&lt;span class="caps"&gt;CI&lt;/span&gt; of b&lt;sub&gt;2&lt;/sub&gt;: 0.00013, 0.00015). Increasing the length of the loan to 60 months would result in an additional 3.3% of interest (&lt;span class="caps"&gt;CI&lt;/span&gt;: 3.1,&amp;nbsp;3.52).&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;Our results show a significant negative association between interest rates and &lt;span class="caps"&gt;FICO&lt;/span&gt; score, modulated by positive associations with loan amount and duration. Due to the log&lt;sub&gt;10&lt;/sub&gt; transformation, model estimates are non-linear with respect to &lt;span class="caps"&gt;FICO&lt;/span&gt; score. Though this makes interpretation of the model rather unintuitive, the associations are in the direction one would&amp;nbsp;expect.&lt;/p&gt;
&lt;p&gt;It should be kept in mind that the analysis only applies to the peer-to-peer loans issues through the Lending Club. Loans in general, e.g. those offered by banks, might follow a different pattern. While the model presented here would allow interested individuals to get an idea of what interest rates to expect given a desired loan and credit history, further work would be needed if accurate predictions are required (e.g. when evaluating what kind of loans to include in a lender’s portfolio). Also, the data analyzed here does not serve to verify whether peer-to-peer loans provided through the Lending Club are indeed cheaper than those offered by banks, as the website&amp;nbsp;claims.&lt;/p&gt;
&lt;p&gt;As noted, none of the variables in the data set were normally distributed, something that could not be remedied by log transformation (or indeed other transforms, such as square roots etc.). It is possible that other non-linear or robust methods would have been more appropriate. Nevertheless, residuals after linear regression, according to histograms and &lt;span class="caps"&gt;QQ&lt;/span&gt;-plots, were approximately normal, indicating that a linear regression might not have been totally&amp;nbsp;unwarranted.&lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ol class="bib"&gt;
&lt;li&gt;&lt;a hre="https://www.lendingclub.com/home.action"&gt;Lending&amp;nbsp;Club.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.lendingclub.com/public/how-we-set-interest-rates.action"&gt;Lending Club interest rate&amp;nbsp;determination.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spark-public.s3.amazonaws.com/dataanalysis/loansData.rda"&gt;Lending Club loans&amp;nbsp;data.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bishop, &lt;span class="caps"&gt;C. M.&lt;/span&gt;(2006). Pattern recognition and machine learning (Vol. 4, No. 4). New York:&amp;nbsp;Springer.&lt;/li&gt;
&lt;li&gt;Wood, Simon (2006). Generalized Additive Models: An Introduction with R. Chapman &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Hall/&lt;span class="caps"&gt;CRC&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;J. Huber (1981) Robust Statistics.&amp;nbsp;Wiley.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;/script&gt;
&lt;script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Buhrmann</dc:creator><pubDate>Thu, 23 Oct 2014 13:19:01 +0200</pubDate><guid>tag:synergenz.github.io,2014-10-23:p2p-loans.html</guid><category>R</category><category>report</category><category>regression</category></item><item><title>Titanic survival prediction</title><link>https://synergenz.github.io/titanic-survival.html</link><description>&lt;p&gt;In this report I will provide an overview of my solution to &lt;a href="http://www.kaggle.com"&gt;kaggle&amp;#8217;s&lt;/a&gt; &lt;a href="https://www.kaggle.com/c/titanic-gettingStarted"&gt;&amp;#8220;Titanic&amp;#8221; competition&lt;/a&gt;. The aim of this competition is to predict the survival of passengers aboard the titanic using information such as a passenger&amp;#8217;s gender, age or socio-economic  status. I will explain my data munging process, explore the available predictor variables, and compare a number of different classification algorithms in terms of their prediction performance. All analysis presented here was performed in R. The corresponding source code is available on &lt;a href="https://github.com/synergenz/kaggle/tree/master/titanic"&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/titanic/titanic.jpg" alt="Titanic"/&gt;
&lt;/figure&gt;

&lt;h3&gt;Data&amp;nbsp;munging&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://www.kaggle.com/c/titanic-gettingStarted/data"&gt;data set&lt;/a&gt; provided by kaggle contains 1309 records of passengers aboard the titanic at the time it sunk. Each record contains 11 variables describing the corresponding person: survival (yes/no), class (1 = Upper, 2 = Middle, 3 = Lower), name, gender and age; the number of siblings and spouses aboard, the number of parents and children aboard, the ticket number, the fare paid, a cabin number, and the port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton). Of the 1309 records 1068 include the label, thus constituting the training set, while a different subset of size 418 does not include the label and is used by kaggle for assessing the accuracy of the predictions&amp;nbsp;submitted.&lt;/p&gt;
&lt;p&gt;To facilitate the training of classifiers for the prediction of survival, and for purposes of presentation, the data was preprocessed in the following way. All categorical variables were treated as factors (ordered where appropriate, e.g. in the case of class). From each passenger&amp;#8217;s name her title was extracted and added as a new predictor&amp;nbsp;variable. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data&lt;span class="o"&gt;$&lt;/span&gt;title &lt;span class="o"&gt;=&lt;/span&gt; sapply&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;name&lt;span class="p"&gt;,&lt;/span&gt; FUN&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; strsplit&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;,&lt;/span&gt; split&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;[,.]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
data&lt;span class="o"&gt;$&lt;/span&gt;title &lt;span class="o"&gt;=&lt;/span&gt; sub&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; data&lt;span class="o"&gt;$&lt;/span&gt;title&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This resulted in a factor with a great number of different levels, many of which could be considered similar in terms of implied societal status. To simplify matters the following levels were combined: &amp;#8216;Mme&amp;#8217;, &amp;#8216;Mlle&amp;#8217;, &amp;#8216;Ms&amp;#8217; were re-assigned to the level &amp;#8216;Miss&amp;#8217;; &amp;#8216;Capt&amp;#8217;, &amp;#8216;Col&amp;#8217;, &amp;#8216;Don&amp;#8217;, &amp;#8216;Major&amp;#8217;, &amp;#8216;Sir&amp;#8217; and &amp;#8216;Dr&amp;#8217; as titles of male nobility to the level &amp;#8216;Sir&amp;#8217;; and &amp;#8216;Dona&amp;#8217;, &amp;#8216;Lady&amp;#8217;, &amp;#8216;the Countess&amp;#8217; and &amp;#8216;Jonkheer&amp;#8217; as titles of female nobility to the level&amp;nbsp;&amp;#8216;Lady&amp;#8217;.&lt;/p&gt;
&lt;p&gt;The number of all family members aboard was combined into a single family size variable. In addition, a categorical variable was formed from this data by assigning records to three approx. equally sized levels of &amp;#8216;singles&amp;#8217;, &amp;#8216;small&amp;#8217; and &amp;#8216;big&amp;#8217; families. Also, another factor was added aimed at &lt;em&gt;uniquely&lt;/em&gt; identifying big families. To this send each passenger&amp;#8217;s surname was combined with the corresponding family size (resulting e.g. in the factor level &amp;#8220;11Sage&amp;#8221;), but such that families smaller than a certain number (n=4) were all assigned the level&amp;nbsp;&amp;#8220;small&amp;#8221;.&lt;/p&gt;
&lt;p&gt;Age information was missing for many records (about 20%). Since age can be hypothesised to correlate well with such information as a person&amp;#8217;s title (e.g. &amp;#8220;Master&amp;#8221; was used to refer politely to young children), this data was imputed using a random forest (essentially a bagged decision tree) trained to predict age from the remaining&amp;nbsp;variables:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;agefit &lt;span class="o"&gt;=&lt;/span&gt; rpart&lt;span class="p"&gt;(&lt;/span&gt;age &lt;span class="o"&gt;~&lt;/span&gt; pclass &lt;span class="o"&gt;+&lt;/span&gt; sex &lt;span class="o"&gt;+&lt;/span&gt; sibsp &lt;span class="o"&gt;+&lt;/span&gt; parch &lt;span class="o"&gt;+&lt;/span&gt; fare &lt;span class="o"&gt;+&lt;/span&gt; embarked &lt;span class="o"&gt;+&lt;/span&gt; title &lt;span class="o"&gt;+&lt;/span&gt; familysize&lt;span class="p"&gt;,&lt;/span&gt; data&lt;span class="o"&gt;=&lt;/span&gt;data&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;age&lt;span class="p"&gt;),],&lt;/span&gt; method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;anova&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
data&lt;span class="o"&gt;$&lt;/span&gt;age&lt;span class="p"&gt;[&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;age&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; predict&lt;span class="p"&gt;(&lt;/span&gt;agefit&lt;span class="p"&gt;,&lt;/span&gt; data&lt;span class="p"&gt;[&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;age&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;From the imputed age variable a factor was constructed indicating whether or not a passenger is a &amp;#8220;child&amp;#8221; (age &amp;lt;&amp;nbsp;16).&lt;/p&gt;
&lt;p&gt;The fare variable contained 18 missing values (17 fares with a value of 0 and one &lt;span class="caps"&gt;NA&lt;/span&gt;), which were imputed using a decision tree analogous to the above method for the age variable. Since this variable was far from normally distributed (which might violate some algorithm&amp;#8217;s assumptions), another factor was created splitting the fare into 3 approx. equally distributed&amp;nbsp;levels.&lt;/p&gt;
&lt;p&gt;Cabin and tickets information was sparse, i.e. missing for most passengers, and not considered for further analysis or as predictors for classification. The embarkation variable contained a single missing value, for which was substituted the majority value&amp;nbsp;(Southampton).&lt;/p&gt;
&lt;p&gt;All of the above transformations were performed on the joined train and test data, which was thereafter split again into the original two&amp;nbsp;sets.&lt;/p&gt;
&lt;p&gt;In summary, the processed data set contains the following features. 5 unordered factors: gender, port of embarkation, title, child and family id. 3 ordered factors: class, family size category, fare category. And three numerical predictors: age, fare price and family size (of which only age is approx. normal&amp;nbsp;distributed).&lt;/p&gt;
&lt;h3&gt;Data&amp;nbsp;exploration&lt;/h3&gt;
&lt;p&gt;Some &lt;a href="http://en.wikipedia.org/wiki/RMS_Titanic"&gt;background information&lt;/a&gt; about the titanic disaster might prove useful to formulate hypotheses about the type of people more probable to have survived, i.e. those more likely to have had access to lifeboats. The ship only carried enough lifeboats for slightly more than half the number of people on board (and many were launched half-full). In this respect, the most significant aspect of the rescue effort was the &amp;#8220;women and children first&amp;#8221; policy followed in the majority of life boat loadings. Additionally, those on the upper decks (i.e. those in the upper classes) had easier access to lifeboats, not the least because of closer physical proximity than the lower decks. It should thus not come as a surprise that survival was heavily skewed towards women, children and in general those of the upper&amp;nbsp;class.&lt;/p&gt;
&lt;p&gt;As a first step let&amp;#8217;s look at survival rates as a function of each factor variable in the training set, shown in Figure 1.
&lt;figure &gt;
&lt;img src="/images/titanic/facBars.png" alt="Survival vs Factors" /&gt;
&lt;img src="/images/titanic/isChildBars.png" alt="Survival vs Child"/&gt;
&lt;img src="/images/titanic/titleBars.png" alt="Survival vs Title"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 1: Proportion of survivors as a function of several categorical predictors. Blue:survived, red: perished. For the title variable, proportions are relative to each level. For the remaining variables overall proportions are displayed. &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Clearly, male passengers were at a huge disadvantage. They were about 5 times more likely to die than to survive. In contrast, female passengers were almost 3 times more likely to survive than to die. Next, while 1st class passengers were more likely to survive, chances were tilted badly against 3nd class passengers (in the 2nd class the chance was about equal). While a difference in survival rate can also be seen depending on the port of embarkation, the variable is so highly imbalanced that these differences could be spurious. In regards to family size, singles were much more likely to die than to survive. However, this balance is affected highly by the fact that of the 537 singles 411 were male and only 126 female. The gender thus confounds this family size level. When considering only non-singles we see a slight effect of larger families size leading to lower probability of survival. The fare variable essentially mirrors the class variable. Those who paid more for their ticket (and thus probably of a higher socio-economical status) are somewhat more likely to survive than to perish, while passengers with the cheapest tickets were much more probable to die. The title variable mostly confirms the earlier trends. Passengers with female titles (Lady, Miss, Mrs), as well as young passengers (Master) are more likely to survive than adult male passengers (Mr, Sir, Reverend). And amongst the male adults, those of nobility (Sir) had a better chance of survival than &amp;#8220;common&amp;#8221; travellers (Mr). A slight effect of age on survival can also be seen in the &amp;#8220;is child&amp;#8221; variable (most children survived, while most adults died), but the number of children was relatively low&amp;nbsp;overall. &lt;/p&gt;
&lt;p&gt;The numeric variables further support the trend observed in the corresponding factors, as can be seen in Figure 2 below.
&lt;figure&gt;
&lt;img src="/images/titanic/expContVar.png" alt="Numerical predictors" /&gt;
&lt;figcaption  class="capCenter"&gt;Figure 2: Survival distributions for numerical predictors (red=survived, blue=died). Left: A box plot of fair price, y axis is log-scaled. Right: density estimate of survival vs age. &lt;/figcaption&gt;
&lt;/figure&gt;
Those that survived travelled on a more expensive ticket on average than those who died. And for young children we see a peak in the probability of&amp;nbsp;survival.&lt;/p&gt;
&lt;p&gt;To develop some intuition about the importance of the different predictors and how they might be used by a classifier it may help to train a simple decision tree on the data, which is a model easy to interpret. Let&amp;#8217;s start by sticking mostly to the original predictors (not including non-normal variables converted to factors, nor engineered variables like the&amp;nbsp;title):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;dc1 &lt;span class="o"&gt;=&lt;/span&gt; rpart&lt;span class="p"&gt;(&lt;/span&gt;survived &lt;span class="o"&gt;~&lt;/span&gt; pclass &lt;span class="o"&gt;+&lt;/span&gt; sex &lt;span class="o"&gt;+&lt;/span&gt; age &lt;span class="o"&gt;+&lt;/span&gt; familysize &lt;span class="o"&gt;+&lt;/span&gt; fare &lt;span class="o"&gt;+&lt;/span&gt; embarked&lt;span class="p"&gt;,&lt;/span&gt; data&lt;span class="o"&gt;=&lt;/span&gt;train&lt;span class="p"&gt;,&lt;/span&gt; method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;class&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A tree trained on the remaining predictors is shown below in Figure&amp;nbsp;3.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/titanic/dectree1.png" alt="Decision tree 1"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 3: Decision tree predicting survival. Each node displays its survival prediction (yes=blue, no=red), the probability of belonging to each class conditioned on the node (sum to one within node), as well as the percentage of observations in each node (sum to 100% across leaves). &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The resulting decision tree should not be surprising. Without any further information (at the root node) the classifier always predicts that a passenger would not survive, which is of course correct given that 62% of all passengers died while only 38% survived. Next, the tree splits on the gender variable. For male passengers over the age of 13 the classifier predicts death, while children are more likely to survive, unless they belong to a large family. On the female branch, those belonging to the upper class are predicted to survive. Those in the third class, in contrast, are predicted to survive only when they belong to a relatively small family (size &amp;lt; 4.5) and are under the age of 36. Those older, or member of a bigger family are more probable to have died. The fare and embarkation variables are not used in the final tree. Since we already know that fare correlates strongly with class, and since embarkation is strongly imbalanced, this is not surprising. &amp;#8220;Factorised&amp;#8221; variables derived from non-uniformly distributed predictors (fare category, family size category and &amp;#8220;is child&amp;#8221;) are not required in the training of the tree, as it automatically determines the best level at which to split the&amp;nbsp;variables.&lt;/p&gt;
&lt;p&gt;How about the engineered variables of a passenger&amp;#8217;s title and familyId? One possible problem here is that these factors contain relatively many levels. Decision trees split nodes by information gain, and this measure in decision trees is biased in favour of attributes with more levels. Regular trees will therefore often produce results with those categorical variables dominating others. However, biased predictor selection can be avoided using Conditional Inference Trees (ctrees), which will be employed later when more methodologically exploring different&amp;nbsp;classifiers.&lt;/p&gt;
&lt;p&gt;As a last step, we compare the distribution of variables from the train and the test set, to avoid potential surprises arising from imbalanced splits of the data. Instead of pulling out and displaying here all tables for the categorical variables in both sets, we first use a chi-square test to single out those categorical variables whose levels are differently&amp;nbsp;distributed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;factabs &lt;span class="o"&gt;=&lt;/span&gt; lapply&lt;span class="p"&gt;(&lt;/span&gt;varnames&lt;span class="p"&gt;[&lt;/span&gt;facvars&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; data.frame&lt;span class="p"&gt;(&lt;/span&gt;cbind&lt;span class="p"&gt;(&lt;/span&gt;table&lt;span class="p"&gt;(&lt;/span&gt;train&lt;span class="p"&gt;[,&lt;/span&gt;x&lt;span class="p"&gt;]),&lt;/span&gt; table&lt;span class="p"&gt;(&lt;/span&gt;test&lt;span class="p"&gt;[,&lt;/span&gt; x&lt;span class="p"&gt;])))})&lt;/span&gt;
pvals &lt;span class="o"&gt;=&lt;/span&gt; sapply&lt;span class="p"&gt;(&lt;/span&gt;faccomp&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; chisq.test&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;p.value&lt;span class="p"&gt;)&lt;/span&gt;
faccomp&lt;span class="p"&gt;[[&lt;/span&gt;which&lt;span class="p"&gt;(&lt;/span&gt;pvals&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="m"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Only the embarkation shows a slight but apparently significant difference between the train and test set, with the difference in the proportions of people embarked in Cherbourg vs. Southhamption being slightly less pronounced in the test set (C=0.188, S=0.725 in the training set, and C=0.244, S=0.646 in the test set). Since the overall tendency is preserved we assume this difference will not affect the quality of our following predictions. Comparing five-number summaries for the numerical variables showed no further differences in distribution between the train and test&amp;nbsp;sets.&lt;/p&gt;
&lt;h3&gt;Classifier&amp;nbsp;training&lt;/h3&gt;
&lt;p&gt;I decided to use the caret package in R to train and compare a variety of different models. I should note that finding a better way to preprocess, engineer and extend the available data is often more important than small improvements gained from using a better classifier. However, I suspect that since the titanic data set is very small and consists mostly of categorical variables, and since I know of no way to collect more data on the problem (without cheating), some classifiers might in this particular case perform better than&amp;nbsp;others.&lt;/p&gt;
&lt;p&gt;The caret package provides a unified interface for training of a large number of different learning algorithms, including options for validating learners using cross-validation (and related validation techniques), which can be used simultaneously for the tuning of model-specific hyper-parameters. My overall approach will be this: first I train a number of classifiers using repeated cross-validation to estimate their prediction accuracy. Next I create ensembles of these classifiers and compare their accuracy to that of individual classifiers. Lastly, I choose the best (individual or ensemble) classifier to create predictions for the kaggle competition. Usually, I would maintain a hold out set for validation and comparison of the various hypertuned algorithms. Because the data set is already small, however, I decided to try and rely on the results from repeated cross-validation (10 folds, 10 repeats). It might nevertheless be insightful to at least compare the cross-validated metrics (using the full data set) to those measured on a holdout set, even when ultimately training the final classifier on the whole training set. We&amp;#8217;ll start by training with 20% of data reserved for the validation&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s my approach to more or less flexibly building a set of different classifiers using&amp;nbsp;caret:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;rseed &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;42&lt;/span&gt;
scorer &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;ROC&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;# &amp;#39;ROC&amp;#39; or &amp;quot;Accuracy&amp;#39;&lt;/span&gt;
summarizor &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kr"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;scorer &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; defaultSummary &lt;span class="kr"&gt;else&lt;/span&gt; twoClassSummary
selector &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;best&amp;quot;&lt;/span&gt; &lt;span class="c1"&gt;# &amp;quot;best&amp;quot; or &amp;quot;oneSE&amp;quot;&lt;/span&gt;
folds &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;
repeats &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;
pp &lt;span class="o"&gt;=&lt;/span&gt; c&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;center&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;scale&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

cvctrl &lt;span class="o"&gt;=&lt;/span&gt; trainControl&lt;span class="p"&gt;(&lt;/span&gt;method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;repeatedcv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; number&lt;span class="o"&gt;=&lt;/span&gt;folds&lt;span class="p"&gt;,&lt;/span&gt; repeats&lt;span class="o"&gt;=&lt;/span&gt;repeats&lt;span class="p"&gt;,&lt;/span&gt; p&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    summaryFunction&lt;span class="o"&gt;=&lt;/span&gt;summarizor&lt;span class="p"&gt;,&lt;/span&gt; selectionFunction&lt;span class="o"&gt;=&lt;/span&gt;selector&lt;span class="p"&gt;,&lt;/span&gt; classProbs&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k-Variable"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    savePredictions&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k-Variable"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; returnData&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k-Variable"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    index&lt;span class="o"&gt;=&lt;/span&gt;createMultiFolds&lt;span class="p"&gt;(&lt;/span&gt;trainset&lt;span class="o"&gt;$&lt;/span&gt;survived&lt;span class="p"&gt;,&lt;/span&gt; k&lt;span class="o"&gt;=&lt;/span&gt;folds&lt;span class="p"&gt;,&lt;/span&gt; times&lt;span class="o"&gt;=&lt;/span&gt;repeats&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;First, use a random seed to make results repeatable! Next we select whether to optimise prediction accuracy or the area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve, and the number of folds for cross-validation and the number of times to repeat the validation. Some algorithms require normalised data, which means centering and scaling here. Lastly, setup the training control structure expected by the caret package. Next we set up a number of formulas to be used by the&amp;nbsp;classifiers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;fmla0 &lt;span class="o"&gt;=&lt;/span&gt; survived &lt;span class="o"&gt;~&lt;/span&gt; pclass &lt;span class="o"&gt;+&lt;/span&gt; sex &lt;span class="o"&gt;+&lt;/span&gt; age
fmla1 &lt;span class="o"&gt;=&lt;/span&gt; survived &lt;span class="o"&gt;~&lt;/span&gt; pclass &lt;span class="o"&gt;+&lt;/span&gt; sex &lt;span class="o"&gt;+&lt;/span&gt; age &lt;span class="o"&gt;+&lt;/span&gt; fare &lt;span class="o"&gt;+&lt;/span&gt; embarked &lt;span class="o"&gt;+&lt;/span&gt; familysizefac &lt;span class="o"&gt;+&lt;/span&gt; title
&lt;span class="kc"&gt;...&lt;/span&gt;
fmla &lt;span class="o"&gt;=&lt;/span&gt; fmla1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;No surprise here. Caret accepts parameter grids over which to search for hyperparameters. Here we set these up for our selected algorithms and combine them in a list along with additional model parameters expected by caret (such as a string identifying the type of model&amp;nbsp;etc):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;glmnetgrid &lt;span class="o"&gt;=&lt;/span&gt; expand.grid&lt;span class="p"&gt;(&lt;/span&gt;.alpha &lt;span class="o"&gt;=&lt;/span&gt; seq&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; .lambda &lt;span class="o"&gt;=&lt;/span&gt; seq&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="kc"&gt;...&lt;/span&gt;
rfgrid &lt;span class="o"&gt;=&lt;/span&gt; data.frame&lt;span class="p"&gt;(&lt;/span&gt;.mtry &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

configs &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;()&lt;/span&gt;
configs&lt;span class="o"&gt;$&lt;/span&gt;glmnet &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;(&lt;/span&gt;method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;glmnet&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; tuneGrid&lt;span class="o"&gt;=&lt;/span&gt;glmnetgrid&lt;span class="p"&gt;,&lt;/span&gt; preProcess&lt;span class="o"&gt;=&lt;/span&gt;pp&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kc"&gt;...&lt;/span&gt;
configs&lt;span class="o"&gt;$&lt;/span&gt;rf &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;(&lt;/span&gt;method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; tuneGrid&lt;span class="o"&gt;=&lt;/span&gt;rfgrid&lt;span class="p"&gt;,&lt;/span&gt; preProcess&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; ntree&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have a list of training algorithms along with their required parameters, it&amp;#8217;s just a matter of looping over it to train the corresponding&amp;nbsp;classifiers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;arg &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;(&lt;/span&gt;form &lt;span class="o"&gt;=&lt;/span&gt; fmla&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; trainset&lt;span class="p"&gt;,&lt;/span&gt; trControl &lt;span class="o"&gt;=&lt;/span&gt; cvctrl&lt;span class="p"&gt;,&lt;/span&gt; metric &lt;span class="o"&gt;=&lt;/span&gt; scorer&lt;span class="p"&gt;)&lt;/span&gt;
models &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;()&lt;/span&gt;
set.seed&lt;span class="p"&gt;(&lt;/span&gt;rseed&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kr"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;i &lt;span class="kr"&gt;in&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;length&lt;span class="p"&gt;(&lt;/span&gt;configs&lt;span class="p"&gt;))&lt;/span&gt; 
&lt;span class="p"&gt;{&lt;/span&gt;
  models&lt;span class="p"&gt;[[&lt;/span&gt;i&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; do.call&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;train.formula&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; c&lt;span class="p"&gt;(&lt;/span&gt;arg&lt;span class="p"&gt;,&lt;/span&gt; configs&lt;span class="p"&gt;[[&lt;/span&gt;i&lt;span class="p"&gt;]]))&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
names&lt;span class="p"&gt;(&lt;/span&gt;models&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; sapply&lt;span class="p"&gt;(&lt;/span&gt;models&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; x&lt;span class="o"&gt;$&lt;/span&gt;method&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&amp;#8217;s look at some comparisons of the individual classifiers (Table&amp;nbsp;1):&lt;/p&gt;
&lt;figure &gt;
&lt;div class="figCenter"&gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt; &lt;TH&gt;  &lt;/TH&gt; &lt;TH&gt; glmnet &lt;/TH&gt; &lt;TH&gt; rf &lt;/TH&gt; &lt;TH&gt; gbm &lt;/TH&gt; &lt;TH&gt; ada &lt;/TH&gt; &lt;TH&gt; svmRadial &lt;/TH&gt; &lt;TH&gt; cforest &lt;/TH&gt; &lt;TH&gt; blackboost &lt;/TH&gt; &lt;TH&gt; earth &lt;/TH&gt; &lt;TH&gt; gamboost &lt;/TH&gt; &lt;TH&gt; bayesglm &lt;/TH&gt;  &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; train &lt;/TD&gt; &lt;TD align="right"&gt; 0.838 &lt;/TD&gt; &lt;TD align="right"&gt; 0.870 &lt;/TD&gt; &lt;TD align="right"&gt; 0.891 &lt;/TD&gt; &lt;TD align="right"&gt; 0.891 &lt;/TD&gt; &lt;TD align="right"&gt; 0.850 &lt;/TD&gt; &lt;TD align="right"&gt; 0.853 &lt;/TD&gt; &lt;TD align="right"&gt; 0.843 &lt;/TD&gt; &lt;TD align="right"&gt; 0.838 &lt;/TD&gt; &lt;TD align="right"&gt; 0.842 &lt;/TD&gt; &lt;TD align="right"&gt; 0.832 &lt;/TD&gt; &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; val &lt;/TD&gt; &lt;TD align="right"&gt; 0.808 &lt;/TD&gt; &lt;TD align="right"&gt; 0.797 &lt;/TD&gt; &lt;TD align="right"&gt; 0.853 &lt;/TD&gt; &lt;TD align="right"&gt; 0.825 &lt;/TD&gt; &lt;TD align="right"&gt; 0.825 &lt;/TD&gt; &lt;TD align="right"&gt; 0.808 &lt;/TD&gt; &lt;TD align="right"&gt; 0.802 &lt;/TD&gt; &lt;TD align="right"&gt; 0.797 &lt;/TD&gt; &lt;TD align="right"&gt; 0.785 &lt;/TD&gt; &lt;TD align="right"&gt; 0.808 &lt;/TD&gt; &lt;/TR&gt;
   &lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 1: Accuracy of individual classifiers on training and validation set.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The ada and gbm classifiers seems to do best in terms of accuracy, on both the training as well as the validation set, followed by the svm. However, since we have used the area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve as the optimized metric it might be more informative to drill down into how the classifiers perform in terms of &lt;span class="caps"&gt;ROC&lt;/span&gt;, specificity and&amp;nbsp;sensitivity. &lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/titanic/Roc.png" alt="Dot plot of ROC metrics for individual classifiers obtained from resamples created during cross-validation."/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 4: Dot plot of &lt;span class="caps"&gt;ROC&lt;/span&gt; metrics for individual classifiers estimated from resampled data (10 repeats of 10-fold cross-validation). &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Figure 4 uses the resample results from cross-validation to display means and 95% confidence intervals for the shown metrics. We note that though gbm and ada had the best accuracy on the validation set, there are other models that seem to find a better trade-off between sensitivity and specificity, at least as estimated on the resampled data. More specifically, gbm, ada and svm show relatively high sensitivity, but low specificity. The generalized linear and additive models (glm, gam) seem to do better. Also, while the svm has high accuracy on the validation set and high sensitivity (recall) in cross-validation, i.e. is good at identifying the survivors, it performs worst amongst all classifiers in correctly identifying those who died&amp;nbsp;(specificity).&lt;/p&gt;
&lt;p&gt;Finally, let&amp;#8217;s create ensembles from the individual models and compare their &lt;span class="caps"&gt;ROC&lt;/span&gt; performance to the models on the validation set. Two ensembles are created with the help of Zach Mayer&amp;#8217;s &lt;a href="https://github.com/zachmayer/caretEnsemble"&gt;caretEnsemble&lt;/a&gt; package (itself based on a paper by &lt;a href="http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf"&gt;Caruana et al. 2004&lt;/a&gt;): the first employs a greedy forward selection of individual models to incrementally add those to the ensemble that minimize the ensemble&amp;#8217;s chosen error metric. The ensemble&amp;#8217;s predictions are then essentially a weighted average of the individual predictions. The second ensemble simply trains a new caret model of choice using the matrix of individual model predictions as features (in this case I use a generalized linear model), also known as a&amp;nbsp;&amp;#8220;stack&amp;#8221;.&lt;/p&gt;
&lt;figure&gt;
&lt;div style="display:table"&gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt; &lt;TH&gt;  &lt;/TH&gt; &lt;TH&gt; glmnet &lt;/TH&gt; &lt;TH&gt; gbm &lt;/TH&gt; &lt;TH&gt; LogitBoost &lt;/TH&gt; &lt;TH&gt; earth &lt;/TH&gt; &lt;TH&gt; blackboost &lt;/TH&gt; &lt;TH&gt; bayesglm &lt;/TH&gt; &lt;TH&gt; gamboost &lt;/TH&gt; &lt;TH&gt; svmRadial &lt;/TH&gt; &lt;TH&gt; ada &lt;/TH&gt;  &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; weight &lt;/TD&gt; &lt;TD align="right"&gt; 0.387 &lt;/TD&gt; &lt;TD align="right"&gt; 0.292 &lt;/TD&gt; &lt;TD align="right"&gt; 0.203 &lt;/TD&gt; &lt;TD align="right"&gt; 0.080 &lt;/TD&gt; &lt;TD align="right"&gt; 0.019 &lt;/TD&gt; &lt;TD align="right"&gt; 0.009 &lt;/TD&gt; &lt;TD align="right"&gt; 0.007 &lt;/TD&gt; &lt;TD align="right"&gt; 0.002 &lt;/TD&gt; &lt;TD align="right"&gt; 0.001 &lt;/TD&gt; &lt;/TR&gt;
   &lt;/TABLE&gt;

&lt;TABLE class="table"&gt;
&lt;TR&gt; &lt;TH&gt;  &lt;/TH&gt; &lt;TH&gt; earth &lt;/TH&gt; &lt;TH&gt; gamboost &lt;/TH&gt; &lt;TH&gt; blackboost &lt;/TH&gt; &lt;TH&gt; cforest &lt;/TH&gt; &lt;TH&gt; bayesglm &lt;/TH&gt; &lt;TH&gt; glmnet &lt;/TH&gt; &lt;TH&gt; svmRadial &lt;/TH&gt; &lt;TH&gt; rf &lt;/TH&gt; &lt;TH&gt; greedyEns &lt;/TH&gt; &lt;TH&gt; ada &lt;/TH&gt; &lt;TH&gt; gbm &lt;/TH&gt; &lt;TH&gt; linearEns &lt;/TH&gt;  &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.836 &lt;/TD&gt; &lt;TD align="right"&gt; 0.846 &lt;/TD&gt; &lt;TD align="right"&gt; 0.846 &lt;/TD&gt; &lt;TD align="right"&gt; 0.858 &lt;/TD&gt; &lt;TD align="right"&gt; 0.861 &lt;/TD&gt; &lt;TD align="right"&gt; 0.862 &lt;/TD&gt; &lt;TD align="right"&gt; 0.862 &lt;/TD&gt; &lt;TD align="right"&gt; 0.865 &lt;/TD&gt; &lt;TD align="right"&gt; 0.873 &lt;/TD&gt; &lt;TD align="right"&gt; 0.876 &lt;/TD&gt; &lt;TD align="right"&gt; 0.878 &lt;/TD&gt; &lt;TD align="right"&gt; 0.879 &lt;/TD&gt; &lt;/TR&gt;
   &lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 2: Top: classifier weights determined by the greedy ensemble. Bottom: &lt;span class="caps"&gt;ROC&lt;/span&gt; measured on validation set for individual and ensemble classifiers.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;On the unseen validation set we notice once again that ada and gbm perform best amongst the individual classifiers, not only in terms of accuracy as demonstrated above, but also in terms of the area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve. Both, however, are outperformed slightly by the stacked ensemble&amp;nbsp;(linearEns). &lt;/p&gt;
&lt;p&gt;Finally, let&amp;#8217;s compare the performances on the validation set to those obtained from cross-validated training on the whole training set. Table 3 summarises corresponding metrics for all&amp;nbsp;classifiers:&lt;/p&gt;
&lt;figure&gt;
&lt;div&gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt; &lt;TH&gt;  &lt;/TH&gt; &lt;TH&gt; glmnet &lt;/TH&gt; &lt;TH&gt; rf &lt;/TH&gt; &lt;TH&gt; gbm &lt;/TH&gt; &lt;TH&gt; ada &lt;/TH&gt; &lt;TH&gt; svmRadial &lt;/TH&gt; &lt;TH&gt; cforest &lt;/TH&gt; &lt;TH&gt; blackboost &lt;/TH&gt; &lt;TH&gt; earth &lt;/TH&gt; &lt;TH&gt; gamboost &lt;/TH&gt; &lt;TH&gt; bayesglm &lt;/TH&gt; &lt;TH&gt; linearEns &lt;/TH&gt; &lt;TH&gt; greedyEns &lt;/TH&gt;  &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.871 &lt;/TD&gt; &lt;TD align="right"&gt; 0.875 &lt;/TD&gt; &lt;TD align="right"&gt; 0.877 &lt;/TD&gt; &lt;TD align="right"&gt; 0.875 &lt;/TD&gt; &lt;TD align="right"&gt; 0.864 &lt;/TD&gt; &lt;TD align="right"&gt; 0.871 &lt;/TD&gt; &lt;TD align="right"&gt; 0.866 &lt;/TD&gt; &lt;TD align="right"&gt; 0.869 &lt;/TD&gt; &lt;TD align="right"&gt; 0.873 &lt;/TD&gt; &lt;TD align="right"&gt; 0.870 &lt;/TD&gt; &lt;TD align="right"&gt; 0.880 &lt;/TD&gt; &lt;TD align="right"&gt; 0.878 &lt;/TD&gt; &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; Sens &lt;/TD&gt; &lt;TD align="right"&gt; 0.879 &lt;/TD&gt; &lt;TD align="right"&gt; 0.910 &lt;/TD&gt; &lt;TD align="right"&gt; 0.892 &lt;/TD&gt; &lt;TD align="right"&gt; 0.897 &lt;/TD&gt; &lt;TD align="right"&gt; 0.923 &lt;/TD&gt; &lt;TD align="right"&gt; 0.908 &lt;/TD&gt; &lt;TD align="right"&gt; 0.890 &lt;/TD&gt; &lt;TD align="right"&gt; 0.883 &lt;/TD&gt; &lt;TD align="right"&gt; 0.876 &lt;/TD&gt; &lt;TD align="right"&gt; 0.871 &lt;/TD&gt; &lt;TD align="right"&gt; 0.894 &lt;/TD&gt; &lt;TD align="right"&gt;  &lt;/TD&gt; &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; Spec &lt;/TD&gt; &lt;TD align="right"&gt; 0.750 &lt;/TD&gt; &lt;TD align="right"&gt; 0.699 &lt;/TD&gt; &lt;TD align="right"&gt; 0.743 &lt;/TD&gt; &lt;TD align="right"&gt; 0.739 &lt;/TD&gt; &lt;TD align="right"&gt; 0.678 &lt;/TD&gt; &lt;TD align="right"&gt; 0.701 &lt;/TD&gt; &lt;TD align="right"&gt; 0.723 &lt;/TD&gt; &lt;TD align="right"&gt; 0.721 &lt;/TD&gt; &lt;TD align="right"&gt; 0.740 &lt;/TD&gt; &lt;TD align="right"&gt; 0.752 &lt;/TD&gt; &lt;TD align="right"&gt; 0.733 &lt;/TD&gt; &lt;TD align="right"&gt;  &lt;/TD&gt; &lt;/TR&gt;
   &lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 3: Area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve, sensitivity and specificity of all models estimated in 10 repeats of 10-fold cross-validation after training on the whole data set (sens and spec are not calculated automatically by the greedy ensemble) . &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The results seem to confirm our finding from predictions on the validation set. After training on the whole data set ada and gbm exhibit the best cross-validated &lt;span class="caps"&gt;ROC&lt;/span&gt; measures, but the ensemble classifiers do even&amp;nbsp;better.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;Based on an assessment of the area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve, on both a validation subset of the data as well as repeated cross-validation on the whole set, boosted classification trees (&lt;a href="http://dept.stat.lsa.umich.edu/~gmichail/ada_final.pdf"&gt;ada&lt;/a&gt; and &lt;a href="http://gradientboostedmodels.googlecode.com/git/gbm/inst/doc/gbm.pdf"&gt;gbm&lt;/a&gt;) seem to perform best amongst single classifiers on the titanic data set. Ensembles built using a range of different classifiers, in particular in the form of a stack, lead to a small but seemingly consistent improvement over the performance of individual classifiers. I therefore chose to submit the predictions of the generalized linear stack. Interestingly, this did not lead to my best submission score. The ensemble has an accuracy of 0.78947 on the public leaderboard, i.e. on the part of the test set used to score different submissions. In comparison, I&amp;#8217;ve also trained a single forest of conditional inference trees using the familyid information as an additional predictor, which obtained an accuracy score of 0.81818 and ended up much higher on the leaderboard. Now, kaggle leaderboard position in itself &lt;a href="http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/"&gt;doesn&amp;#8217;t always correlate well&lt;/a&gt; with final performance on the whole test set, essentially because of overfitting to the leaderboard if many submissions are made and models selected on the basis of achieved position. Nevertheless, before the end of the competition it might be worth comparing the above classifiers and ensembles with different formulas (combinations of predictors, including family identifiers). Another option is to perform the full training again with accuracy rather than &lt;span class="caps"&gt;AUC&lt;/span&gt; as the optimized metric, which is the one used to assess predictions by kaggle in this competition. However, as have commented many kagglers involved in past competitions, it is probably better to rely on one&amp;#8217;s own cross-validation scores, rather than potentially overinflated leaderboard scores, to predict a model&amp;#8217;s final&amp;nbsp;success.&lt;/p&gt;
&lt;script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Buhrmann</dc:creator><pubDate>Thu, 23 Oct 2014 13:18:09 +0200</pubDate><guid>tag:synergenz.github.io,2014-10-23:titanic-survival.html</guid><category>R</category><category>kaggle</category><category>titanic</category><category>report</category></item><item><title>Categorisation of inertial activity data</title><link>https://synergenz.github.io/activity-data.html</link><description>&lt;p&gt;The ubiquity of mobile phones equipped with a wide range of sensors presents interesting opportunities for data mining applications. In this report we aim to find out whether data from accelerometers and gyroscopes can be used to identify physical activities performed by subjects wearing mobile phones on their&amp;nbsp;wrist.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/activitycat/muybridge.jpg" alt="Human activity"/&gt;&lt;/p&gt;
&lt;h3&gt;Methods&lt;/h3&gt;
&lt;p&gt;The data used in this analysis is based on the “Human activity recognition using smartphones” data set available from the &lt;span class="caps"&gt;UCL&lt;/span&gt; Machine Learning Repository [1]. A preprocessed version was downloaded from the Data Analysis online course [2]. The set contains data derived from 3-axial linear acceleration and 3-axial angular velocity sampled at 50Hz from a Samsung Galaxy S &lt;span class="caps"&gt;II&lt;/span&gt;. These signals were preprocessed using various filters and other methods to reduce noise and to separate low- and high-frequency components. From this data a set of 17 individual signals was extracted by separating e.g. accelerations due to gravity from those due to body motion, separating acceleration magnitude into its individual axis-aligned components and so on. The final feature variables were calculated from both the time and frequency domain of these signals. They include too large a range to cover entirely here, but examples include variables related to the spread and centre of each signal, its entropy, skewness and kurtosis in frequency space and many&amp;nbsp;more.&lt;/p&gt;
&lt;p&gt;All data was recorded while subjects (age 19-48) performed one of six activities and labelled accordingly: lying, sitting, standing, walking, walking down stairs and walking up&amp;nbsp;stairs.&lt;/p&gt;
&lt;p&gt;The problem to be solved in our analysis is the prediction of the activity class from sensor data. Since we are only interested in prediction, and not in producing an accurate or easily comprehensible model of the relation between activity and sensor data, we have chosen to investigate the performance of the following three classifiers only: random forest (&lt;span class="caps"&gt;RF&lt;/span&gt;), support vector machine (&lt;span class="caps"&gt;SVM&lt;/span&gt;) and linear discriminant analysis (&lt;span class="caps"&gt;LDA&lt;/span&gt;). A short description of each algorithm is given in the next&amp;nbsp;sections.&lt;/p&gt;
&lt;p&gt;In order to assess and compare the performance of these classifiers we separated the data into a training and a test set. The latter consisted of data for subjects 27 to 30 and the former of the&amp;nbsp;remainder.&lt;/p&gt;
&lt;h4&gt;Random&amp;nbsp;forests&lt;/h4&gt;
&lt;p&gt;Random forests are a recursive partitioning method [3]. In the case of classification, the algorithm creates a set of decision trees calculated on random subsets of the data, using at each split of a decision tree a random subset of predictors. The final prediction is made on the basis of a majority vote across all trees. Random trees have been chosen for this analysis in part because of their accuracy and their applicability to large data sets without the need for feature&amp;nbsp;selection.&lt;/p&gt;
&lt;p&gt;Because the trees in random forests are already build from random subsamples of the data, they do not require cross-validation to estimate accuracy, and the &lt;span class="caps"&gt;OOB&lt;/span&gt; (out-off-bag) error calculated internally is generally considered a good estimator of prediction error. They also do no require the tuning of many hyper-parameters. The algorithm is not sensitive, for example, to the number of trees fitted, as long as that number is greater than a few hundred. However, some have reported variation in performance depending on the proportion of variables tested at each split. We therefore tuned this parameter using a monotonic error reduction criterion which searches for performance improvement to both sides of the default value (the square root of the number of variables, approx. 23 in this case). Using the best identified value we then trained a final random forest for&amp;nbsp;prediction.&lt;/p&gt;
&lt;p&gt;Random forests conveniently can provide a measure of each predictor’s importance. This is achieved by comparing the performance of the tree before and after shuffling the values of the variables in question, thereby removing its relation with the outcome&amp;nbsp;variable.&lt;/p&gt;
&lt;h4&gt;Support vector&amp;nbsp;machines&lt;/h4&gt;
&lt;p&gt;Support vector machines (SVMs) classify data by separating it into classes such that the distance between their decision boundaries and the closest data points is maximised (i.e. by finding maximum margin hyperplanes) [4]. The algorithm is based on a mathematical trick that involves the use of simple linear boundaries in a high-dimensional non-linear feature space; without requiring computations on this complex transformation of the data. The mapping of the feature space is done using kernel functions, which can be selected based on the classification problem. The data is then modeled using a weighted combination of the closest points in transformed space (the support&amp;nbsp;vectors).&lt;/p&gt;
&lt;p&gt;Here we use the &lt;span class="caps"&gt;SVM&lt;/span&gt; classifier provided in the e1071 package for R [5]. For multiclass problems this algorithm performs a one-against-one voting scheme. We chose the default optimization method “C-classification”, where the hyper-parameter C scales the misclassification cost, such that the higher the value the more complex the model (i.e. the larger the bias). We also chose to use the radial basis kernel, which is commonly considered a good first choice. The cost parameter C, along with γ, which defines the size of the kernel (the spatial extent of the influence of a training example), was tuned using grid-search [6] with 10-fold cross validation (tuning function provided in e1071&amp;nbsp;package).&lt;/p&gt;
&lt;h4&gt;Linear discriminant&amp;nbsp;analysis&lt;/h4&gt;
&lt;p&gt;Linear discriminant analysis (&lt;span class="caps"&gt;LDA&lt;/span&gt;) is similar to &lt;span class="caps"&gt;SVM&lt;/span&gt; in that it also tries to transform the problem such that classes separated by non-linear decision boundaries become linearly separable [4]. Instead of using kernels and support vectors, however, it identifies a linear transformation of the predictor variables (a “discriminant function”) that allows for more accurate classification than individual predictors. Identification of the transformation is based on the maximisation of the ratio of between-class variance to within-class variance. The transformation thereby maximises the separation between&amp;nbsp;classes.&lt;/p&gt;
&lt;h4&gt;Combination of&amp;nbsp;classifiers&lt;/h4&gt;
&lt;p&gt;We evaluate the performance of each classifier using its error rate (the proportion of misclassified data) or equivalently its accuracy (proportion of correctly classified data). We then combine all three methods using a simple majority vote on the prediction&amp;nbsp;set.&lt;/p&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;The data set contains 7352 observations of 561 features (in addition to a subject index and the activity performed). Of the 21 subjects included in the data, the last four were used only for evaluating the final performance of the algorithms (test set, 1485 observations) and the rest for training (5867 observations). The same sets were used for all classifiers unless stated otherwise. Data was reasonably distributed across activities (number of data points in each class: lying=1407, sitting=1286, standing=1374, walk=1226, walking down stairs=986, walking up stairs=1073). Since the classifiers used here do not make strong assumptions about the distribution of data (they are relatively robust), no detailed investigation of the statistical properties of individual features was performed. In particular, the methods employed did not require transformations of individual features (e.g. such as to improve normality of their distribution). However, as can be expected from the fact that all features derive from the same few sensor signals, the data exhibits high collinearity. While this would have led to problems with confounders in e.g. a regression model, this was not generally the case with the methods employed here. It was addressed explicitly for the &lt;span class="caps"&gt;LDA&lt;/span&gt; however (see&amp;nbsp;below).&lt;/p&gt;
&lt;p&gt;We first report results from individual classifiers and then their&amp;nbsp;combination.&lt;/p&gt;
&lt;h4&gt;Random&amp;nbsp;Forest&lt;/h4&gt;
&lt;p&gt;We tuned the proportion of variables considered in each split using 100 trees for each evaluation. The best value found was 20. A final random forest was then trained using the optimal value and 500 trees. Error rate remained low (&amp;lt; 5%) and stable after about 250 trees had been added. Analysis of variable importances, considering both the mean decrease in accuracy and Gini index, shows that the most significant variables are related to the acceleration due to gravity along the X and Y axes, as well as the mean angle with respect to gravity in the same directions (with corresponding measures from the time&amp;nbsp;domain).&lt;/p&gt;
&lt;p&gt;The error rate of the fitted &lt;span class="caps"&gt;RF&lt;/span&gt; is 1.6% on the training set and 4.6% on the test set (accuracy of 0.954). The confusion matrix of the predicted activities (Table 1) shows that misclassification is almost exclusively due to an inability to distinguish sitting from standing. For example, while precision is greater than 0.977 for all other activities, it is 0.912 and 0.876 for sitting and standing&amp;nbsp;respectively.&lt;/p&gt;
&lt;figure&gt;
&lt;div class="figCenter"&gt;

&lt;TABLE class="table"&gt;
&lt;TR&gt;
&lt;TH&gt;  &lt;/TH&gt;&lt;TH&gt; lying &lt;/TH&gt;&lt;TH&gt; sitting &lt;/TH&gt;&lt;TH&gt; standing &lt;/TH&gt;&lt;TH&gt; walking &lt;/TH&gt;&lt;TH&gt; walk down &lt;/TH&gt;&lt;TH&gt; walk up &lt;/TH&gt;&lt;TH&gt; precision &lt;/TH&gt;
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; lying &lt;/TD&gt; &lt;TD align="right"&gt; 293 &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sitting &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 227 &lt;/TD&gt; &lt;TD align="right"&gt; 22 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.9116 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; standing &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 37 &lt;/TD&gt; &lt;TD align="right"&gt; 261 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.8758 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walking &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 228 &lt;/TD&gt; &lt;TD align="right"&gt; 2 &lt;/TD&gt; &lt;TD align="right"&gt; 1 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9870 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk down &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 194 &lt;/TD&gt; &lt;TD align="right"&gt; 1 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9949 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk up &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 1 &lt;/TD&gt; &lt;TD align="right"&gt; 4 &lt;/TD&gt; &lt;TD align="right"&gt; 214 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9772 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sensitivity &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt;&lt;TD align="right"&gt; 0.8598 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9223 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9956 &lt;/TD&gt; &lt;TD align="right"&gt; 0.97 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9907 &lt;/TD&gt; &lt;TD align="right"&gt; accuracy=0.954 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 1: Confusion matrix of random forest predictions. Rows correspond to predicted, and columns to reference (real observed) activities. Zero counts are omitted for clarity and misclassifications appear in off-diagonal entries (precision = positive predictive value, sensitivity = true positive rate).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4&gt;Support Vector&amp;nbsp;Machine&lt;/h4&gt;
&lt;p&gt;Tuning of &lt;span class="caps"&gt;SVM&lt;/span&gt; hyper-parameters using the training set resulted in optimal values of the cost C = 100 and kernel size γ = 0.001 (search was performed in intervals γ ∈ [1e-6, 0.1] and C ∈ [1,100]). To reduce computation time, the search was performed on a fraction (20%) of data randomly sampled from the training set. Using these optimal values a final &lt;span class="caps"&gt;SVM&lt;/span&gt; was trained on the whole&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;The resulting &lt;span class="caps"&gt;SVM&lt;/span&gt; uses 22.6% of the data points as support vectors (1326 out of 5867). Since this number depends on the tuned parameter C, which was found using cross-validation, we assume that we have not overfit the model. This is supported by the model’s high accuracy of 0.989 on the training set when averaged over a 10-fold cross validation. On the test set its accuracy is 0.96, i.e. slightly better than the random&amp;nbsp;forest. &lt;/p&gt;
&lt;p&gt;The confusion matrix of predictions is shown in Table 2. As we can see, the &lt;span class="caps"&gt;SVM&lt;/span&gt; exhibits perfect classification for all activities other than sitting and standing, where its performance is similar to the random&amp;nbsp;forest.&lt;/p&gt;
&lt;figure&gt;
&lt;div class="figCenter"&gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt;
&lt;TH&gt;  &lt;/TH&gt;&lt;TH&gt; lying &lt;/TH&gt;&lt;TH&gt; sitting &lt;/TH&gt;&lt;TH&gt; standing &lt;/TH&gt;&lt;TH&gt; walking &lt;/TH&gt;&lt;TH&gt; walk down &lt;/TH&gt;&lt;TH&gt; walk up &lt;/TH&gt;&lt;TH&gt; precision &lt;/TH&gt;
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; lying &lt;/TD&gt; &lt;TD align="right"&gt; 293 &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sitting &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 232 &lt;/TD&gt; &lt;TD align="right"&gt; 27 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.8958 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; standing &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 32 &lt;/TD&gt; &lt;TD align="right"&gt; 256 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.8889 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walking &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 229 &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk down &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 200 &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk up &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 216 &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sensitivity &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt;&lt;TD align="right"&gt; 0.8788 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9046 &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; &lt;TD align="right"&gt; accuracy=0.96 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 2: Confusion matrix of &lt;span class="caps"&gt;SVM&lt;/span&gt; predictions. See Table 1 for further details.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4&gt;Linear Discriminant&amp;nbsp;Analysis&lt;/h4&gt;
&lt;p&gt;&lt;span class="caps"&gt;LDA&lt;/span&gt; can be sensitive or even fail when the data exhibits a high degree of collinearity. Since our sensor data essentially consists of different transformations of the same few signals we can expect that this is indeed the case in our data set. We therefore performed two &lt;span class="caps"&gt;LDA&lt;/span&gt; classifications. For the first model (&lt;span class="caps"&gt;LDA1&lt;/span&gt;) the complete training set was used. For the second model (&lt;span class="caps"&gt;LDA2&lt;/span&gt;) we removed those variables that exhibited pair-wise correlations greater than R=0.9 (removing one from each pair) using the findCorrelation function in R’s caret package. A total of 346 variables were thus removed, leaving 215 less correlated predictors. Using these two training sets, &lt;span class="caps"&gt;LDA&lt;/span&gt; models were trained with 10-fold cross validation to assess whether we would expect a difference in their accuracy. The &lt;span class="caps"&gt;LDA2&lt;/span&gt; model, trained on relatively uncorrelated data, showed an error rate of 3.5%, and &lt;span class="caps"&gt;LDA1&lt;/span&gt; a rate of 5.2%. Based on these results we have to conclude that &lt;span class="caps"&gt;LDA2&lt;/span&gt; should be used for our final&amp;nbsp;predictions.&lt;/p&gt;
&lt;p&gt;Table 3 shows the confusion matrix for the &lt;span class="caps"&gt;LDA2&lt;/span&gt; model when predicting on the test&amp;nbsp;set.&lt;/p&gt;
&lt;figure&gt;
&lt;div class="figCenter" &gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt;
&lt;TH&gt;  &lt;/TH&gt;&lt;TH&gt; lying &lt;/TH&gt;&lt;TH&gt; sitting &lt;/TH&gt;&lt;TH&gt; standing &lt;/TH&gt;&lt;TH&gt; walking &lt;/TH&gt;&lt;TH&gt; walk down &lt;/TH&gt;&lt;TH&gt; walk up &lt;/TH&gt;&lt;TH&gt; precision &lt;/TH&gt;
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; lying &lt;/TD&gt; &lt;TD align="right"&gt; 293 &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sitting &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 223 &lt;/TD&gt; &lt;TD align="right"&gt; 24 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.9028 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; standing &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 41 &lt;/TD&gt; &lt;TD align="right"&gt; 259 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.8633 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walking &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 226 &lt;/TD&gt; &lt;TD align="right"&gt; 3 &lt;/TD&gt; &lt;TD align="right"&gt; 2 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9784 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk down &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 196 &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk up &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 3 &lt;/TD&gt; &lt;TD align="right"&gt; 1 &lt;/TD&gt; &lt;TD align="right"&gt; 214 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9817 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sensitivity &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt;&lt;TD align="right"&gt; 0.8447 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9152 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9869 &lt;/TD&gt; &lt;TD align="right"&gt; 0.98 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9907 &lt;/TD&gt; &lt;TD align="right"&gt; accuracy=0.95 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 3: Confusion matrix of &lt;span class="caps"&gt;LDA2&lt;/span&gt; predictions. See Table 1 for further details.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We can observe the same pattern of misclassification as in the other two models.
Interestingly, when we use &lt;span class="caps"&gt;LDA1&lt;/span&gt; for prediction, accuracy is increased to 0.9785 (error rate of 2.15%). Nevertheless, since in cross-validation on the training set &lt;span class="caps"&gt;LDA2&lt;/span&gt; performed better, we assume that this increase is a result of chance only and does not reflect a truly better&amp;nbsp;model.&lt;/p&gt;
&lt;h4&gt;Comparison of&amp;nbsp;classifiers&lt;/h4&gt;
&lt;p&gt;Comparing the three classifiers in terms of their sensitivity (recall), i.e. the proportion of correct predictions for each class, we have already seen that all three models perform very similar, with the &lt;span class="caps"&gt;SVM&lt;/span&gt; having a slight advantage. We can speculate that this is due to the non-linear (radial basis) decision boundaries of the classifier, which stands in contrast to the linear methods employed in the other two&amp;nbsp;models.&lt;/p&gt;
&lt;p&gt;Based on the previous results we expect not to gain much predictive power from the combination of individual models using a simple majority vote. All models exhibit the same problem of misclassification of sitting and standing activities, and therefore do not complement each other. This is confirmed by a combined accuracy of 0.958 when predictions are made based on a majority vote of the three models, which sits exactly between the lower scoring &lt;span class="caps"&gt;RF&lt;/span&gt; and &lt;span class="caps"&gt;LDA&lt;/span&gt; on the one hand, and the slightly higher scoring &lt;span class="caps"&gt;SVM&lt;/span&gt; on the&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;What explains the consistent misclassification of sitting and standing across all three models? Intuitively it is clear that since in both “activities” subjects remain more or less motionless, inertial data will not provide much differentiating information. This is reflected in the data. To illustrate this we trained another random forest on a new subset of the training data which a) included only sitting and standing activities, and b) only included predictors with pair-wise correlations less than R=0.9 (same procedure as for the &lt;span class="caps"&gt;LDA&lt;/span&gt; model). This data set therefore consisted of a binary outcome and 2113 observations (1022 and 1091 in each level). The importances of the resulting random forest show that the most significant split is achieved on the mean angle of gravity with respect to the Y axis (θy), followed by the energy measure of acceleration due to gravity in the Y dimension in the time domain (gey) or, according to the mean decrease in Gini index, the entropy measure of the same variable. In the left panel of Figure 1 we plot the data along these two axes (θy vs. gey) and color the data according to&amp;nbsp;activity. &lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/activitycat/intertial.jpg" alt="Inertial data"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 1: Overlap of data from sitting and standing activities underlying the failure to perfectly separate these two classes. Left panel: scatterplot of the two most important variables for distinguishing sitting and standing activities (according to a random forest fitted to data for these two activities only). θy is the mean angle of gravity with respect to the y-axis, and gey is the entropy of acceleration due to gravity in the y-dimension (see main text for further details). Only part of the range for θy is shown to highlight the region of overlap. Right panel: the same overlap is more clearly seen in the histogram of the θy variable only. Even though the means of θy for sitting and standing are different (p-value in t-test &lt; 2.2e-16), their distributions overlap significantly.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We can see that while the data falls into two identifiable regions, these are not perfectly separable but rather show significant overlap. This can be seen even more clearly in the right panel of Figure 1, where we overimpose histograms of θy separated by activity. The distributions of sitting and standing in this variable are clearly different statistically, but also overlap significantly. Their difference is confirmed by a t-test of their means (-0.01 and 0.21 for sitting and standing respectively, p– value &amp;lt; 2.2e-16). Nevertheless, the overlap means that no classifier should be able to distinguish these two activities perfectly, at least not based on this single variable. Adding further variables might help in separating the two distributions. But as the three trained models seem to indicate, the data set does not appear to contain the kind of variables that allows for perfect discrimination of sitting and&amp;nbsp;standing.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;We have used three different types of classifiers to predict a subject’s physical activity from inertial data captured using the accelerometer and gyroscope embedded in mobile phones worn at the wrist. All classifiers performed well overall (accuracy &amp;gt; 0.95), but failed equally to distinguish some cases of sitting and standing. We observe, however, that the non-linear &lt;span class="caps"&gt;SVM&lt;/span&gt; seems to have a slight advantage over the two linear models. This suggests that perhaps a non-linear variant of the &lt;span class="caps"&gt;LDA&lt;/span&gt; algorithm (namely quadratic discriminant analysis, or &lt;span class="caps"&gt;QDA&lt;/span&gt;), and equally a random forest using decision trees with non-linear boundaries, would have been more appropriate for this data set. Further work would also be needed to determine whether the radial kernel used in the &lt;span class="caps"&gt;SVM&lt;/span&gt; model is in fact the optimal kernel for this data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;We have shown that the data used in this analysis does not seem to contain individual variables that can separate sitting and standing activities perfectly. The failure of all three classifiers also suggests that the two activities cannot be resolved in higher dimensions. This is corroborated by the fact that the classifiers all take rather different approaches, e.g. parametric (&lt;span class="caps"&gt;LDA&lt;/span&gt;) and non-parametric (&lt;span class="caps"&gt;RF&lt;/span&gt;), or linear (decision trees) and non-linear decision boundaries (&lt;span class="caps"&gt;SVM&lt;/span&gt;). Of course, the failure to distinguish sitting and standing using inertial data only is not surprising, as both activities imply near stationarity of the sensors. However, we can hypothesise that other transformations of the data not provided in this set could be helpful. E.g. accelerations in the vertical direction due to body motion should show non-linear step changes at the moment of sitting down, while this would not be the case if a person continued standing. Adding the existence of such step-changes to the data set could potentially lead to better separability of these&amp;nbsp;activities.&lt;/p&gt;
&lt;p&gt;We have not here performed an analysis of variation between subjects. It is possible that the behaviour of some subjects differs significantly from that of others, and that in the process of “averaging” across subjects information is lost. Future work should also address this&amp;nbsp;question.&lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;ol class="bib"&gt;
&lt;li&gt;&lt;span class="caps"&gt;UCI&lt;/span&gt; Data set: &lt;a href="http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"&gt;Human Activity Recognition Using&amp;nbsp;Smartphones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spark-public.s3.amazonaws.com/dataanalysis/samsungData.rda"&gt;Preprocessed data set&lt;/a&gt; on Amazon S3&amp;nbsp;storage.&lt;/li&gt;
&lt;li&gt;Breiman, L. (2001), Random Forests, Machine Learning 45(1),&amp;nbsp;5-32.&lt;/li&gt;
&lt;li&gt;Bishop, &lt;span class="caps"&gt;C. M.&lt;/span&gt;(2006). Pattern recognition and machine learning (Vol. 4, No. 4). New York:&amp;nbsp;Springer.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cran.r-project.org/web/packages/e1071/index.html"&gt;&lt;span class="caps"&gt;SVM&lt;/span&gt; package&amp;nbsp;&amp;#8216;e1071&amp;#8217;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bergstra, J. and Bengio, Y. (2012). Random Search for Hyper-Parameter Optimization. J. Machine Learning Research 13:&amp;nbsp;281—305.&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Buhrmann</dc:creator><pubDate>Thu, 23 Oct 2014 13:18:03 +0200</pubDate><guid>tag:synergenz.github.io,2014-10-23:activity-data.html</guid><category>R</category><category>report</category><category>categorisation</category><category>svm</category><category>random forest</category><category>lda</category></item><item><title>Dash+ visualization of running data</title><link>https://synergenz.github.io/dash+.html</link><description>&lt;p&gt;&lt;a href="http://dashplus.herokuapp.com/"&gt;Dash+&lt;/a&gt; is a python web application I built with &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt;, which imports Nike+ running data into a NoSQL database (MongoDB) and uses &lt;a href="http://d3js.org/"&gt;D3.js&lt;/a&gt; to visualize and analyze&amp;nbsp;it. &lt;/p&gt;
&lt;p&gt;The app is work in progress and primarily intended as a personal playground for exploring d3 visualization of my own running data. Having said that, if you want to fork your own version on &lt;a href="https://github.com/synergenz/dash"&gt;github&lt;/a&gt;, simply add your Nike access token in the corresponding&amp;nbsp;file.&lt;/p&gt;
&lt;div class="row"&gt;
  &lt;div class="col-sm-8 col-md-6"&gt;
&lt;div class="thumbnail"&gt;
&lt;a href="http://dashplus.herokuapp.com"&gt;&lt;img src="/images/dash/screen1.png" alt="Dash+ screenshot 1"/&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Buhrmann</dc:creator><pubDate>Thu, 23 Oct 2014 13:18:00 +0200</pubDate><guid>tag:synergenz.github.io,2014-10-23:dash+.html</guid><category>visualization</category><category>d3</category><category>nosql</category><category>python</category></item><item><title>Reading from distributed cache in Hadoop</title><link>https://synergenz.github.io/haddop-distributed-cache.html</link><description>&lt;p&gt;The distributed cache can be used to make small files (or jars etc.) available to mapreduce functions locally on each node. This can be useful e.g. when a global stopword list is needed by all mappers for index creation.  Here are two correct ways of reading a file from distributed cache in Hadoop 2. This has changed in the new &lt;span class="caps"&gt;API&lt;/span&gt; and very few books and tutorials have updated&amp;nbsp;examples.&lt;/p&gt;
&lt;h3&gt;Named&amp;nbsp;File&lt;/h3&gt;
&lt;p&gt;In the&amp;nbsp;driver:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Job&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Job&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getInstance&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
&lt;span class="n"&gt;job&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;addCacheFile&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;URI&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/path/to/file.csv&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;#filelabel&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the&amp;nbsp;mapper:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nd"&gt;@Override&lt;/span&gt;
&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setup&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Context&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;InterruptedException&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;URI&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;cacheFiles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getCacheFiles&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cacheFiles&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;cacheFiles&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;length&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;BufferedReader&lt;/span&gt; &lt;span class="n"&gt;reader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;BufferedReader&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;FileReader&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;filelabel&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;File&amp;nbsp;system&lt;/h3&gt;
&lt;p&gt;In the&amp;nbsp;driver:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Job&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Job&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getInstance&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
&lt;span class="n"&gt;job&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;addCacheFile&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;URI&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/path/to/file.csv&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the&amp;nbsp;mapper:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nd"&gt;@Override&lt;/span&gt;
&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setup&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Context&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;InterruptedException&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;URI&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;cacheFiles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getCacheFiles&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cacheFiles&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;cacheFiles&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;length&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;FileSystem&lt;/span&gt; &lt;span class="n"&gt;fs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FileSystem&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getConfiguration&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
        &lt;span class="n"&gt;Path&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cacheFiles&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;].&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
        &lt;span class="n"&gt;BufferedReader&lt;/span&gt; &lt;span class="n"&gt;reader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;BufferedReader&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;InputStreamReader&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;open&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;)));&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Thomas Buhrmann</dc:creator><pubDate>Thu, 16 Oct 2014 12:14:20 +0200</pubDate><guid>tag:synergenz.github.io,2014-10-16:haddop-distributed-cache.html</guid><category>hadoop</category><category>big data</category><category>java</category></item></channel></rss>