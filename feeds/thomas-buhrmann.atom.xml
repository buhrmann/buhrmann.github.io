<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>datawerk</title><link href="https://buhrmann.github.io/" rel="alternate"></link><link href="https://buhrmann.github.io/feeds/thomas-buhrmann.atom.xml" rel="self"></link><id>https://buhrmann.github.io/</id><updated>2015-07-20T00:00:00+02:00</updated><entry><title>Elegans now features PubMed search</title><link href="https://buhrmann.github.io/elegans-pubmed.html" rel="alternate"></link><updated>2015-07-20T00:00:00+02:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2015-07-20:elegans-pubmed.html</id><summary type="html">&lt;p&gt;I&amp;#8217;ve added a new PubMed search feature to &lt;a href="https://elegans.herokuapp.com/"&gt;&lt;em&gt;Elegans&lt;/em&gt;&lt;/a&gt;, the visual worm brain explorer. The idea here is to show the network of &lt;em&gt;C. Elegans&lt;/em&gt; neurons that get mentioned in more than n papers on PubMed, in the context of a given search query. So, for example, if one is interested in the worm&amp;#8217;s chemotaxis behaviour, one would type in &amp;#8216;chemotaxis&amp;#8217; and choose the citation threshold n. Initiating the search will then return the neurons that get mentioned in at least n papers along with the word &amp;#8216;chemotaxis&amp;#8217;. The search is in fact performed once for each neuron class, and the results collated. One can further select to either show the network of only those neurons returned by the search, or use the resulting neurons to populate the subgraph search panel, which contains additional filtering&amp;nbsp;options.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/elegans/pubmed.png" alt="Pubmed search in Elegans."/&gt;&lt;/p&gt;</summary><category term="visualization"></category><category term="d3"></category><category term="neo4j"></category><category term="python"></category><category term="graph"></category><category term="nosql"></category></entry><entry><title>Exploration of voopter airfare data</title><link href="https://buhrmann.github.io/voopter.html" rel="alternate"></link><updated>2015-07-14T00:00:00+02:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2015-07-14:voopter.html</id><summary type="html">&lt;p&gt;I am currently working as a data science freelancer for &lt;a href="https://voopter.com.br"&gt;voopter.com.br&lt;/a&gt;, helping them analyze the data generated by airfare searches on their website. Voopter is a metasearch engine for flights from and to Brazil. The first thing I did was to create an interactive dashboard in R and shiny for some explorative statistics of the millions of seaches performed by users of their website (which has already led to more specific business-driven&amp;nbsp;questions).&lt;/p&gt;
&lt;p&gt;The dashboard provides a quick and easy way to filter and aggregate the data, which is stored in
an &lt;span class="caps"&gt;SQL&lt;/span&gt; database. The idea is to start with a destination of interest (which could be a specific city or country) and optionally a particular point of departure. For the given destination (or route in the latter case), the dashboard then allows for the selection of a particular date range, and level of aggregation (year, month, weekday etc.) and displays the most useful descriptive statistics. Here are some examples of interesting&amp;nbsp;results.&lt;/p&gt;
&lt;h2&gt;Overview of&amp;nbsp;origins&lt;/h2&gt;
&lt;p&gt;The first graph displayed, after having selected a destination of interest, is an overview of the most popular origins from which people intend to&amp;nbsp;depart:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/voopter/top_origins.png" alt="Top n origins"/&gt;&lt;/p&gt;
&lt;p&gt;Here I&amp;#8217;ve removed the origin names on the y-axis, as the data is proprietary. The origins are ordered by their frequency of searches (right), and are shown along with their overall price distribution (left). The violin plot shows the relative number of returned flights at the given price point, their 25 and 75 percentiles (ends of white horizontal bars), and the median price (red). Several observations can be made already from these distributions. For example, the plot is produces after removal of the top 0.5 percentile of prices. Still, prices exhibit a long tail of expensive flights, very different from the majority of flights. A simple average price for a given route is therefore not very informative (the median shown is more useful). Also, some distributions seem to be bimodal. Though this cannot be deduced from this plot alone, this is due to big differences in fares for flights made around popular holidays and those during low&amp;nbsp;season.&lt;/p&gt;
&lt;h2&gt;Factors influencing&amp;nbsp;price&lt;/h2&gt;
&lt;p&gt;Next we can look at the way a fare depends on the time of year, the airline, or how far in advance one is booking. For example, choosing to aggregate by calendar week, the dashboard will produce the following&amp;nbsp;graph:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/voopter/aggr_week.png" alt="Aggregate by calendar week"/&gt;&lt;/p&gt;
&lt;p&gt;Here, a boxplot summarizes the price distribution for each week, while a background scatter plot shows the actual fares color-coded by the number of days between the booking and departure (&amp;#8220;advance&amp;#8221;). It is obvious that prices increase around the time of popular holidays, like the summer and christmas. What&amp;#8217;s less obvious is the fact that people tend to try and book their christmas flights on relatively short notice, while they seem to plan their summer holidays more in advance (with a probability of overspending on their christmas&amp;nbsp;flights). &lt;/p&gt;
&lt;p&gt;Median prices can also be visualized on a per-day basis in a calendar view, which more easily picks out shorter popular holiday periods, like the carnival in Brazil in&amp;nbsp;February:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/voopter/price_cal.png" alt="Avg price calendar"/&gt;&lt;/p&gt;
&lt;p&gt;The dashboard also creates a boxplot similar to that above to summarize prices for different&amp;nbsp;airlines:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/voopter/aggr_airline.png" alt="Aggregate by airline"/&gt;&lt;/p&gt;
&lt;p&gt;Clearly, savings can be found on average by selecting the cheapest airline. Also, for each airline their most expensive flights are those around christmas and the cheapest those during the weeks before the summer&amp;nbsp;holidays.&lt;/p&gt;
&lt;p&gt;Next, what is the best time to book, i.e. how far in advance would it be best to start looking for flights to the selected destination? The following scatter plot shows fare prices as a function of advance and color-coded by the calendar week of&amp;nbsp;departure:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/voopter/price_adv.png" alt="Price vs. advance"/&gt;&lt;/p&gt;
&lt;p&gt;A common pattern, independent of the particular destination (in most cases), is that prices tend to decrease the further in advance a booking is made (see average price indicated by red line). However, the best time to book usually seems to be 3 to 4 months in advance (depending on the destination), after which fares tend to increase again. The plot also illustrates again the curious clustering of christmas flights that are being searched for only a few days or weeks before departure (while cheaper christmas flights, on average, can theoretically be found booking months in&amp;nbsp;advance).&lt;/p&gt;
&lt;p&gt;Overall, there are some clear trends describing the distribution of prices for flights to a given destination. These are rather simple to capture in a model, which allows both for the prediction of prices ranges, as well as recommendations as to the best time to fly and book&amp;nbsp;(inference). &lt;/p&gt;</summary><category term="R"></category><category term="visualization"></category><category term="exploratory"></category><category term="sql"></category></entry><entry><title>Analyzing tf-idf results in scikit-learn</title><link href="https://buhrmann.github.io/tfidf-analysis.html" rel="alternate"></link><updated>2015-06-22T00:00:00+02:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2015-06-22:tfidf-analysis.html</id><summary type="html">&lt;p&gt;In a &lt;a href="https://buhrmann.github.io/sklearn-pipelines.html"&gt;previous post&lt;/a&gt; I have shown how to create text-processing pipelines for machine learning in python using &lt;a href="http://scikit-learn.org/stable/"&gt;scikit-learn&lt;/a&gt;. The core of such pipelines in many cases is the vectorization of text using the &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;tf-idf&lt;/a&gt; transformation. In this post I will show some ways of analysing and making sense of the result of a tf-idf. As an example I will use the same &lt;a href="https://www.kaggle.com/c/stumbleupon"&gt;kaggle dataset&lt;/a&gt;, namely webpages provided and classified by StumbleUpon as either ephemeral (content that is short-lived) or evergreen (content that can be recommended long after its initial&amp;nbsp;discovery).&lt;/p&gt;
&lt;h3&gt;Tf-idf&lt;/h3&gt;
&lt;p&gt;As explained in the previous post, the tf-idf vectorization of a corpus of text documents assigns each word in a document a number that is proportional to its frequency in the document and inversely proportional to the number of documents in which it occurs. Very common words, such as &amp;#8220;a&amp;#8221; or &amp;#8220;the&amp;#8221;, thereby receive heavily discounted tf-idf scores, in contrast to words that are very specific to the document in question. The result is a matrix of tf-idf scores with one row per document and as many columns as there are different words in the&amp;nbsp;dataset.&lt;/p&gt;
&lt;p&gt;How do we make sense of this resulting matrix, specifically in the context of text classification? For example, how do the most important words, as measured by their tf-idf score, relate to the class of a document? Or can we characterise the documents that a tf-idf-based classifier commonly&amp;nbsp;misclassifies?&lt;/p&gt;
&lt;h3&gt;Analysing classifier&amp;nbsp;performance&lt;/h3&gt;
&lt;p&gt;Let&amp;#8217;s start by collecting some data about the performance of our classifier. We will then use this information to drill into specific groups of documents in terms of their tf-idf&amp;nbsp;scores.&lt;/p&gt;
&lt;p&gt;A typical way to assess a model&amp;#8217;s performance is to score it using cross-validation on the training set. One may do that using scikit-learn&amp;#8217;s built-in &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.cross_val_score.html"&gt;cross_validation.cross_val_score()&lt;/a&gt; function, but that only calculates the overall performance of the model on individual folds, and doesn&amp;#8217;t hang on to other information that may be useful. A manual cross-validation may therefore be more appropriate. The following code shows a typical implementation of cross-validation in&amp;nbsp;scikit-learn:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;analyze_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;folds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Run x-validation and return scores, averaged confusion matrix, and df with false positives and negatives &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;   &lt;span class="c"&gt;# to numpy&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c"&gt;# Manual x-validation to accumulate actual&lt;/span&gt;
&lt;span class="n"&gt;cv_skf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StratifiedKFold&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_folds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;folds&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;conf_mat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;      &lt;span class="c"&gt;# Binary classification&lt;/span&gt;
&lt;span class="n"&gt;false_pos&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Set&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;false_neg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Set&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;train_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val_i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cv_skf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;val_i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;val_i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Fitting fold...&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Predicting fold...&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;y_pprobs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c"&gt;# Predicted probabilities&lt;/span&gt;
    &lt;span class="n"&gt;y_plabs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_val&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c"&gt;# Predicted class labels&lt;/span&gt;

    &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pprobs&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="n"&gt;confusion&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_plabs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;conf_mat&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;confusion&lt;/span&gt;

    &lt;span class="c"&gt;# Collect indices of false positive and negatives&lt;/span&gt;
    &lt;span class="n"&gt;fp_i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_plabs&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;fn_i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_plabs&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_val&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;false_pos&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;fp_i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;false_neg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;update&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;val_i&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;fn_i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Fold score: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Fold CM: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;confusion&lt;/span&gt;

&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;Mean score: &lt;/span&gt;&lt;span class="si"&gt;%0.2f&lt;/span&gt;&lt;span class="s"&gt; (+/- &lt;/span&gt;&lt;span class="si"&gt;%0.2f&lt;/span&gt;&lt;span class="s"&gt;)&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;conf_mat&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="n"&gt;folds&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Mean CM: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conf_mat&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;Mean classification measures: &lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf_mat&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conf_mat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;fp&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;false_pos&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;fn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;false_neg&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This function not only calculates the average score (e.g. accuracy, in this case &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"&gt;area under the &lt;span class="caps"&gt;ROC&lt;/span&gt;-curve&lt;/a&gt;), but also calculates an averaged &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"&gt;confusion-matrix&lt;/a&gt; (across the different folds) and keeps a list of the documents (or more generally samples) that have been misclassified (false positives and false negatives separately). Finally, using the averaged confusion matrix, it also calculates averaged classification measures such as accuracy, precision etc. The corresponding function class_report() is&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;class_report&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf_mat&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;tp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;conf_mat&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;measures&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;measures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fn&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;measures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;specificity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tn&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tn&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        &lt;span class="c"&gt;# (true negative rate)&lt;/span&gt;
    &lt;span class="n"&gt;measures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sensitivity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tp&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;        &lt;span class="c"&gt;# (recall, true positive rate)&lt;/span&gt;
    &lt;span class="n"&gt;measures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;precision&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tp&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;measures&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;f1score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;fn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;measures&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One may, for example, use the confusion matrix or classification report to compare models with different classifiers to see whether there are differences in the misclassified samples (if different models perform very similar in this regard there may be no need to &amp;#8220;ensemblify&amp;#8221;&amp;nbsp;them). &lt;/p&gt;
&lt;p&gt;Here I will use the false positives and negatives to see whether we can use their tf-idf scores to understand why they are being&amp;nbsp;misclassified.&lt;/p&gt;
&lt;h3&gt;Making sense of the tf-idf&amp;nbsp;matrix&lt;/h3&gt;
&lt;p&gt;Let&amp;#8217;s assume we have a scikit-learn Pipeline that vectorizes our corpus of documents. Let X be the matrix of dimensionality (n_samples, 1) of text documents, y the vector of corresponding class labels, and &amp;#8216;vec_pipe&amp;#8217; a Pipeline that contains an instance of scikit-learn&amp;#8217;s TfIdfVectorizer. We produce the tf-idf matrix by transforming the text documents, and get a reference to the vectorizer&amp;nbsp;itself:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vec_pipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;vec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vec_pipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;named_steps&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;vec&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;vec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_feature_names&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;features&amp;#8217; here is a variable that holds a list of all the words in the tf-idf&amp;#8217;s vocabulary, in the same order as the columns in the matrix. Next, we create a function that takes a single row of the tf-idf matrix (corresponding to a particular document), and return the n highest scoring words (or more generally tokens or&amp;nbsp;features):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;top_tfidf_feats&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Get top n tfidf values in row and return them with their corresponding feature names.&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;topn_ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argsort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;)[::&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][:&lt;/span&gt;&lt;span class="n"&gt;top_n&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;top_feats&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;topn_ids&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;top_feats&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;feature&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;tfidf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here we use argsort to produce the indices that would order the row by tf-idf value, reverse them (into descending order), and select the first top_n. We then return a pandas DataFrame with the words themselves (feature names) and their corresponding&amp;nbsp;score.&lt;/p&gt;
&lt;p&gt;The result of a tf-idf, however, is typically a sparse matrix, which doesn&amp;#8217;t support all the usual matrix or array operations. So in order to apply the above function to inspect a particular document, we convert a single row into dense format&amp;nbsp;first:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;top_feats_in_doc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Top tfidf features in specific document (matrix row) &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;row_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;top_tfidf_feats&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Using this to show the top 10 words used in the third document of our matrix, for example, which StumbleUpon has classified as &amp;#8216;evergreen&amp;#8217;,&amp;nbsp;gives:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;         feature     tfidf
0            flu  0.167878
1  prevent heart  0.130590
2    fruits that  0.128400
3     of vitamin  0.123592
4    cranberries  0.119959
5        the flu  0.117032
6      fight the  0.115101
7      vitamin c  0.113120
8        vitamin  0.111867
9        bananas  0.107010
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This seems to be webpage about foods or supplements to prevent or fight flu symptoms. Let&amp;#8217;s see if this topic is represented also in the overall corpus. For this, we will calculate the average tf-idf score of all words across a number of documents (in this case all documents), i.e. the average per column of a tf-idf&amp;nbsp;matrix:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;top_mean_feats&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grp_ids&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_tfidf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Return the top n features that on average are most important amongst documents in rows&lt;/span&gt;
&lt;span class="sd"&gt;        indentified by indices in grp_ids. &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;grp_ids&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;grp_ids&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toarray&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;min_tfidf&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;tfidf_means&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;D&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;top_tfidf_feats&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tfidf_means&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, we provide a list of row indices which pick out the particular documents we want to inspect. Providing &amp;#8216;None&amp;#8217; indicates, somewhat counterintuitively, that we&amp;#8217;re interested in all documents. We then calculate the mean of each column across the selected rows, which results in a single row of tf-idf values. And this row we then simply pass on to our previous function for picking out the top n words. One crucial trick here, however, is to first filter out the words with relatively low scores (smaller than the provided threshold). This is because common words, such as &amp;#8216;a&amp;#8217; or &amp;#8216;the&amp;#8217;, while having low tf-idf scores within each document, are so frequent that when averaged over all documents they would otherwise easily dominate all other&amp;nbsp;terms. &lt;/p&gt;
&lt;p&gt;Calling this function with grp_ids=None, gives us the most important words across the whole corpus. Here are the top&amp;nbsp;15:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;                   feature     tfidf
0                    funny  0.003522
1                   sports  0.003491
2                 swimsuit  0.003456
3                  fashion  0.003337
4                       si  0.002972
5                    video  0.002700
6           insidershealth  0.002472
7       sports illustrated  0.002329
8   insidershealth article  0.002294
9              si swimsuit  0.002258
10                    html  0.002216
11             illustrated  0.002208
12              allrecipes  0.002171
13                 article  0.002144
14                   humor  0.002143
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There is no obvious pattern here, beyond the fact that sports, health, fashion and humour seem to characterize the majority of articles. What might be more interesting, though, is to separately consider groups of documents falling into a particular category. For example, let&amp;#8217;s calculate the mean tf-idf scores depending on a document&amp;#8217;s class&amp;nbsp;label:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;top_feats_by_class&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_tfidf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Return a list of dfs, where each df holds top_n features and their mean tfidf value&lt;/span&gt;
&lt;span class="sd"&gt;        calculated across documents with the same class label. &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;dfs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;ids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;feats_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;top_mean_feats&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;min_tfidf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;min_tfidf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top_n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;top_n&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;feats_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;
        &lt;span class="n"&gt;dfs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feats_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;dfs&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This function uses the previously defined functions to return a list of DataFrames, one per document class, and each containing the top n features. Instead of printing them out as a table, let&amp;#8217;s create a figure in&amp;nbsp;matplotlib:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/tfidf/tfidf-features.png" alt="Tfidf per class"/&gt;&lt;/p&gt;
&lt;p&gt;This looks much more interesting! Web pages classified as ephemeral (class label=0) seem to fall mostly into the categories of photos and videos of sports illustrated models (the abbreviation si refers to the magazine also), or otherwise articles related to fashion, humor or technology. Those pages classified as evergreen, in contrast, seem to relate mostly to health, food and recipes in particular (class label=1). This also includes, of course, the first evergreen article we identified above (about fruits and vitamins preventing flu). Some overlap also exists, however. The word &amp;#8216;funny&amp;#8217; appears in the top 25 tf-idf tokens for both categories, as does the name of the allrecipes website. Together this tells us a little bit about the features (the presence of tokens) on the basis of which a trained classifier may categorize pages as belonging to one class or the&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;For reference, here is the function to plot the tf-idf values using&amp;nbsp;matplotlib:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;plot_tfidf_classfeats_h&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dfs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Plot the data frames returned by the function plot_tfidf_classfeats(). &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;facecolor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dfs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dfs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dfs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;spines&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;top&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_visible&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;spines&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;right&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_visible&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_frame_on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_xaxis&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tick_bottom&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_yaxis&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tick_left&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Mean Tf-Idf Score&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labelpad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;label = &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ticklabel_format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;sci&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scilimits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;barh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tfidf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;align&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;center&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;#3F5D7D&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_yticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylim&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;yticks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_yticklabels&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots_adjust&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bottom&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.09&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.97&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;left&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;top&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.52&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As a last step, let&amp;#8217;s plot the top tf-df features for webpages misclassified by our full text-classification pipeline, using the indices of false positive and negatives identified&amp;nbsp;above:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/tfidf/misclf-features.png" alt="Top features for misclassified pages."/&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately we can at best get some initial hints as to the reason for misclassification from this figure. Our false positives are very similar to the true positives, in that they are also mostly health, food and recipe pages. One clue may be the presence of words like christmas and halloween in these pages, which may indicate that their content is specific to a particular season or date of the year, and therefore not necessarily recommendable at other times. The picture is similar for the false negatives, though in this case there is nothing at all indicating any difference with true positives. One would probably have to dig a little deeper into individual cases here to see how they may&amp;nbsp;differ.&lt;/p&gt;
&lt;h3&gt;Final&amp;nbsp;thoughts&lt;/h3&gt;
&lt;p&gt;This post barely scratches the surface of how one might go about analyzing the results of a tf-idf transformation in python, and is directed primarily at people who may use it as a black box algorithm without necessarily knowing what&amp;#8217;s inside. There may be many other, and probably better ways of going about this. I nevertheless think it&amp;#8217;s a useful tool to have around. Note that a similar analysis of top features amongst a group of documents could be applied also after clustering the documents first. One could then use the cluster index, instead of the class label, to group documents and plot their top tf-idf tokens to get further insight about the specific characteristics of each&amp;nbsp;cluster.&lt;/p&gt;</summary><category term="sklearn"></category><category term="python"></category><category term="classification"></category><category term="tf-idf"></category><category term="kaggle"></category><category term="text"></category></entry><entry><title>Pipelines for text classification in scikit-learn</title><link href="https://buhrmann.github.io/sklearn-pipelines.html" rel="alternate"></link><updated>2015-06-17T00:00:00+02:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2015-06-17:sklearn-pipelines.html</id><summary type="html">&lt;p&gt;&lt;a href="http://scikit-learn.org"&gt;Scikit-learn&amp;#8217;s&lt;/a&gt; &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"&gt;pipelines&lt;/a&gt; provide a useful layer of abstraction for building complex estimators or classification models. Its purpose is to aggregate a number of data transformation steps, and a model operating on the result of these transformations, into a single object that can then be used in place of a simple estimator. This allows for the one-off definition of complex pipelines that can be re-used, for example, in cross-validation functions, grid-searches, learning curves and so on. I will illustrate their use, and some pitfalls, in the context of a kaggle text-classification&amp;nbsp;challenge.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/pipelines/stumbleupon_evergreen.jpg" alt="StumbleUpon Evergreen" width="1000"/&gt;&lt;/p&gt;
&lt;h3&gt;The&amp;nbsp;challenge&lt;/h3&gt;
&lt;p&gt;The goal in the &lt;a href="https://www.kaggle.com/c/stumbleupon"&gt;StumbleUpon Evergreen&lt;/a&gt; classification challenge is the prediction of whether a given web page is relevant for a short period of time only (ephemeral) or can be recommended still a long time after initial discovery&amp;nbsp;(evergreen). &lt;/p&gt;
&lt;p&gt;Each webpage in the provided dataset is represented by its html content as well as additional meta-data, the latter of which I will ignore here for simplicity. Instead I will focus on the use of pipelines to 1) transform text data into a numerical form appropriate for machine learning purposes, and 2) for creating ensembles of different classifiers to (hopefully) improve prediction accuracy (or at least its&amp;nbsp;variance). &lt;/p&gt;
&lt;h3&gt;Text&amp;nbsp;transformation&lt;/h3&gt;
&lt;p&gt;A useful tool for the representation of text in a machine learning context is the so-called &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;tf-idf&lt;/a&gt; transformation, short for &amp;#8220;term frequency–inverse document frequency&amp;#8221;. The idea is simple. Each word in a document is represented by a number that is proportional to its frequency in the document, and inversely proportional to the number of documents in which it occurs. Very common words, such as &amp;#8220;a&amp;#8221; or &amp;#8220;the&amp;#8221;, thereby receive heavily discounted tf-df scores, in contrast to words that are very specific to the document in question. Scikit-learn provides a TfidfVectorizer class, which implements this transformation, along with a few other text-processing options, such as removing the most common words in the given language (stop words). The result is a matrix with one row per document and as many columns as there are different words in the dataset&amp;nbsp;(corpus).&lt;/p&gt;
&lt;h3&gt;Pipelines&lt;/h3&gt;
&lt;p&gt;In few cases, however, is the vectorization of text into numerical values as simple as applying tf-idf to the raw data. Often, the relevant text to be converted needs to be extracted first. Also, the tf-idf transformation will usually result in matrices too large to be used with certain machine learning algorithms. Hence dimensionality reduction techniques are often applied too. Manually implementing these steps everytime text needs to be transformed quickly becomes repetitive and tedious. It needs to be done for the training as well as test set. Ideally, when using cross-validation to assess one&amp;#8217;s model, the transformation needs to be applied separately in each fold, particularly when feature selection (dimensionality reduction) is involved. If care is not taken, information about the whole dataset otherwise leaks into supposedly independent evaluations of individual&amp;nbsp;folds.&lt;/p&gt;
&lt;p&gt;Pipelines help reduce this repetition. What follows is an example of a typical vectorization&amp;nbsp;pipeline:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_vec_pipe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_comp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reducer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svd&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Create text vectorization pipeline with optional dimensionality reduction. &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="n"&gt;tfv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TfidfVectorizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;min_df&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;strip_accents&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;unicode&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;analyzer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;word&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;token_pattern&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;r&amp;#39;\w{1,}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ngram_range&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="n"&gt;use_idf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;smooth_idf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sublinear_tf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c"&gt;# Vectorizer&lt;/span&gt;
    &lt;span class="n"&gt;vec_pipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;col_extr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;JsonFields&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;body&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;url&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])),&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;squash&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Squash&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;vec&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tfv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c"&gt;# Reduce dimensions of tfidf&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num_comp&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;reducer&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;svd&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;vec_pipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dim_red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TruncatedSVD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_comp&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;reducer&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;kbest&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;vec_pipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dim_red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SelectKBest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chi2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_comp&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
        &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;reducer&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;percentile&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;vec_pipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;dim_red&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SelectPercentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f_classif&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;num_comp&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

        &lt;span class="n"&gt;vec_pipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;norm&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Normalizer&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vec_pipe&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, we first create an instance of the tf-idf vectorizer (for its parameters see &lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"&gt;documentation)&lt;/a&gt;. We then create a list of tuples, each of which represents a data transformation step and its name (the latter of which is required, e.g., for identifying individual transformer parameters in a grid search). The first two are custom transformers and the last one our vectorizer. The first transformer (&amp;#8220;JsonFields&amp;#8221;), for example, extracts a particular column from the dataset, in this case the first (0-indexed), interprets its content as json-encoded text, and extracts the json fields with the keys &amp;#8216;title&amp;#8217;, &amp;#8216;body&amp;#8217; and &amp;#8216;url&amp;#8217;. The corresponding values are concatenated into a single string per row in the dataset. The result is a new transformed dataset with a single column containing the extracted text, which can then be processed by the vectorizer. After the vectorization step, an optional dimensionality reduction is added to the list of transformations before the final pipeline is constructed and&amp;nbsp;returned.&lt;/p&gt;
&lt;h4&gt;Transformers&lt;/h4&gt;
&lt;p&gt;Custom transformers such as those above are easily created by subclassing from scikit&amp;#8217;s &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html"&gt;TransformerMixin&lt;/a&gt;. This base class exposes a single fit_transform() function, which in turn calls (to be implemented) fit() and transform() functions. For transformers that do not require fitting (no internal parameters to be selected based on the dataset), we can create a simpler base class that only needs the transform function to be&amp;nbsp;implemented:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Transformer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TransformerMixin&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Base class for pure transformers that don&amp;#39;t need a fit method &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;fit_params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;transform_params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;deep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With this in place, the JsonFields transformer looks like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;JsonFields&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Transformer&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Extract json encoded fields from a numpy array. Returns (iterable) numpy array so it can be used as input to e.g. Tdidf &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fields&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;column&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fields&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fields&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;join&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;deep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fields&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fields&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;transform_params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_np&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vectorize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;extract_json&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;excluded&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;fields&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])(&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fields&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fields&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;JsonFields itself encapsulates another custom transformer (Select), used here to keep the specification of pipelines concise. It could also have been used as a prior step in the definition of the pipeline. The Select transformer does nothing other than extracting a number of specified columns from a&amp;nbsp;dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Transformer&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;  Extract specified columns from a pandas df or numpy array &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_np&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_np&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;to_np&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;deep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;to_np&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_np&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;transform_params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;allint&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; 
                &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; 
                 &lt;span class="nb"&gt;all&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;allint&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ix&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="nb"&gt;all&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;isinstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
                &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Select error: mixed or wrong column type.&amp;quot;&lt;/span&gt;
                &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;

            &lt;span class="c"&gt;# to numpy ?&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_np&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unsquash&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unsquash&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;res&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This transformer is slightly more complicated than strictly necessary as it allows for selection of columns by index or name in the case of a pandas&amp;nbsp;DataFrame.&lt;/p&gt;
&lt;p&gt;You may have noticed the use of the function unsquash() and the Transformer Squash in the first definition of the pipeline. This is an unfortunate but apparently required part of dealing with numpy arrays in scikit-learn. The problem is this. One may want, as part of the transform pipeline, to concatenate features from different sources into a single feature matrix. One may do this using numpy&amp;#8217;s &lt;a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html"&gt;hstack&lt;/a&gt; function or scikit&amp;#8217;s built-in &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html"&gt;FeatureUnion&lt;/a&gt; class. However, both only operate on feature columns of dimensionality (n,1). So, for this purpose custom transformers should always return single-column &amp;#8220;2-dimensional&amp;#8221; arrays or matrices. Scikit&amp;#8217;s &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TfidfVectorizer&lt;/a&gt;, on the other hand, only operates on arrays of dimensionality (n,), i.e. on truly one-dimensional arrays (and probably pandas Series). As a result, when working with multiple feature sources, one of them being vectorized text, it is necessary to convert back and forth between the two ways of representing a feature column. For example by&amp;nbsp;using&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;for conversion from (n,1) to (n,)&amp;nbsp;or&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;for the other direction. The Squash (and Unsquash) class used above simply wraps this functionality for use in pipelines. For these and some other Transformers you may find useful check &lt;a href="https://github.com/synergenz/kaggle/blob/master/stumble/python/transform.py"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Ensembles&lt;/h3&gt;
&lt;p&gt;The last step in a Pipeline is usually an estimator or classifier (unless the pipeline is only used for data transformation). However, a simple extension allows for much more complex ensembles of models to be used for classification. One way to do this flexibly is to first create a &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html"&gt;FeatureUnion&lt;/a&gt; of different models, in which the predictions of individual models are treated as new features and concatenated into a new feature matrix (one column per predictor). An ensemble prediction can then be made simply by averaging the predictions (or using a majority vote), or by using the predictions as inputs to a final predictor, for&amp;nbsp;example.&lt;/p&gt;
&lt;p&gt;For the creation of a FeatureUnion of models, we require the individual models to return their predictions in their transform calls (since the fitting of a Pipeline only calls the fit and transform functions for all but the last step, but not the predict function). We hence need to turn a predictor into a transformer, wich can be done using a wrapper such as&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;ModelTransformer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TransformerMixin&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Use model predictions as transformed data. &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;probs&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_params&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;deep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;transform_params&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;Xtrf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)[:,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;Xtrf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;unsquash&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Xtrf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With this in place we may build a FeatureUnion-based ensemble like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;build_ensemble&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Build an ensemble as a FeatureUnion of ModelTransformers and a final estimator using their&lt;/span&gt;
&lt;span class="sd"&gt;    predictions as input. &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;models&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;model_transform&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;ModelTransformer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;FeatureUnion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;FeatureUnion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;estimator&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We are now in a position to create a rather complex text-classification pipeline. For example, 
one pipeline I&amp;#8217;ve built for the kaggle competition trains a logistic regression on the result of the tf-idf vectorization, then combines the prediction with those from three different models trained on a dimensionality-reduced form of the&amp;nbsp;tf-idf:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_custom_pipe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_comp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Create complex text vectorization pipeline. &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="c"&gt;# Get non-dim-reduced vectorizer&lt;/span&gt;
&lt;span class="n"&gt;pipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_vec_pipe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_comp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# Add a logit on non-reduced tfidf, and ensemble on reduced tfidf&lt;/span&gt;
&lt;span class="n"&gt;clfs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;rf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;sgd&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;gbc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;pipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;union&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;FeatureUnion&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;logit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ModelTransformer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;build_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;logit&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;featpipe&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Pipeline&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svd&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TruncatedSVD&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_comp&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;svd_norm&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Normalizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;red_featunion&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;build_ensemble&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;build_classifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;clfs&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="p"&gt;]))&lt;/span&gt;
    &lt;span class="p"&gt;]))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;pipe&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;steps&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;ensemblifier&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;pipe&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This function takes as input the final classifier that should be trained on the component predictions. One may, for example, use a built-in classifier (say another logistic regression), in
which case one ends up with a &lt;a href="https://en.wikipedia.org/wiki/Ensemble_learning#Stacking"&gt;stacked ensemble&lt;/a&gt;. Or one may simply average or take the majority vote of the individual prediction, in which case one is simply creating a kind of &lt;a href="http://www.scholarpedia.org/article/Ensemble_learning#Ensemble_combination_rules"&gt;combiner&lt;/a&gt;. For the latter there is no built-in class in scikit-learn, but one can easily be&amp;nbsp;created:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;EnsembleBinaryClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseEstimator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ClassifierMixin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;TransformerMixin&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Average or majority-vote several different classifiers. Assumes input is a matrix of individual predictions, such as the output of a FeatureUnion of ModelTransformers [n_samples, n_predictors]. Also see http://sebastianraschka.com/Articles/2014_ensemble_classifier.html.&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Predict (weighted) probabilities &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="n"&gt;probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;average&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column_stack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;probs&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39; Predict class labels. &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mode&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;average&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;binarize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)[:,[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]],&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;binarize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply_along_axis&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bincount&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;arr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For prediction of class probabilities this model simply returns a (possibly weighted) average of individual predictions. For compatibility with some of scikit-learn&amp;#8217;s built-in functionality I return the probabilities both for negative and positive classes (scikit expects the latter in the second column). For the prediction of class labels, the model either uses a thresholded version of the averaged probabilities, or a majority vote directly on thresholded individual predictions (it may be useful to allow for specification of the threshold as well). In either case, the hope is that the combined predictions of several classifiers will reduce the variance in prediction accuracy when compared to a single model only. Supplying an instance of this class to the above get_custom_pipe() function completes our relatively complex&amp;nbsp;pipeline.&lt;/p&gt;
&lt;h3&gt;Use of&amp;nbsp;Pipelines&lt;/h3&gt;
&lt;p&gt;Though requiring some additional work in the beginning to wrap custom data transformations in their own classes, once a pipeline has been defined, it can be used anywhere in scikit-learn in place of a simple estimator or&amp;nbsp;classifier.&lt;/p&gt;
&lt;p&gt;For example, estimating the performance of the pipeline using cross-validation on training data is as simple&amp;nbsp;as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cross_validation&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;roc_auc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;One advantage is that this applies all data transformations (including any feature selection steps) independently on each fold, without leaking information from the whole dataset. Note though, that there are kinds of data mangling or preprocessing that are better done once for the whole&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;Equally easily predictions are created on new&amp;nbsp;data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_new&lt;/span&gt;&lt;span class="p"&gt;)[:,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And here is a grid search to automatically determine the best parameters of models used in the pipeline (using cross-validation&amp;nbsp;internally):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;gs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;roc_auc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here the only subtelty involves specification of the parameter grid (the parameter values to be tested). Since our pipelines can form a complex hierarchy, the parameter names of individual models need to refer to the name of the model in the pipeline. For example, if the pipeline contains a logistic regression step, named &amp;#8216;logit&amp;#8217;, then the values to be tested for the model&amp;#8217;s &amp;#8216;C&amp;#8217; parameter need to be supplied&amp;nbsp;as&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;logit__C&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;i.e. using the model name followed by a double underscore followed by the parameter&amp;nbsp;name. &lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I hope there is some useful information here. For the code I used to predict StumbleUpon pages see &lt;a href="https://github.com/synergenz/kaggle/tree/master/stumble/python"&gt;here on github&lt;/a&gt;. Somewhat disappointingly though, the complex pipeline in this case doesn&amp;#8217;t perform significantly better than a simple tf-idf followed by logistic regression (without the ensemble). This may be due to the small size of the data set, the fact that the different models in the ensemble all fail in similar ways, or a range of other reasons. In any case, also check &lt;a href="http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"&gt;Zac Stewart&amp;#8217;s blog post&lt;/a&gt; for another introduction to Pipelines. And in a follow-up post I will show some ways of analysing the results of a tf-idf in&amp;nbsp;scikit-learn.&lt;/p&gt;
&lt;h3&gt;Afterword&lt;/h3&gt;
&lt;p&gt;As mentioned in the beginning a Pipeline instance may also be used with scikit-learn&amp;#8217;s validation and learning curve. Here is the learning curve for the above&amp;nbsp;pipeline:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/pipelines/lc_ensemble_roc.png" alt="Ensemble learning curve" width="750"/&gt;&lt;/p&gt;
&lt;p&gt;The complex pipeline is certainly not suffering from high bias, as that would imply a higher error on the training set. From the gap between training and test error it rather seems like the model may exhibit too much variance, i.e. overfitting on the training folds. This makes sense both because our model is rather complex, and also because the size of the whole training data is relatively small (less than 8000 documents, compare that to the number of features produced by the tf-df, which can run into several tens of thousands without dimensionality reduction). Collection of more data would thus be one way to try and improve performance here (and it might also be useful to investigate different forms of regularization to avoid overfitting. Interestingly though, grid-search of the logistic regression led to best results without regularization). On the other hand, test error does not seem to be decreasing much with increasing size of the training set, indicating perhaps some inherent unpredictability in the data (some comments in the forum e.g. indicate that the class labels seem to have been assigned somewhat&amp;nbsp;inconsistently).&lt;/p&gt;</summary><category term="sklearn"></category><category term="python"></category><category term="classification"></category><category term="tf-idf"></category><category term="kaggle"></category></entry><entry><title>Sql to excel</title><link href="https://buhrmann.github.io/xql.html" rel="alternate"></link><updated>2015-02-02T00:00:00+01:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2015-02-02:xql.html</id><summary type="html">&lt;p&gt;A little python tool to execute an sql script (postgresql in this case, but should be easily modifiable for mysql etc.) and store the result in a csv or excel (xls&amp;nbsp;file):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;Executes an sql script and stores the result in a file.&lt;/span&gt;
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;os&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;subprocess&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;csv&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;xlwt&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Workbook&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sql_to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sql_fnm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;csv_fnm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Write result of executing sql script to txt file&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sql_fnm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sql_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sql_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;COPY (&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;) TO STDOUT WITH CSV HEADER&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;cmd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;psql -c &amp;quot;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;cmd&lt;/span&gt;

        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;check_output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cmd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_fnm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;csv_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;csv_writer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csv_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitlines&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;csv_writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writerow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;sql_to_xsl&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sql_fnm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xls_fnm&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot; Write result of executing sql script to xls file&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sql_fnm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;sql_file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sql_file&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;COPY (&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;) TO STDOUT WITH CSV HEADER&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;cmd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;psql -c &amp;quot;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;quot;&amp;#39;&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;cmd&lt;/span&gt;

        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;check_output&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cmd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;book&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Workbook&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;sheet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;book&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_sheet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Sheet 1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;rows&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitlines&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;row_idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;col_idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
                &lt;span class="n"&gt;sheet&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row_idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col_idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;val&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;book&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xls_fnm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;sqlfnm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;outfnm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;sql_to_xsl&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sqlfnm&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;outfnm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="sql"></category><category term="python"></category></entry><entry><title>Retrieving your Google Scholar data</title><link href="https://buhrmann.github.io/scholar.html" rel="alternate"></link><updated>2015-01-27T00:00:00+01:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2015-01-27:scholar.html</id><summary type="html">&lt;p&gt;For my interactive &lt;span class="caps"&gt;CV&lt;/span&gt; I decided to try not only to automate the creation of a bibliography of my publications, but also to extend it with a citation count for each paper, which Google Scholar happens to keep track of. Unfortunately there is no Scholar &lt;span class="caps"&gt;API&lt;/span&gt;. But I figured since my own profile is based on data I essentially donated to Google, it is only fair that I can have access to it too. Hence I wrote a little scraper that iterates over the publications in my Scholar profile, extracts all citations, and bins them per year. That way I can track how many citations each paper had over time. Like this graph, for&amp;nbsp;example:&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/scholar/citations.png" /&gt;
&lt;/figure&gt;

&lt;p&gt;The source code for the scraper is available &lt;a href="https://github.com/synergenz/datawerk/blob/master/theme/static/py/citations.py"&gt;here on github&lt;/a&gt;. It uses &lt;a href="http://www.crummy.com/software/BeautifulSoup/"&gt;Beautiful Soup&lt;/a&gt; for the html parsing. Note that it deliberately runs very slowly (waits a random amount of time between page loads), because otherwise Google will notice the scraping attempt and will lock your &lt;span class="caps"&gt;IP&lt;/span&gt; out, at least for a while (it happened to me initially). Feel free to use it to liberate your own Scholar&amp;nbsp;data.&lt;/p&gt;</summary><category term="scraping"></category><category term="python"></category></entry><entry><title>Tag graph plugin for Pelican</title><link href="https://buhrmann.github.io/tag-graph.html" rel="alternate"></link><updated>2015-01-27T00:00:00+01:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2015-01-27:tag-graph.html</id><summary type="html">&lt;p&gt;On my front page I display a sort of sitemap for my blog. Since the structure of the site is not very hierarchical, I decided to show pages and posts as a graph along with their tags. To do so, I created a mini plugin for the Pelican static blog engine. The plugin is essentially a sort of callback that gets executed when the engine has generated all posts and pages from their markdown files. I then simply take the results and write them out in a json format that d3.js understands (a list of nodes and a list of edges indexed on node positions in their&amp;nbsp;list).&lt;/p&gt;
&lt;p&gt;The plugin&amp;#8217;s source code is available &lt;a href="https://github.com/synergenz/datawerk/blob/master/plugins/tag_graph.py"&gt;here on github&lt;/a&gt;. It you&amp;#8217;re interested in the Pelican blog engine, check it out &lt;a href="http://blog.getpelican.com/"&gt;here&lt;/a&gt;.&lt;/p&gt;</summary><category term="graph"></category><category term="python"></category><category term="visualization"></category><category term="d3"></category></entry><entry><title>C. elegans connectome explorer</title><link href="https://buhrmann.github.io/elegans.html" rel="alternate"></link><updated>2015-01-15T00:00:00+01:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2015-01-15:elegans.html</id><summary type="html">&lt;p&gt;I&amp;#8217;ve build a prototype &lt;a href="https://elegans.herokuapp.com"&gt;visual exploration tool&lt;/a&gt; for the connectome of c. elegans. The data describing the worm&amp;#8217;s neural network is preprocessed from publicly available information and stored as a graph database in neo4j. The d3.js visualization then fetches either the whole network or a subgraph and displays it using a force-directed layout (for&amp;nbsp;now).&lt;/p&gt;
&lt;figure&gt;
&lt;a href="https://elegans.herokuapp.com"&gt;&lt;img src="/images/elegans/elegans.png" alt="Elegans"/&gt;&lt;/a&gt;
&lt;/figure&gt;</summary><category term="visualization"></category><category term="d3"></category><category term="neo4j"></category><category term="python"></category><category term="graph"></category><category term="nosql"></category></entry><entry><title>Movement control without internal models</title><link href="https://buhrmann.github.io/spinal-coordination.html" rel="alternate"></link><updated>2014-11-13T00:00:00+01:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2014-11-13:spinal-coordination.html</id><summary type="html">&lt;p&gt;My latest paper has just been published by Frontiers in Computational Neuroscience and can be accessed free of charge &lt;a href="http://journal.frontiersin.org/Journal/10.3389/fncom.2014.00144/abstract"&gt;here&lt;/a&gt;. It concerns the question of whether we need internal models (simulations) in order to control our movements, or whether our body and the lower-level neural circuits innervating it provide some control &amp;#8220;for&amp;nbsp;free&amp;#8221;.&lt;/p&gt;
&lt;p&gt;From the&amp;nbsp;abstract:&lt;/p&gt;
&lt;blockquote&gt;
The dynamic interaction of limb segments during movements that involve multiple joints creates torques in one joint due to motion about another. Evidence shows that such interaction torques are taken into account during the planning or control of movement in humans. Two alternative hypotheses could explain the compensation of these dynamic torques. One involves the use of internal models to centrally compute predicted interaction torques and their explicit compensation through anticipatory adjustment of descending motor commands. The alternative, based on the equilibrium-point hypothesis, claims that descending signals can be simple and related to the desired movement kinematics only, while spinal feedback mechanisms are responsible for the appropriate creation and coordination of dynamic muscle forces. Partial supporting evidence exists in each case. However, until now no model has explicitly shown, in the case of the second hypothesis, whether peripheral feedback is really sufficient on its own for coordinating the motion of several joints while at the same time accommodating intersegmental interaction torques. Here we propose a minimal computational model to examine this question. Using a biomechanics simulation of a two-joint arm controlled by spinal neural circuitry, we show for the first time that it is indeed possible for the neuromusculoskeletal system to transform simple descending control signals into muscle activation patterns that accommodate interaction forces depending on their direction and magnitude. This is achieved without the aid of any central predictive signal. Even though the model makes various simplifications and abstractions compared to the complexities involved in the control of human arm movements, the finding lends plausibility to the hypothesis that some multijoint movements can in principle be controlled even in the absence of internal models of intersegmental dynamics or learned compensatory motor signals.
&lt;/blockquote&gt;

&lt;figure&gt;
&lt;a href="http://journal.frontiersin.org/Journal/10.3389/fncom.2014.00144/abstract"&gt;&lt;img src="http://www.frontiersin.org/files/Articles/103472/fncom-08-00144-HTML/image_m/fncom-08-00144-g001.jpg" style="width: 400px"/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;Get it here: &lt;a href="http://journal.frontiersin.org/Journal/10.3389/fncom.2014.00144/abstract"&gt;Frontiers in Cognition | Learning to perceive in the sensorimotor approach: Piaget’s theory of equilibration interpreted&amp;nbsp;dynamically&lt;/a&gt;&lt;/p&gt;</summary><category term="smcs"></category></entry><entry><title>Interest rates on P2P loans</title><link href="https://buhrmann.github.io/p2p-loans.html" rel="alternate"></link><updated>2014-11-06T18:27:37+01:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2014-11-06:p2p-loans.html</id><summary type="html">&lt;p&gt;Like other peer-to-peer services, the &lt;a href="https://www.lendingclub.com/home.action"&gt;Lending Club&lt;/a&gt; aims to directly connect producers and consumers, or in this case borrowers and lenders, by cutting out the middleman. Borrowers apply for loans online and provide details about the desired loan as well their financial status (such as &lt;span class="caps"&gt;FICO&lt;/span&gt; score). Lenders use the information provided to choose which loans to invest in. The Lending Club, finally, uses a &lt;a href="https://www.lendingclub.com/public/how-we-set-interest-rates.action"&gt;proprietary algorithm&lt;/a&gt; to determine the interest charged on an applicant’s loan. Given the secret nature of this process, a borrower or lender might be interested in which variables, beside the obvious &lt;span class="caps"&gt;FICO&lt;/span&gt; credit score, influence the final interest rate and how strong this influence is. It is the aim of this analysis to identify such&amp;nbsp;associations.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/p2ploans/lendingclub.jpg" alt="Fico score analysis"/&gt;
&lt;/figure&gt;

&lt;h3&gt;Methods&lt;/h3&gt;
&lt;p&gt;Data about peer-to-peer loans issued through the Lending Club was provided by the Data Analysis class on Coursera (&lt;a href="https://spark-public.s3.amazonaws.com/dataanalysis/loansData.rda"&gt;file here&lt;/a&gt;). The set used in this analysis was downloaded on the 17th of February, 2013. After accounting for missing values in the data, an exploratory analysis was performed to identify variables that required transformation prior to statistical modeling (boxplots, histograms etc.), and to find a subset of variables to be used in a regression model relating interest rate to an applicant’s &lt;span class="caps"&gt;FICO&lt;/span&gt; score (using correlation analysis and &lt;span class="caps"&gt;PCA&lt;/span&gt;). The statistical model itself was a simple linear multivariate regression [1]. Since most variables were not normally distributed, results were also compared to robust estimation&amp;nbsp;techniques.&lt;/p&gt;
&lt;p&gt;To reproduce the results of this report the complementary R script (provided on github) can be run with the corresponding data&amp;nbsp;file.&lt;/p&gt;
&lt;h3&gt;Summary of&amp;nbsp;data&lt;/h3&gt;
&lt;p&gt;Besides a loan’s interest rate, the data set analyzed here contains information about the amount requested and the amount eventually funded by investors (1000$-35000$), the length of the loan (36 or 60 months), and its purpose (the majority went towards debt consolidation or to pay off credit cards). Information about the applicant included his or her duration of employment, state of residence, information about home ownership (the majority either renting or having a mortgage), debt to income ratio, monthly income, &lt;span class="caps"&gt;FICO&lt;/span&gt; score, number of open credit lines, revolving credit balance, and the number of inquiries made in the last 6 months. In total the set included information about 2500 loans, out of which 79 contained missing data (most in employment length). Given this relatively small number, and the relatively large set, the corresponding data was simply&amp;nbsp;discarded.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;FICO&lt;/span&gt; scores were transformed from a 38-level factor (the lowest being [640-644] and the highest [830-834]) to mean values for each range. The number of recent loan inquiries showed a Poisson- or exponential-like distribution. Since the kinds of analyses performed on the data&amp;#8212;such as linear regression&amp;#8212;might be sensitive to data not being normally distributed, we created a new factor variable with only two levels: 0 inquiries (1213 data points) and 1-9 inquiries (1208 data&amp;nbsp;points).&lt;/p&gt;
&lt;p&gt;Histograms of quantitative variables indicated that the distributions of monthly incomes, &lt;span class="caps"&gt;FICO&lt;/span&gt; score, revolving credit balance and number of open credit lines were not normal, and more specifically right-skewed to different degrees. While log&lt;sub&gt;10&lt;/sub&gt; transformation generally brought the distributions closer to normal (at least visually), the results of a Shapiro test [2] indicated that the null-hypothesis of normal distribution still needed to be rejected. Inspection of normal &lt;span class="caps"&gt;QQ&lt;/span&gt;-plots, however, indicated fairly normal distributions across a wide range centered at the means of most variables. While removal of outliers (e.g. monthly incomes greater than 4 or 5 standard deviations from mean) improved normality, in the remaining analysis the whole data set was&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;The histogram of the interest rate variable showed a bimodal distribution, suggestive of a superposition of two separate distributions. While none of the factors separated these, when interest rate histograms were plotted for different ranges of the &lt;span class="caps"&gt;FICO&lt;/span&gt; variable (by cutting the latter at quantiles) normal-like distributions could be identified. The shift in interest rate mean as a function of &lt;span class="caps"&gt;FICO&lt;/span&gt; score indicates that for very high &lt;span class="caps"&gt;FICO&lt;/span&gt; scores, relatively low interest rates become disproportionally more&amp;nbsp;probable.&lt;/p&gt;
&lt;h3&gt;Exploratory&amp;nbsp;analysis&lt;/h3&gt;
&lt;p&gt;Since intuitively we know that interest rate should correlate strongly with &lt;span class="caps"&gt;FICO&lt;/span&gt; scores, we examined the associations between the two, as well as those between either and third&amp;nbsp;variables.&lt;/p&gt;
&lt;p&gt;Box plots of interest rate and &lt;span class="caps"&gt;FICO&lt;/span&gt; scores by factor variables showed that only two factors seemed to influence these variables. Loan length had a significant effect on interest rate (p-value of t-test &amp;lt; 2.2e&lt;sup&gt;-16&lt;/sup&gt;, effect size of 4.24%), but not on &lt;span class="caps"&gt;FICO&lt;/span&gt; scores (p-value=0.41). The number of inquiries (two-level factor) had significant effects on the means of both variables (p &amp;lt; 2.2e&lt;sup&gt;-16&lt;/sup&gt; and p = &lt;sup&gt;1.6e-5&lt;/sup&gt;), but the effect size was interesting only in the case of interest rate&amp;nbsp;(1.6%).&lt;/p&gt;
&lt;p&gt;Using their correlation matrix, as well as pair-wise scatter plots with linear models fitted, we aimed to reduce the set of quantitative variables by discarding those (except interest rate and &lt;span class="caps"&gt;FICO&lt;/span&gt; score) that showed high correlations amongst&amp;nbsp;themselves.&lt;/p&gt;
&lt;p&gt;First we eliminated the amount funded by investors. This is justified as a) we are interested here only in the resulting interest rate, not the size of the eventual loans, and b) for most applications the eventual loan equalled the amount requested, i.e. a strong linear relationship (with slope 1) existed between the two (linear regression resulted in adjusted coefficient of determination R&lt;sup&gt;2&lt;/sup&gt;=0.94, i.e. approx. 94% variance&amp;nbsp;explained).&lt;/p&gt;
&lt;p&gt;The correlation matrix further showed an association between monthly income and loan amount requested (Pearson correlation R = 0.47; linear regression with adj. R&lt;sup&gt;2&lt;/sup&gt; = 0.23 and p &amp;lt; 2.2e&lt;sup&gt;-16&lt;/sup&gt;). This is not surprising as we would expect people with higher incomes to be able to afford larger loans. To avoid confounders we also rejected monthly income as an independent variable in further&amp;nbsp;models.&lt;/p&gt;
&lt;p&gt;Of the remaining four covariates (excluding &lt;span class="caps"&gt;FICO&lt;/span&gt;), three are related to the applicant&amp;#8217;s current debt. In particular, correlation analysis revealed that the number of open credit lines correlates (relatively weakly) with both debt to income ratio (R = 0.38) and revolving credit balance (0.34). We therefore use only the first to stand in for the overall debt&amp;nbsp;burden.&lt;/p&gt;
&lt;p&gt;In summary, we consider as quantitative covariates in the statistical model only the amount requested, open credit lines and the &lt;span class="caps"&gt;FICO&lt;/span&gt; score. A &lt;span class="caps"&gt;PCA&lt;/span&gt; analysis confirms that these are indeed relatively independent (see Figure 1). The variable for open credit lines is aligned mostly with the first principal component, &lt;span class="caps"&gt;FICO&lt;/span&gt; score with the second, and the amount requested falls in between the other two (i.e. they are not orthogonal, but far from parallel in the space of the principal&amp;nbsp;components).&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/p2ploans/pca.png" alt="PCA"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 1: Biplot of principal component analysis of the data set containing only the selected three covariates. Data points (color-coded by the loan amount factor) and covariate directions are plotted in the space of the first two principal components. Ellipses contain with 68% probability the points belonging to each level of the loan amount. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In addition we see that loans of different amount are slightly but significantly separated in &lt;span class="caps"&gt;PCA&lt;/span&gt; space. Longer loans can usually be found in the direction of larger amounts and higher &lt;span class="caps"&gt;FICO&lt;/span&gt; score, for example. A &lt;span class="caps"&gt;PCA&lt;/span&gt; using all covariates (not shown) confirms our above finding that monthly income and the amount requested largely capture the same variance in the data (the projections in &lt;span class="caps"&gt;PCA&lt;/span&gt; space are almost parallel), further justifying our exclusion of the former. Equally, the number of open credit lines captures almost the variance as the revolving credit balance. Finally, Debt to income ratio in &lt;span class="caps"&gt;PCA&lt;/span&gt; space is almost parallel but points in the opposite direction from &lt;span class="caps"&gt;FICO&lt;/span&gt; score, indicating that the former is probably a strong determining factor in the computation of the latter and can therefore also be excluded without losing much of the&amp;nbsp;variance.&lt;/p&gt;
&lt;p&gt;The exploratory analysis can be summarised by the following relationships between interest rate and retained&amp;nbsp;covariates:&lt;/p&gt;
&lt;p&gt;Factors:&lt;ul&gt;
&lt;li&gt;The longer the loan, the higher the&amp;nbsp;interest.&lt;/li&gt;
&lt;li&gt;The more often an applicant has recently inquired about a loan, the higher its&amp;nbsp;interest.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
&lt;p&gt;Quantitative:&lt;ul&gt;
&lt;li&gt;The larger the loan, the higher its&amp;nbsp;interest.&lt;/li&gt;
&lt;li&gt;The smaller the &lt;span class="caps"&gt;FICO&lt;/span&gt; score, the higher the&amp;nbsp;interest.&lt;/li&gt;
&lt;li&gt;The higher the number of open credit lines, the higher the&amp;nbsp;interest.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
&lt;p&gt;In the following section we aim to quantify these associations in more detail using statistical&amp;nbsp;modeling.&lt;/p&gt;
&lt;h3&gt;Statistical&amp;nbsp;Modeling&lt;/h3&gt;
&lt;p&gt;As a first test, a simple linear regression was performed relating interest rate to log&lt;sub&gt;10&lt;/sub&gt;- transformed &lt;span class="caps"&gt;FICO&lt;/span&gt; scores only, as these two variables exhibited the greatest correlation (R=0.71). Analysis of the residuals showed non-random patterns as a function of both the requested amount and loan length, but not the number of open credit lines or number of recent inquiries. We therefore chose to include the first two as potential confounders in a more complicated&amp;nbsp;model:&lt;/p&gt;
&lt;script type="math/tex; mode=display"&gt;
IR = LL_{36} + b_1 log_{10}(FICO) + b_2(Amount) + b_3(LL_{60}) + e
&lt;/script&gt;

&lt;p&gt;where &lt;span class="caps"&gt;IR&lt;/span&gt; is the interest rate; the intercept &lt;span class="caps"&gt;LL&lt;/span&gt;&lt;sub&gt;36&lt;/sub&gt; corresponds to the estimated mean of interest rates for loans over a 36 months period given a &lt;span class="caps"&gt;FICO&lt;/span&gt; score of 1 (log&lt;sub&gt;10&lt;/sub&gt;(1)=0); b&lt;sub&gt;1&lt;/sub&gt; represents the change in interest rate associated with a change of 1 unit in log&lt;sub&gt;10&lt;/sub&gt; &lt;span class="caps"&gt;FICO&lt;/span&gt; score for loans of the same amount; b&lt;sub&gt;2&lt;/sub&gt; captures the change in interest rate as a function of the loan amount requested; b&lt;sub&gt;3&lt;/sub&gt; the increase in interest rate of loans lasting 60 rather than 36 months; and e are unmodelled random variations. A scatterplot matrix illustrates the relationship between these covariates (Figure 2, left&amp;nbsp;panel).&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/p2ploans/fico-figure.jpg" alt="Fico score analysis"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 2: Covariates used in final regression model (left panel) and its residuals (right panel). Left: scatter plot matrix of covariates color-coded according to loan length (gray: 36 months; black: 60 months). Longer loans tend to have higher interest rates and correspond to larger loans. Green lines indicate univariate regression lines, supporting the observed trends. Above the diagonal, Pearson correlations are shown. Besides the association between interest rate and &lt;span class="caps"&gt;FICO&lt;/span&gt; score (here log10-transformed) as well as amount requested, it can be seen that the latter two are not associated with each other (R=0.091). Right: residuals after fitting a univariate regression using only &lt;span class="caps"&gt;FICO&lt;/span&gt; score (top row), and residuals when using the full model (bottom row). In the left column residuals are color-coded with respect to the levels of the loan length factor, and in the right column according to four different levels of the loan amount variable. The non-random patterns of the univariate model vanish in the full model.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As can be be seen in the right panel of Figure 2, the full model largely removes the patterns in residuals observed in the simple model. Further analysis also reveals that the residuals are approximately normal in distribution (see Figure 3), indicating that at least this assumption of the linear regression approximately&amp;nbsp;holds. &lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/p2ploans/residuals.png" alt="Linear regression residuals"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 3: Histogram and density of residuals for the simple univariate linear regression (left) and the full model (right). Clearly, the residuals of the full model are close to normal distributed, but not those of the simpler model.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Whereas the simple model accounted for only 50.65% of the variance in the data, the full model captured 75.18% (R&lt;sup&gt;2&lt;/sup&gt;=0.75, the significance of associations being p &amp;lt; 2.2e&lt;sup&gt;-16&lt;/sup&gt;). The added covariates did not however change the sign nor much the value of the resulting coefficients (&lt;span class="caps"&gt;LL&lt;/span&gt;&lt;sub&gt;36&lt;/sub&gt;=427.3, b&lt;sub&gt;1&lt;/sub&gt;=-146.2, b&lt;sub&gt;2&lt;/sub&gt;=0.00014, b&lt;sub&gt;3&lt;/sub&gt;=3.3). We also checked whether inclusion of the number of open credit lines or the number of recent inquiries in the model would improve the result even further, but the change in explained variance was small (+2.2%). Allowing for interactions between covariates in the full model also did not result in better fit. Since the distributions of most variables in the data set were not perfectly normal even after transforming (see histograms in Figure 2), we also tested whether non-linear and robust forms of regression would perform better. But neither a generalized linear model (glm in R [2]) nor a robust regression using an M-estimator (rlm in R [3]) produced significantly different&amp;nbsp;coefficients.&lt;/p&gt;
&lt;p&gt;In the final model, a change of one unit in log&lt;sub&gt;10&lt;/sub&gt; &lt;span class="caps"&gt;FICO&lt;/span&gt; score corresponds to a change of -146.2% (95% confidence interval: -142,-152) in interest rate over the base rate of 427.3% (&lt;span class="caps"&gt;CI&lt;/span&gt;: 416, 438). So for example, the interest rate for a 10000$ loan over 36 months would be 12.1% for the average reported &lt;span class="caps"&gt;FICO&lt;/span&gt; score of 707, and would decrease by 0.89% for an additional 10 points of the &lt;span class="caps"&gt;FICO&lt;/span&gt; score. An increase in the size of the loan by 1000$ corresponds to an increase of 0.14% in interest rate (&lt;span class="caps"&gt;CI&lt;/span&gt; of b&lt;sub&gt;2&lt;/sub&gt;: 0.00013, 0.00015). Increasing the length of the loan to 60 months would result in an additional 3.3% of interest (&lt;span class="caps"&gt;CI&lt;/span&gt;: 3.1,&amp;nbsp;3.52).&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;Our results show a significant negative association between interest rates and &lt;span class="caps"&gt;FICO&lt;/span&gt; score, modulated by positive associations with loan amount and duration. Due to the log&lt;sub&gt;10&lt;/sub&gt; transformation, model estimates are non-linear with respect to &lt;span class="caps"&gt;FICO&lt;/span&gt; score. Though this makes interpretation of the model rather unintuitive, the associations are in the direction one would&amp;nbsp;expect.&lt;/p&gt;
&lt;p&gt;It should be kept in mind that the analysis only applies to the peer-to-peer loans issues through the Lending Club. Loans in general, e.g. those offered by banks, might follow a different pattern. While the model presented here would allow interested individuals to get an idea of what interest rates to expect given a desired loan and credit history, further work would be needed if accurate predictions are required (e.g. when evaluating what kind of loans to include in a lender’s portfolio). Also, the data analyzed here does not serve to verify whether peer-to-peer loans provided through the Lending Club are indeed cheaper than those offered by banks, as the website&amp;nbsp;claims.&lt;/p&gt;
&lt;p&gt;As noted, none of the variables in the data set were normally distributed, something that could not be remedied by log transformation (or indeed other transforms, such as square roots etc.). It is possible that other non-linear or robust methods would have been more appropriate. Nevertheless, residuals after linear regression, according to histograms and &lt;span class="caps"&gt;QQ&lt;/span&gt;-plots, were approximately normal, indicating that a linear regression might not have been totally&amp;nbsp;unwarranted.&lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ol class="bib"&gt;
&lt;li&gt;Bishop, &lt;span class="caps"&gt;C. M.&lt;/span&gt;(2006). Pattern recognition and machine learning (Vol. 4, No. 4). New York:&amp;nbsp;Springer.&lt;/li&gt;
&lt;li&gt;Wood, Simon (2006). Generalized Additive Models: An Introduction with R. Chapman &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Hall/&lt;span class="caps"&gt;CRC&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;J. Huber (1981) Robust Statistics.&amp;nbsp;Wiley.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;/script&gt;
&lt;script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt;&lt;/p&gt;</summary><category term="R"></category><category term="report"></category><category term="regression"></category></entry><entry><title>Categorisation of inertial activity data</title><link href="https://buhrmann.github.io/activity-data.html" rel="alternate"></link><updated>2014-11-06T00:00:00+01:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2014-11-06:activity-data.html</id><summary type="html">&lt;p&gt;The ubiquity of mobile phones equipped with a wide range of sensors presents interesting opportunities for data mining applications. In this report we aim to find out whether data from accelerometers and gyroscopes can be used to identify physical activities performed by subjects wearing mobile phones on their&amp;nbsp;wrist.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/activitycat/muybridge.jpg" alt="Human activity" width="1000"/&gt;&lt;/p&gt;
&lt;h3&gt;Methods&lt;/h3&gt;
&lt;p&gt;The data used in this analysis is based on the “Human activity recognition using smartphones” data set available from the &lt;span class="caps"&gt;UCL&lt;/span&gt; Machine Learning Repository [1]. A preprocessed version was downloaded from the Data Analysis online course [2]. The set contains data derived from 3-axial linear acceleration and 3-axial angular velocity sampled at 50Hz from a Samsung Galaxy S &lt;span class="caps"&gt;II&lt;/span&gt;. These signals were preprocessed using various filters and other methods to reduce noise and to separate low- and high-frequency components. From this data a set of 17 individual signals was extracted by separating e.g. accelerations due to gravity from those due to body motion, separating acceleration magnitude into its individual axis-aligned components and so on. The final feature variables were calculated from both the time and frequency domain of these signals. They include too large a range to cover entirely here, but examples include variables related to the spread and centre of each signal, its entropy, skewness and kurtosis in frequency space and many&amp;nbsp;more.&lt;/p&gt;
&lt;p&gt;All data was recorded while subjects (age 19-48) performed one of six activities and labelled accordingly: lying, sitting, standing, walking, walking down stairs and walking up&amp;nbsp;stairs.&lt;/p&gt;
&lt;p&gt;The problem to be solved in our analysis is the prediction of the activity class from sensor data. Since we are only interested in prediction, and not in producing an accurate or easily comprehensible model of the relation between activity and sensor data, we have chosen to investigate the performance of the following three classifiers only: random forest (&lt;span class="caps"&gt;RF&lt;/span&gt;), support vector machine (&lt;span class="caps"&gt;SVM&lt;/span&gt;) and linear discriminant analysis (&lt;span class="caps"&gt;LDA&lt;/span&gt;). A short description of each algorithm is given in the next&amp;nbsp;sections.&lt;/p&gt;
&lt;p&gt;In order to assess and compare the performance of these classifiers we separated the data into a training and a test set. The latter consisted of data for subjects 27 to 30 and the former of the&amp;nbsp;remainder.&lt;/p&gt;
&lt;h4&gt;Random&amp;nbsp;forests&lt;/h4&gt;
&lt;p&gt;Random forests are a recursive partitioning method [3]. In the case of classification, the algorithm creates a set of decision trees calculated on random subsets of the data, using at each split of a decision tree a random subset of predictors. The final prediction is made on the basis of a majority vote across all trees. Random trees have been chosen for this analysis in part because of their accuracy and their applicability to large data sets without the need for feature&amp;nbsp;selection.&lt;/p&gt;
&lt;p&gt;Because the trees in random forests are already build from random subsamples of the data, they do not require cross-validation to estimate accuracy, and the &lt;span class="caps"&gt;OOB&lt;/span&gt; (out-off-bag) error calculated internally is generally considered a good estimator of prediction error. They also do no require the tuning of many hyper-parameters. The algorithm is not sensitive, for example, to the number of trees fitted, as long as that number is greater than a few hundred. However, some have reported variation in performance depending on the proportion of variables tested at each split. We therefore tuned this parameter using a monotonic error reduction criterion which searches for performance improvement to both sides of the default value (the square root of the number of variables, approx. 23 in this case). Using the best identified value we then trained a final random forest for&amp;nbsp;prediction.&lt;/p&gt;
&lt;p&gt;Random forests conveniently can provide a measure of each predictor’s importance. This is achieved by comparing the performance of the tree before and after shuffling the values of the variables in question, thereby removing its relation with the outcome&amp;nbsp;variable.&lt;/p&gt;
&lt;h4&gt;Support vector&amp;nbsp;machines&lt;/h4&gt;
&lt;p&gt;Support vector machines (SVMs) classify data by separating it into classes such that the distance between their decision boundaries and the closest data points is maximised (i.e. by finding maximum margin hyperplanes) [4]. The algorithm is based on a mathematical trick that involves the use of simple linear boundaries in a high-dimensional non-linear feature space; without requiring computations on this complex transformation of the data. The mapping of the feature space is done using kernel functions, which can be selected based on the classification problem. The data is then modeled using a weighted combination of the closest points in transformed space (the support&amp;nbsp;vectors).&lt;/p&gt;
&lt;p&gt;Here we use the &lt;span class="caps"&gt;SVM&lt;/span&gt; classifier provided in the e1071 package for R [5]. For multiclass problems this algorithm performs a one-against-one voting scheme. We chose the default optimization method “C-classification”, where the hyper-parameter C scales the misclassification cost, such that the higher the value the more complex the model (i.e. the larger the bias). We also chose to use the radial basis kernel, which is commonly considered a good first choice. The cost parameter C, along with γ, which defines the size of the kernel (the spatial extent of the influence of a training example), was tuned using grid-search [6] with 10-fold cross validation (tuning function provided in e1071&amp;nbsp;package).&lt;/p&gt;
&lt;h4&gt;Linear discriminant&amp;nbsp;analysis&lt;/h4&gt;
&lt;p&gt;Linear discriminant analysis (&lt;span class="caps"&gt;LDA&lt;/span&gt;) is similar to &lt;span class="caps"&gt;SVM&lt;/span&gt; in that it also tries to transform the problem such that classes separated by non-linear decision boundaries become linearly separable [4]. Instead of using kernels and support vectors, however, it identifies a linear transformation of the predictor variables (a “discriminant function”) that allows for more accurate classification than individual predictors. Identification of the transformation is based on the maximisation of the ratio of between-class variance to within-class variance. The transformation thereby maximises the separation between&amp;nbsp;classes.&lt;/p&gt;
&lt;h4&gt;Combination of&amp;nbsp;classifiers&lt;/h4&gt;
&lt;p&gt;We evaluate the performance of each classifier using its error rate (the proportion of misclassified data) or equivalently its accuracy (proportion of correctly classified data). We then combine all three methods using a simple majority vote on the prediction&amp;nbsp;set.&lt;/p&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;The data set contains 7352 observations of 561 features (in addition to a subject index and the activity performed). Of the 21 subjects included in the data, the last four were used only for evaluating the final performance of the algorithms (test set, 1485 observations) and the rest for training (5867 observations). The same sets were used for all classifiers unless stated otherwise. Data was reasonably distributed across activities (number of data points in each class: lying=1407, sitting=1286, standing=1374, walk=1226, walking down stairs=986, walking up stairs=1073). Since the classifiers used here do not make strong assumptions about the distribution of data (they are relatively robust), no detailed investigation of the statistical properties of individual features was performed. In particular, the methods employed did not require transformations of individual features (e.g. such as to improve normality of their distribution). However, as can be expected from the fact that all features derive from the same few sensor signals, the data exhibits high collinearity. While this would have led to problems with confounders in e.g. a regression model, this was not generally the case with the methods employed here. It was addressed explicitly for the &lt;span class="caps"&gt;LDA&lt;/span&gt; however (see&amp;nbsp;below).&lt;/p&gt;
&lt;p&gt;We first report results from individual classifiers and then their&amp;nbsp;combination.&lt;/p&gt;
&lt;h4&gt;Random&amp;nbsp;Forest&lt;/h4&gt;
&lt;p&gt;We tuned the proportion of variables considered in each split using 100 trees for each evaluation. The best value found was 20. A final random forest was then trained using the optimal value and 500 trees. Error rate remained low (&amp;lt; 5%) and stable after about 250 trees had been added. Analysis of variable importances, considering both the mean decrease in accuracy and Gini index, shows that the most significant variables are related to the acceleration due to gravity along the X and Y axes, as well as the mean angle with respect to gravity in the same directions (with corresponding measures from the time&amp;nbsp;domain).&lt;/p&gt;
&lt;p&gt;Figure 1 shows the data, color-coded by activity, in the first two dimensions identified. We can see that several activities are already well-separated in these two dimensions, but others (standing, walk and walk-down) are largely&amp;nbsp;overlapping.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/activitycat/centers.png" alt="RF centers"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 1: Scatter plot of data in the two most important dimensions according to the random forest. Bigger disks indicate the class centers (for each class the data point that has most nearest neighbours of the same class).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The error rate of the fitted &lt;span class="caps"&gt;RF&lt;/span&gt; is 1.6% on the training set and 4.6% on the test set (accuracy of 0.954). The confusion matrix of the predicted activities (Table 1) shows that misclassification is almost exclusively due to an inability to distinguish sitting from standing. For example, while precision is greater than 0.977 for all other activities, it is 0.912 and 0.876 for sitting and standing respectively. Apparently the activities showing large overlap in the two most important dimensions (see Figure 1) can easily be separated taking into account other variables, while for sitting and standing activities this is not the&amp;nbsp;case.&lt;/p&gt;
&lt;figure&gt;
&lt;div class="figCenter"&gt;

&lt;TABLE class="table"&gt;
&lt;TR&gt;
&lt;TH&gt;  &lt;/TH&gt;&lt;TH&gt; lying &lt;/TH&gt;&lt;TH&gt; sitting &lt;/TH&gt;&lt;TH&gt; standing &lt;/TH&gt;&lt;TH&gt; walking &lt;/TH&gt;&lt;TH&gt; walk down &lt;/TH&gt;&lt;TH&gt; walk up &lt;/TH&gt;&lt;TH&gt; precision &lt;/TH&gt;
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; lying &lt;/TD&gt; &lt;TD align="right"&gt; 293 &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sitting &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 227 &lt;/TD&gt; &lt;TD align="right"&gt; 22 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.9116 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; standing &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 37 &lt;/TD&gt; &lt;TD align="right"&gt; 261 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.8758 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walking &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 228 &lt;/TD&gt; &lt;TD align="right"&gt; 2 &lt;/TD&gt; &lt;TD align="right"&gt; 1 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9870 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk down &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 194 &lt;/TD&gt; &lt;TD align="right"&gt; 1 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9949 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk up &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 1 &lt;/TD&gt; &lt;TD align="right"&gt; 4 &lt;/TD&gt; &lt;TD align="right"&gt; 214 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9772 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sensitivity &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt;&lt;TD align="right"&gt; 0.8598 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9223 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9956 &lt;/TD&gt; &lt;TD align="right"&gt; 0.97 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9907 &lt;/TD&gt; &lt;TD align="right"&gt; accuracy=0.954 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 1: Confusion matrix of random forest predictions. Rows correspond to predicted, and columns to reference (real observed) activities. Zero counts are omitted for clarity and misclassifications appear in off-diagonal entries (precision = positive predictive value, sensitivity = true positive rate).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The accuracy of the random forest can be appreciated when comparing the actual activities in the test set with those predicted by the model. Figure 2 below plots for the two most important variables the conditional density plots of both actual and predicted activities. In each panel the density plot shows the frequency of each activity as a function of the given variable. Clearly, at least in the two chosen dimensions, the model&amp;#8217;s predictions match the actual distribution of activities very&amp;nbsp;closely.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/activitycat/density.png" alt="RF CDF"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 2: Conditional density plots for actual and predicted activities using the two most important variables of the data set.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4&gt;Support Vector&amp;nbsp;Machine&lt;/h4&gt;
&lt;p&gt;Tuning of &lt;span class="caps"&gt;SVM&lt;/span&gt; hyper-parameters using the training set resulted in optimal values of the cost C = 100 and kernel size γ = 0.001 (search was performed in intervals γ ∈ [1e-6, 0.1] and C ∈ [1,100]). To reduce computation time, the search was performed on a fraction (20%) of data randomly sampled from the training set. Using these optimal values a final &lt;span class="caps"&gt;SVM&lt;/span&gt; was trained on the whole&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;The resulting &lt;span class="caps"&gt;SVM&lt;/span&gt; uses 22.6% of the data points as support vectors (1326 out of 5867). Since this number depends on the tuned parameter C, which was found using cross-validation, we assume that we have not overfit the model. This is supported by the model’s high accuracy of 0.989 on the training set when averaged over a 10-fold cross validation. On the test set its accuracy is 0.96, i.e. slightly better than the random&amp;nbsp;forest. &lt;/p&gt;
&lt;p&gt;The confusion matrix of predictions is shown in Table 2. As we can see, the &lt;span class="caps"&gt;SVM&lt;/span&gt; exhibits perfect classification for all activities other than sitting and standing, where its performance is similar to the random&amp;nbsp;forest.&lt;/p&gt;
&lt;figure&gt;
&lt;div class="figCenter"&gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt;
&lt;TH&gt;  &lt;/TH&gt;&lt;TH&gt; lying &lt;/TH&gt;&lt;TH&gt; sitting &lt;/TH&gt;&lt;TH&gt; standing &lt;/TH&gt;&lt;TH&gt; walking &lt;/TH&gt;&lt;TH&gt; walk down &lt;/TH&gt;&lt;TH&gt; walk up &lt;/TH&gt;&lt;TH&gt; precision &lt;/TH&gt;
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; lying &lt;/TD&gt; &lt;TD align="right"&gt; 293 &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sitting &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 232 &lt;/TD&gt; &lt;TD align="right"&gt; 27 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.8958 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; standing &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 32 &lt;/TD&gt; &lt;TD align="right"&gt; 256 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.8889 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walking &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 229 &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk down &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 200 &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk up &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 216 &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sensitivity &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt;&lt;TD align="right"&gt; 0.8788 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9046 &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; &lt;TD align="right"&gt; accuracy=0.96 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 2: Confusion matrix of &lt;span class="caps"&gt;SVM&lt;/span&gt; predictions. See Table 1 for further details.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4&gt;Linear Discriminant&amp;nbsp;Analysis&lt;/h4&gt;
&lt;p&gt;&lt;span class="caps"&gt;LDA&lt;/span&gt; can be sensitive or even fail when the data exhibits a high degree of collinearity. Since our sensor data essentially consists of different transformations of the same few signals we can expect that this is indeed the case in our data set. We therefore performed two &lt;span class="caps"&gt;LDA&lt;/span&gt; classifications. For the first model (&lt;span class="caps"&gt;LDA1&lt;/span&gt;) the complete training set was used. For the second model (&lt;span class="caps"&gt;LDA2&lt;/span&gt;) we removed those variables that exhibited pair-wise correlations greater than R=0.9 (removing one from each pair) using the findCorrelation function in R’s caret package. A total of 346 variables were thus removed, leaving 215 less correlated predictors. Using these two training sets, &lt;span class="caps"&gt;LDA&lt;/span&gt; models were trained with 10-fold cross validation to assess whether we would expect a difference in their accuracy. The &lt;span class="caps"&gt;LDA2&lt;/span&gt; model, trained on relatively uncorrelated data, showed an error rate of 3.5%, and &lt;span class="caps"&gt;LDA1&lt;/span&gt; a rate of 5.2%. Based on these results we have to conclude that &lt;span class="caps"&gt;LDA2&lt;/span&gt; should be used for our final&amp;nbsp;predictions.&lt;/p&gt;
&lt;p&gt;Table 3 shows the confusion matrix for the &lt;span class="caps"&gt;LDA2&lt;/span&gt; model when predicting on the test&amp;nbsp;set.&lt;/p&gt;
&lt;figure&gt;
&lt;div class="figCenter" &gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt;
&lt;TH&gt;  &lt;/TH&gt;&lt;TH&gt; lying &lt;/TH&gt;&lt;TH&gt; sitting &lt;/TH&gt;&lt;TH&gt; standing &lt;/TH&gt;&lt;TH&gt; walking &lt;/TH&gt;&lt;TH&gt; walk down &lt;/TH&gt;&lt;TH&gt; walk up &lt;/TH&gt;&lt;TH&gt; precision &lt;/TH&gt;
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; lying &lt;/TD&gt; &lt;TD align="right"&gt; 293 &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sitting &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 223 &lt;/TD&gt; &lt;TD align="right"&gt; 24 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.9028 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; standing &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; 41 &lt;/TD&gt; &lt;TD align="right"&gt; 259 &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.8633 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walking &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 226 &lt;/TD&gt; &lt;TD align="right"&gt; 3 &lt;/TD&gt; &lt;TD align="right"&gt; 2 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9784 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk down &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 196 &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; walk up &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt;&lt;TD align="right"&gt; &lt;/TD&gt; &lt;TD align="right"&gt; &lt;/TD&gt; 
&lt;TD align="right"&gt; 3 &lt;/TD&gt; &lt;TD align="right"&gt; 1 &lt;/TD&gt; &lt;TD align="right"&gt; 214 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9817 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;TR&gt;
&lt;TD align="right"&gt; sensitivity &lt;/TD&gt; &lt;TD align="right"&gt; 1.0 &lt;/TD&gt;&lt;TD align="right"&gt; 0.8447 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9152 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9869 &lt;/TD&gt; &lt;TD align="right"&gt; 0.98 &lt;/TD&gt; &lt;TD align="right"&gt; 0.9907 &lt;/TD&gt; &lt;TD align="right"&gt; accuracy=0.95 &lt;/TD&gt; 
&lt;/TR&gt;
&lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 3: Confusion matrix of &lt;span class="caps"&gt;LDA2&lt;/span&gt; predictions. See Table 1 for further details.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We can observe the same pattern of misclassification as in the other two models. Interestingly, when we use &lt;span class="caps"&gt;LDA1&lt;/span&gt; for prediction, accuracy is increased to 0.9785 (error rate of 2.15%). Nevertheless, since in cross-validation on the training set &lt;span class="caps"&gt;LDA2&lt;/span&gt; performed better, we assume that this increase is a result of chance only and does not reflect a truly better&amp;nbsp;model.&lt;/p&gt;
&lt;p&gt;To visually demonstrate the reason for the model&amp;#8217;s misclassification we can plot the test data in the first two dimensions of the trained linear discriminant, color-coded by true activity (Figure&amp;nbsp;3).&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/activitycat/lda.png" alt="RF CDF"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 3: Test data scattered in the first two discriminant dimensions.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Again, we find that two clusters of activities are similar: sitting and standing on the one hand, and the different walking activities on the other. But at least the two clusters (as well as the data points for the lying acticity) are well separated, in contrast to the &amp;#8220;raw&amp;#8221; dimensions shown in Figure&amp;nbsp;1.&lt;/p&gt;
&lt;h4&gt;Comparison of&amp;nbsp;classifiers&lt;/h4&gt;
&lt;p&gt;Comparing the three classifiers in terms of their sensitivity (recall), i.e. the proportion of correct predictions for each class, we have already seen that all three models perform very similar, with the &lt;span class="caps"&gt;SVM&lt;/span&gt; having a slight advantage. We can speculate that this is due to the non-linear (radial basis) decision boundaries of the classifier, which stands in contrast to the linear methods employed in the other two&amp;nbsp;models.&lt;/p&gt;
&lt;p&gt;Based on the previous results we expect not to gain much predictive power from the combination of individual models using a simple majority vote. All models exhibit the same problem of misclassification of sitting and standing activities, and therefore do not complement each other. This is confirmed by a combined accuracy of 0.958 when predictions are made based on a majority vote of the three models, which sits exactly between the lower scoring &lt;span class="caps"&gt;RF&lt;/span&gt; and &lt;span class="caps"&gt;LDA&lt;/span&gt; on the one hand, and the slightly higher scoring &lt;span class="caps"&gt;SVM&lt;/span&gt; on the&amp;nbsp;other.&lt;/p&gt;
&lt;p&gt;What explains the consistent misclassification of sitting and standing across all three models? Intuitively it is clear that since in both “activities” subjects remain more or less motionless, inertial data will not provide much differentiating information. This is reflected in the data. To illustrate this we trained another random forest on a new subset of the training data which a) included only sitting and standing activities, and b) only included predictors with pair-wise correlations less than R=0.9 (same procedure as for the &lt;span class="caps"&gt;LDA&lt;/span&gt; model). This data set therefore consisted of a binary outcome and 2113 observations (1022 and 1091 in each level). The importances of the resulting random forest show that the most significant split is achieved on the mean angle of gravity with respect to the Y axis (θy), followed by the energy measure of acceleration due to gravity in the Y dimension in the time domain (gey) or, according to the mean decrease in Gini index, the entropy measure of the same variable. In the left panel of Figure 1 we plot the data along these two axes (θy vs. gey) and color the data according to&amp;nbsp;activity. &lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/activitycat/intertial.jpg" alt="Inertial data"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 1: Overlap of data from sitting and standing activities underlying the failure to perfectly separate these two classes. Left panel: scatterplot of the two most important variables for distinguishing sitting and standing activities (according to a random forest fitted to data for these two activities only). θy is the mean angle of gravity with respect to the y-axis, and gey is the entropy of acceleration due to gravity in the y-dimension (see main text for further details). Only part of the range for θy is shown to highlight the region of overlap. Right panel: the same overlap is more clearly seen in the histogram of the θy variable only. Even though the means of θy for sitting and standing are different (p-value in t-test &lt; 2.2e-16), their distributions overlap significantly.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We can see that while the data falls into two identifiable regions, these are not perfectly separable but rather show significant overlap. This can be seen even more clearly in the right panel of Figure 1, where we overimpose histograms of θy separated by activity. The distributions of sitting and standing in this variable are clearly different statistically, but also overlap significantly. Their difference is confirmed by a t-test of their means (-0.01 and 0.21 for sitting and standing respectively, p– value &amp;lt; 2.2e-16). Nevertheless, the overlap means that no classifier should be able to distinguish these two activities perfectly, at least not based on this single variable. Adding further variables might help in separating the two distributions. But as the three trained models seem to indicate, the data set does not appear to contain the kind of variables that allows for perfect discrimination of sitting and&amp;nbsp;standing.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;We have used three different types of classifiers to predict a subject’s physical activity from inertial data captured using the accelerometer and gyroscope embedded in mobile phones worn at the wrist. All classifiers performed well overall (accuracy &amp;gt; 0.95), but failed equally to distinguish some cases of sitting and standing. We observe, however, that the non-linear &lt;span class="caps"&gt;SVM&lt;/span&gt; seems to have a slight advantage over the two linear models. This suggests that perhaps a non-linear variant of the &lt;span class="caps"&gt;LDA&lt;/span&gt; algorithm (namely quadratic discriminant analysis, or &lt;span class="caps"&gt;QDA&lt;/span&gt;), and equally a random forest using decision trees with non-linear boundaries, would have been more appropriate for this data set. Further work would also be needed to determine whether the radial kernel used in the &lt;span class="caps"&gt;SVM&lt;/span&gt; model is in fact the optimal kernel for this data&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;We have shown that the data used in this analysis does not seem to contain individual variables that can separate sitting and standing activities perfectly. The failure of all three classifiers also suggests that the two activities cannot be resolved in higher dimensions. This is corroborated by the fact that the classifiers all take rather different approaches, e.g. parametric (&lt;span class="caps"&gt;LDA&lt;/span&gt;) and non-parametric (&lt;span class="caps"&gt;RF&lt;/span&gt;), or linear (decision trees) and non-linear decision boundaries (&lt;span class="caps"&gt;SVM&lt;/span&gt;). Of course, the failure to distinguish sitting and standing using inertial data only is not surprising, as both activities imply near stationarity of the sensors. However, we can hypothesise that other transformations of the data not provided in this set could be helpful. E.g. accelerations in the vertical direction due to body motion should show non-linear step changes at the moment of sitting down, while this would not be the case if a person continued standing. Adding the existence of such step-changes to the data set could potentially lead to better separability of these&amp;nbsp;activities.&lt;/p&gt;
&lt;p&gt;We have not here performed an analysis of variation between subjects. It is possible that the behaviour of some subjects differs significantly from that of others, and that in the process of “averaging” across subjects information is lost. Future work should also address this&amp;nbsp;question.&lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;&lt;ol class="bib"&gt;
&lt;li&gt;&lt;span class="caps"&gt;UCI&lt;/span&gt; Data set: &lt;a href="http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones"&gt;Human Activity Recognition Using&amp;nbsp;Smartphones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://spark-public.s3.amazonaws.com/dataanalysis/samsungData.rda"&gt;Preprocessed data set&lt;/a&gt; on Amazon S3&amp;nbsp;storage.&lt;/li&gt;
&lt;li&gt;Breiman, L. (2001), Random Forests, Machine Learning 45(1),&amp;nbsp;5-32.&lt;/li&gt;
&lt;li&gt;Bishop, &lt;span class="caps"&gt;C. M.&lt;/span&gt;(2006). Pattern recognition and machine learning (Vol. 4, No. 4). New York:&amp;nbsp;Springer.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cran.r-project.org/web/packages/e1071/index.html"&gt;&lt;span class="caps"&gt;SVM&lt;/span&gt; package&amp;nbsp;&amp;#8216;e1071&amp;#8217;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bergstra, J. and Bengio, Y. (2012). Random Search for Hyper-Parameter Optimization. J. Machine Learning Research 13:&amp;nbsp;281—305.&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;</summary><category term="R"></category><category term="report"></category><category term="classification"></category><category term="svm"></category><category term="random forest"></category><category term="lda"></category></entry><entry><title>Dash+ visualization of running data</title><link href="https://buhrmann.github.io/dash+.html" rel="alternate"></link><updated>2014-11-06T00:00:00+01:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2014-11-06:dash+.html</id><summary type="html">&lt;p&gt;&lt;a href="http://dashplus.herokuapp.com/"&gt;Dash+&lt;/a&gt; is a python web application I built with &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt;, which imports Nike+ running data into a NoSQL database (MongoDB) and uses &lt;a href="http://d3js.org/"&gt;D3.js&lt;/a&gt; to visualize and analyze&amp;nbsp;it. &lt;/p&gt;
&lt;p&gt;The app is work in progress and primarily intended as a personal playground for exploring d3 visualization of my own running data. Having said that, if you want to fork your own version on &lt;a href="https://github.com/synergenz/dash"&gt;github&lt;/a&gt;, simply add your Nike access token in the corresponding&amp;nbsp;file.&lt;/p&gt;
&lt;figure&gt;
&lt;a href="http://dashplus.herokuapp.com"&gt;&lt;img src="/images/dash/screen1.png" alt="Dash+ screenshot 1" style="width: 400px"/&gt;&lt;/a&gt;
&lt;/figure&gt;</summary><category term="visualization"></category><category term="d3"></category><category term="nosql"></category><category term="python"></category><category term="mongodb"></category></entry><entry><title>Reading from distributed cache in Hadoop</title><link href="https://buhrmann.github.io/hadoop-distributed-cache.html" rel="alternate"></link><updated>2014-10-29T17:43:28+01:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2014-10-29:hadoop-distributed-cache.html</id><summary type="html">&lt;p&gt;The distributed cache can be used to make small files (or jars etc.) available to mapreduce functions locally on each node. This can be useful e.g. when a global stopword list is needed by all mappers for index creation.  Here are two correct ways of reading a file from distributed cache in Hadoop 2. This has changed in the new &lt;span class="caps"&gt;API&lt;/span&gt; and very few books and tutorials have updated&amp;nbsp;examples.&lt;/p&gt;
&lt;h3&gt;Named&amp;nbsp;File&lt;/h3&gt;
&lt;p&gt;In the&amp;nbsp;driver:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Job&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Job&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getInstance&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
&lt;span class="n"&gt;job&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;addCacheFile&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;URI&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/path/to/file.csv&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;#filelabel&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the&amp;nbsp;mapper:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nd"&gt;@Override&lt;/span&gt;
&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setup&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Context&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;InterruptedException&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;URI&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;cacheFiles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getCacheFiles&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cacheFiles&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;cacheFiles&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;length&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="n"&gt;BufferedReader&lt;/span&gt; &lt;span class="n"&gt;reader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;BufferedReader&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;FileReader&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;filelabel&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;File&amp;nbsp;system&lt;/h3&gt;
&lt;p&gt;In the&amp;nbsp;driver:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Job&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Job&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getInstance&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
&lt;span class="n"&gt;job&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;addCacheFile&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;URI&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;/path/to/file.csv&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the&amp;nbsp;mapper:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nd"&gt;@Override&lt;/span&gt;
&lt;span class="kd"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setup&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Context&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="kd"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="n"&gt;InterruptedException&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;URI&lt;/span&gt;&lt;span class="o"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;cacheFiles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getCacheFiles&lt;/span&gt;&lt;span class="o"&gt;();&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cacheFiles&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kc"&gt;null&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;cacheFiles&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;length&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;FileSystem&lt;/span&gt; &lt;span class="n"&gt;fs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;FileSystem&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;get&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;context&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getConfiguration&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
        &lt;span class="n"&gt;Path&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cacheFiles&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;].&lt;/span&gt;&lt;span class="na"&gt;toString&lt;/span&gt;&lt;span class="o"&gt;());&lt;/span&gt;
        &lt;span class="n"&gt;BufferedReader&lt;/span&gt; &lt;span class="n"&gt;reader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;BufferedReader&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;InputStreamReader&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;open&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;)));&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><category term="hadoop"></category><category term="big data"></category><category term="java"></category></entry><entry><title>Titanic survival prediction</title><link href="https://buhrmann.github.io/titanic-survival.html" rel="alternate"></link><updated>2014-10-23T00:00:00+02:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2014-10-23:titanic-survival.html</id><summary type="html">&lt;p&gt;In this report I will provide an overview of my solution to &lt;a href="http://www.kaggle.com"&gt;kaggle&amp;#8217;s&lt;/a&gt; &lt;a href="https://www.kaggle.com/c/titanic-gettingStarted"&gt;&amp;#8220;Titanic&amp;#8221; competition&lt;/a&gt;. The aim of this competition is to predict the survival of passengers aboard the titanic using information such as a passenger&amp;#8217;s gender, age or socio-economic  status. I will explain my data munging process, explore the available predictor variables, and compare a number of different classification algorithms in terms of their prediction performance. All analysis presented here was performed in R. The corresponding source code is available on &lt;a href="https://github.com/synergenz/kaggle/tree/master/titanic"&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/titanic/titanic.jpg" alt="Titanic"/&gt;
&lt;/figure&gt;

&lt;h3&gt;Data&amp;nbsp;munging&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://www.kaggle.com/c/titanic-gettingStarted/data"&gt;data set&lt;/a&gt; provided by kaggle contains 1309 records of passengers aboard the titanic at the time it sunk. Each record contains 11 variables describing the corresponding person: survival (yes/no), class (1 = Upper, 2 = Middle, 3 = Lower), name, gender and age; the number of siblings and spouses aboard, the number of parents and children aboard, the ticket number, the fare paid, a cabin number, and the port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton). Of the 1309 records 1068 include the label, thus constituting the training set, while a different subset of size 418 does not include the label and is used by kaggle for assessing the accuracy of the predictions&amp;nbsp;submitted.&lt;/p&gt;
&lt;p&gt;To facilitate the training of classifiers for the prediction of survival, and for purposes of presentation, the data was preprocessed in the following way. All categorical variables were treated as factors (ordered where appropriate, e.g. in the case of class). From each passenger&amp;#8217;s name her title was extracted and added as a new predictor&amp;nbsp;variable. &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;data&lt;span class="o"&gt;$&lt;/span&gt;title &lt;span class="o"&gt;=&lt;/span&gt; sapply&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;name&lt;span class="p"&gt;,&lt;/span&gt; FUN&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; strsplit&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;,&lt;/span&gt; split&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;[,.]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]})&lt;/span&gt;
data&lt;span class="o"&gt;$&lt;/span&gt;title &lt;span class="o"&gt;=&lt;/span&gt; sub&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; data&lt;span class="o"&gt;$&lt;/span&gt;title&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This resulted in a factor with a great number of different levels, many of which could be considered similar in terms of implied societal status. To simplify matters the following levels were combined: &amp;#8216;Mme&amp;#8217;, &amp;#8216;Mlle&amp;#8217;, &amp;#8216;Ms&amp;#8217; were re-assigned to the level &amp;#8216;Miss&amp;#8217;; &amp;#8216;Capt&amp;#8217;, &amp;#8216;Col&amp;#8217;, &amp;#8216;Don&amp;#8217;, &amp;#8216;Major&amp;#8217;, &amp;#8216;Sir&amp;#8217; and &amp;#8216;Dr&amp;#8217; as titles of male nobility to the level &amp;#8216;Sir&amp;#8217;; and &amp;#8216;Dona&amp;#8217;, &amp;#8216;Lady&amp;#8217;, &amp;#8216;the Countess&amp;#8217; and &amp;#8216;Jonkheer&amp;#8217; as titles of female nobility to the level&amp;nbsp;&amp;#8216;Lady&amp;#8217;.&lt;/p&gt;
&lt;p&gt;The number of all family members aboard was combined into a single family size variable. In addition, a categorical variable was formed from this data by assigning records to three approx. equally sized levels of &amp;#8216;singles&amp;#8217;, &amp;#8216;small&amp;#8217; and &amp;#8216;big&amp;#8217; families. Also, another factor was added aimed at &lt;em&gt;uniquely&lt;/em&gt; identifying big families. To this send each passenger&amp;#8217;s surname was combined with the corresponding family size (resulting e.g. in the factor level &amp;#8220;11Sage&amp;#8221;), but such that families smaller than a certain number (n=4) were all assigned the level&amp;nbsp;&amp;#8220;small&amp;#8221;.&lt;/p&gt;
&lt;p&gt;Age information was missing for many records (about 20%). Since age can be hypothesised to correlate well with such information as a person&amp;#8217;s title (e.g. &amp;#8220;Master&amp;#8221; was used to refer politely to young children), this data was imputed using a random forest (essentially a bagged decision tree) trained to predict age from the remaining&amp;nbsp;variables:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;agefit &lt;span class="o"&gt;=&lt;/span&gt; rpart&lt;span class="p"&gt;(&lt;/span&gt;age &lt;span class="o"&gt;~&lt;/span&gt; pclass &lt;span class="o"&gt;+&lt;/span&gt; sex &lt;span class="o"&gt;+&lt;/span&gt; sibsp &lt;span class="o"&gt;+&lt;/span&gt; parch &lt;span class="o"&gt;+&lt;/span&gt; fare &lt;span class="o"&gt;+&lt;/span&gt; embarked &lt;span class="o"&gt;+&lt;/span&gt; title &lt;span class="o"&gt;+&lt;/span&gt; familysize&lt;span class="p"&gt;,&lt;/span&gt; data&lt;span class="o"&gt;=&lt;/span&gt;data&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;age&lt;span class="p"&gt;),],&lt;/span&gt; method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;anova&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
data&lt;span class="o"&gt;$&lt;/span&gt;age&lt;span class="p"&gt;[&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;age&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; predict&lt;span class="p"&gt;(&lt;/span&gt;agefit&lt;span class="p"&gt;,&lt;/span&gt; data&lt;span class="p"&gt;[&lt;/span&gt;is.na&lt;span class="p"&gt;(&lt;/span&gt;data&lt;span class="o"&gt;$&lt;/span&gt;age&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;From the imputed age variable a factor was constructed indicating whether or not a passenger is a &amp;#8220;child&amp;#8221; (age &amp;lt;&amp;nbsp;16).&lt;/p&gt;
&lt;p&gt;The fare variable contained 18 missing values (17 fares with a value of 0 and one &lt;span class="caps"&gt;NA&lt;/span&gt;), which were imputed using a decision tree analogous to the above method for the age variable. Since this variable was far from normally distributed (which might violate some algorithm&amp;#8217;s assumptions), another factor was created splitting the fare into 3 approx. equally distributed&amp;nbsp;levels.&lt;/p&gt;
&lt;p&gt;Cabin and tickets information was sparse, i.e. missing for most passengers, and not considered for further analysis or as predictors for classification. The embarkation variable contained a single missing value, for which was substituted the majority value&amp;nbsp;(Southampton).&lt;/p&gt;
&lt;p&gt;All of the above transformations were performed on the joined train and test data, which was thereafter split again into the original two&amp;nbsp;sets.&lt;/p&gt;
&lt;p&gt;In summary, the processed data set contains the following features. 5 unordered factors: gender, port of embarkation, title, child and family id. 3 ordered factors: class, family size category, fare category. And three numerical predictors: age, fare price and family size (of which only age is approx. normal&amp;nbsp;distributed).&lt;/p&gt;
&lt;h3&gt;Data&amp;nbsp;exploration&lt;/h3&gt;
&lt;p&gt;Some &lt;a href="http://en.wikipedia.org/wiki/RMS_Titanic"&gt;background information&lt;/a&gt; about the titanic disaster might prove useful to formulate hypotheses about the type of people more probable to have survived, i.e. those more likely to have had access to lifeboats. The ship only carried enough lifeboats for slightly more than half the number of people on board (and many were launched half-full). In this respect, the most significant aspect of the rescue effort was the &amp;#8220;women and children first&amp;#8221; policy followed in the majority of life boat loadings. Additionally, those on the upper decks (i.e. those in the upper classes) had easier access to lifeboats, not the least because of closer physical proximity than the lower decks. It should thus not come as a surprise that survival was heavily skewed towards women, children and in general those of the upper&amp;nbsp;class.&lt;/p&gt;
&lt;p&gt;As a first step let&amp;#8217;s look at survival rates as a function of each factor variable in the training set, shown in Figure 1.
&lt;figure &gt;
&lt;img src="/images/titanic/facBars.png" alt="Survival vs Factors" /&gt;
&lt;img src="/images/titanic/isChildBars.png" alt="Survival vs Child"/&gt;
&lt;img src="/images/titanic/titleBars.png" alt="Survival vs Title"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 1: Proportion of survivors as a function of several categorical predictors. Blue:survived, red: perished. For the title variable, proportions are relative to each level. For the remaining variables overall proportions are displayed. &lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Clearly, male passengers were at a huge disadvantage. They were about 5 times more likely to die than to survive. In contrast, female passengers were almost 3 times more likely to survive than to die. Next, while 1st class passengers were more likely to survive, chances were tilted badly against 3nd class passengers (in the 2nd class the chance was about equal). While a difference in survival rate can also be seen depending on the port of embarkation, the variable is so highly imbalanced that these differences could be spurious. In regards to family size, singles were much more likely to die than to survive. However, this balance is affected highly by the fact that of the 537 singles 411 were male and only 126 female. The gender thus confounds this family size level. When considering only non-singles we see a slight effect of larger families size leading to lower probability of survival. The fare variable essentially mirrors the class variable. Those who paid more for their ticket (and thus probably of a higher socio-economical status) are somewhat more likely to survive than to perish, while passengers with the cheapest tickets were much more probable to die. The title variable mostly confirms the earlier trends. Passengers with female titles (Lady, Miss, Mrs), as well as young passengers (Master) are more likely to survive than adult male passengers (Mr, Sir, Reverend). And amongst the male adults, those of nobility (Sir) had a better chance of survival than &amp;#8220;common&amp;#8221; travellers (Mr). A slight effect of age on survival can also be seen in the &amp;#8220;is child&amp;#8221; variable (most children survived, while most adults died), but the number of children was relatively low&amp;nbsp;overall. &lt;/p&gt;
&lt;p&gt;The numeric variables further support the trend observed in the corresponding factors, as can be seen in Figure 2 below.
&lt;figure&gt;
&lt;img src="/images/titanic/expContVar.png" alt="Numerical predictors" /&gt;
&lt;figcaption  class="capCenter"&gt;Figure 2: Survival distributions for numerical predictors (red=survived, blue=died). Left: A box plot of fair price, y axis is log-scaled. Right: density estimate of survival vs age. &lt;/figcaption&gt;
&lt;/figure&gt;
Those that survived travelled on a more expensive ticket on average than those who died. And for young children we see a peak in the probability of&amp;nbsp;survival.&lt;/p&gt;
&lt;p&gt;To develop some intuition about the importance of the different predictors and how they might be used by a classifier it may help to train a simple decision tree on the data, which is a model easy to interpret. Let&amp;#8217;s start by sticking mostly to the original predictors (not including non-normal variables converted to factors, nor engineered variables like the&amp;nbsp;title):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;dc1 &lt;span class="o"&gt;=&lt;/span&gt; rpart&lt;span class="p"&gt;(&lt;/span&gt;survived &lt;span class="o"&gt;~&lt;/span&gt; pclass &lt;span class="o"&gt;+&lt;/span&gt; sex &lt;span class="o"&gt;+&lt;/span&gt; age &lt;span class="o"&gt;+&lt;/span&gt; familysize &lt;span class="o"&gt;+&lt;/span&gt; fare &lt;span class="o"&gt;+&lt;/span&gt; embarked&lt;span class="p"&gt;,&lt;/span&gt; data&lt;span class="o"&gt;=&lt;/span&gt;train&lt;span class="p"&gt;,&lt;/span&gt; method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;class&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;A tree trained on the remaining predictors is shown below in Figure&amp;nbsp;3.&lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/titanic/dectree1.png" alt="Decision tree 1"/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 3: Decision tree predicting survival. Each node displays its survival prediction (yes=blue, no=red), the probability of belonging to each class conditioned on the node (sum to one within node), as well as the percentage of observations in each node (sum to 100% across leaves). &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The resulting decision tree should not be surprising. Without any further information (at the root node) the classifier always predicts that a passenger would not survive, which is of course correct given that 62% of all passengers died while only 38% survived. Next, the tree splits on the gender variable. For male passengers over the age of 13 the classifier predicts death, while children are more likely to survive, unless they belong to a large family. On the female branch, those belonging to the upper class are predicted to survive. Those in the third class, in contrast, are predicted to survive only when they belong to a relatively small family (size &amp;lt; 4.5) and are under the age of 36. Those older, or member of a bigger family are more probable to have died. The fare and embarkation variables are not used in the final tree. Since we already know that fare correlates strongly with class, and since embarkation is strongly imbalanced, this is not surprising. &amp;#8220;Factorised&amp;#8221; variables derived from non-uniformly distributed predictors (fare category, family size category and &amp;#8220;is child&amp;#8221;) are not required in the training of the tree, as it automatically determines the best level at which to split the&amp;nbsp;variables.&lt;/p&gt;
&lt;p&gt;How about the engineered variables of a passenger&amp;#8217;s title and familyId? One possible problem here is that these factors contain relatively many levels. Decision trees split nodes by information gain, and this measure in decision trees is biased in favour of attributes with more levels. Regular trees will therefore often produce results with those categorical variables dominating others. However, biased predictor selection can be avoided using Conditional Inference Trees (ctrees), which will be employed later when more methodologically exploring different&amp;nbsp;classifiers.&lt;/p&gt;
&lt;p&gt;As a last step, we compare the distribution of variables from the train and the test set, to avoid potential surprises arising from imbalanced splits of the data. Instead of pulling out and displaying here all tables for the categorical variables in both sets, we first use a chi-square test to single out those categorical variables whose levels are differently&amp;nbsp;distributed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;factabs &lt;span class="o"&gt;=&lt;/span&gt; lapply&lt;span class="p"&gt;(&lt;/span&gt;varnames&lt;span class="p"&gt;[&lt;/span&gt;facvars&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; data.frame&lt;span class="p"&gt;(&lt;/span&gt;cbind&lt;span class="p"&gt;(&lt;/span&gt;table&lt;span class="p"&gt;(&lt;/span&gt;train&lt;span class="p"&gt;[,&lt;/span&gt;x&lt;span class="p"&gt;]),&lt;/span&gt; table&lt;span class="p"&gt;(&lt;/span&gt;test&lt;span class="p"&gt;[,&lt;/span&gt; x&lt;span class="p"&gt;])))})&lt;/span&gt;
pvals &lt;span class="o"&gt;=&lt;/span&gt; sapply&lt;span class="p"&gt;(&lt;/span&gt;faccomp&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; chisq.test&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;p.value&lt;span class="p"&gt;)&lt;/span&gt;
faccomp&lt;span class="p"&gt;[[&lt;/span&gt;which&lt;span class="p"&gt;(&lt;/span&gt;pvals&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="m"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)]]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Only the embarkation shows a slight but apparently significant difference between the train and test set, with the difference in the proportions of people embarked in Cherbourg vs. Southhamption being slightly less pronounced in the test set (C=0.188, S=0.725 in the training set, and C=0.244, S=0.646 in the test set). Since the overall tendency is preserved we assume this difference will not affect the quality of our following predictions. Comparing five-number summaries for the numerical variables showed no further differences in distribution between the train and test&amp;nbsp;sets.&lt;/p&gt;
&lt;h3&gt;Classifier&amp;nbsp;training&lt;/h3&gt;
&lt;p&gt;I decided to use the caret package in R to train and compare a variety of different models. I should note that finding a better way to preprocess, engineer and extend the available data is often more important than small improvements gained from using a better classifier. However, I suspect that since the titanic data set is very small and consists mostly of categorical variables, and since I know of no way to collect more data on the problem (without cheating), some classifiers might in this particular case perform better than&amp;nbsp;others.&lt;/p&gt;
&lt;p&gt;The caret package provides a unified interface for training of a large number of different learning algorithms, including options for validating learners using cross-validation (and related validation techniques), which can be used simultaneously for the tuning of model-specific hyper-parameters. My overall approach will be this: first I train a number of classifiers using repeated cross-validation to estimate their prediction accuracy. Next I create ensembles of these classifiers and compare their accuracy to that of individual classifiers. Lastly, I choose the best (individual or ensemble) classifier to create predictions for the kaggle competition. Usually, I would maintain a hold out set for validation and comparison of the various hypertuned algorithms. Because the data set is already small, however, I decided to try and rely on the results from repeated cross-validation (10 folds, 10 repeats). It might nevertheless be insightful to at least compare the cross-validated metrics (using the full data set) to those measured on a holdout set, even when ultimately training the final classifier on the whole training set. We&amp;#8217;ll start by training with 20% of data reserved for the validation&amp;nbsp;set.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s my approach to more or less flexibly building a set of different classifiers using&amp;nbsp;caret:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;rseed &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;42&lt;/span&gt;
scorer &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;ROC&amp;#39;&lt;/span&gt; &lt;span class="c1"&gt;# &amp;#39;ROC&amp;#39; or &amp;quot;Accuracy&amp;#39;&lt;/span&gt;
summarizor &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kr"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;scorer &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;Accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; defaultSummary &lt;span class="kr"&gt;else&lt;/span&gt; twoClassSummary
selector &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;best&amp;quot;&lt;/span&gt; &lt;span class="c1"&gt;# &amp;quot;best&amp;quot; or &amp;quot;oneSE&amp;quot;&lt;/span&gt;
folds &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;
repeats &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;
pp &lt;span class="o"&gt;=&lt;/span&gt; c&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;center&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;scale&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

cvctrl &lt;span class="o"&gt;=&lt;/span&gt; trainControl&lt;span class="p"&gt;(&lt;/span&gt;method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;repeatedcv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; number&lt;span class="o"&gt;=&lt;/span&gt;folds&lt;span class="p"&gt;,&lt;/span&gt; repeats&lt;span class="o"&gt;=&lt;/span&gt;repeats&lt;span class="p"&gt;,&lt;/span&gt; p&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    summaryFunction&lt;span class="o"&gt;=&lt;/span&gt;summarizor&lt;span class="p"&gt;,&lt;/span&gt; selectionFunction&lt;span class="o"&gt;=&lt;/span&gt;selector&lt;span class="p"&gt;,&lt;/span&gt; classProbs&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k-Variable"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    savePredictions&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k-Variable"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; returnData&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k-Variable"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
    index&lt;span class="o"&gt;=&lt;/span&gt;createMultiFolds&lt;span class="p"&gt;(&lt;/span&gt;trainset&lt;span class="o"&gt;$&lt;/span&gt;survived&lt;span class="p"&gt;,&lt;/span&gt; k&lt;span class="o"&gt;=&lt;/span&gt;folds&lt;span class="p"&gt;,&lt;/span&gt; times&lt;span class="o"&gt;=&lt;/span&gt;repeats&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;First, use a random seed to make results repeatable! Next we select whether to optimise prediction accuracy or the area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve, and the number of folds for cross-validation and the number of times to repeat the validation. Some algorithms require normalised data, which means centering and scaling here. Lastly, setup the training control structure expected by the caret package. Next we set up a number of formulas to be used by the&amp;nbsp;classifiers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;fmla0 &lt;span class="o"&gt;=&lt;/span&gt; survived &lt;span class="o"&gt;~&lt;/span&gt; pclass &lt;span class="o"&gt;+&lt;/span&gt; sex &lt;span class="o"&gt;+&lt;/span&gt; age
fmla1 &lt;span class="o"&gt;=&lt;/span&gt; survived &lt;span class="o"&gt;~&lt;/span&gt; pclass &lt;span class="o"&gt;+&lt;/span&gt; sex &lt;span class="o"&gt;+&lt;/span&gt; age &lt;span class="o"&gt;+&lt;/span&gt; fare &lt;span class="o"&gt;+&lt;/span&gt; embarked &lt;span class="o"&gt;+&lt;/span&gt; familysizefac &lt;span class="o"&gt;+&lt;/span&gt; title
&lt;span class="kc"&gt;...&lt;/span&gt;
fmla &lt;span class="o"&gt;=&lt;/span&gt; fmla1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;No surprise here. Caret accepts parameter grids over which to search for hyperparameters. Here we set these up for our selected algorithms and combine them in a list along with additional model parameters expected by caret (such as a string identifying the type of model&amp;nbsp;etc):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;glmnetgrid &lt;span class="o"&gt;=&lt;/span&gt; expand.grid&lt;span class="p"&gt;(&lt;/span&gt;.alpha &lt;span class="o"&gt;=&lt;/span&gt; seq&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; .lambda &lt;span class="o"&gt;=&lt;/span&gt; seq&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="kc"&gt;...&lt;/span&gt;
rfgrid &lt;span class="o"&gt;=&lt;/span&gt; data.frame&lt;span class="p"&gt;(&lt;/span&gt;.mtry &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

configs &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;()&lt;/span&gt;
configs&lt;span class="o"&gt;$&lt;/span&gt;glmnet &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;(&lt;/span&gt;method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;glmnet&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; tuneGrid&lt;span class="o"&gt;=&lt;/span&gt;glmnetgrid&lt;span class="p"&gt;,&lt;/span&gt; preProcess&lt;span class="o"&gt;=&lt;/span&gt;pp&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kc"&gt;...&lt;/span&gt;
configs&lt;span class="o"&gt;$&lt;/span&gt;rf &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;(&lt;/span&gt;method&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;rf&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; tuneGrid&lt;span class="o"&gt;=&lt;/span&gt;rfgrid&lt;span class="p"&gt;,&lt;/span&gt; preProcess&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; ntree&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;2000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we have a list of training algorithms along with their required parameters, it&amp;#8217;s just a matter of looping over it to train the corresponding&amp;nbsp;classifiers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;arg &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;(&lt;/span&gt;form &lt;span class="o"&gt;=&lt;/span&gt; fmla&lt;span class="p"&gt;,&lt;/span&gt; data &lt;span class="o"&gt;=&lt;/span&gt; trainset&lt;span class="p"&gt;,&lt;/span&gt; trControl &lt;span class="o"&gt;=&lt;/span&gt; cvctrl&lt;span class="p"&gt;,&lt;/span&gt; metric &lt;span class="o"&gt;=&lt;/span&gt; scorer&lt;span class="p"&gt;)&lt;/span&gt;
models &lt;span class="o"&gt;=&lt;/span&gt; list&lt;span class="p"&gt;()&lt;/span&gt;
set.seed&lt;span class="p"&gt;(&lt;/span&gt;rseed&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="kr"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;i &lt;span class="kr"&gt;in&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;length&lt;span class="p"&gt;(&lt;/span&gt;configs&lt;span class="p"&gt;))&lt;/span&gt; 
&lt;span class="p"&gt;{&lt;/span&gt;
  models&lt;span class="p"&gt;[[&lt;/span&gt;i&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; do.call&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;train.formula&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; c&lt;span class="p"&gt;(&lt;/span&gt;arg&lt;span class="p"&gt;,&lt;/span&gt; configs&lt;span class="p"&gt;[[&lt;/span&gt;i&lt;span class="p"&gt;]]))&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
names&lt;span class="p"&gt;(&lt;/span&gt;models&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; sapply&lt;span class="p"&gt;(&lt;/span&gt;models&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kr"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;x&lt;span class="p"&gt;)&lt;/span&gt; x&lt;span class="o"&gt;$&lt;/span&gt;method&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let&amp;#8217;s look at some comparisons of the individual classifiers (Table&amp;nbsp;1):&lt;/p&gt;
&lt;figure &gt;
&lt;div class="figCenter"&gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt; &lt;TH&gt;  &lt;/TH&gt; &lt;TH&gt; glmnet &lt;/TH&gt; &lt;TH&gt; rf &lt;/TH&gt; &lt;TH&gt; gbm &lt;/TH&gt; &lt;TH&gt; ada &lt;/TH&gt; &lt;TH&gt; svmRadial &lt;/TH&gt; &lt;TH&gt; cforest &lt;/TH&gt; &lt;TH&gt; blackboost &lt;/TH&gt; &lt;TH&gt; earth &lt;/TH&gt; &lt;TH&gt; gamboost &lt;/TH&gt; &lt;TH&gt; bayesglm &lt;/TH&gt;  &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; train &lt;/TD&gt; &lt;TD align="right"&gt; 0.838 &lt;/TD&gt; &lt;TD align="right"&gt; 0.870 &lt;/TD&gt; &lt;TD align="right"&gt; 0.891 &lt;/TD&gt; &lt;TD align="right"&gt; 0.891 &lt;/TD&gt; &lt;TD align="right"&gt; 0.850 &lt;/TD&gt; &lt;TD align="right"&gt; 0.853 &lt;/TD&gt; &lt;TD align="right"&gt; 0.843 &lt;/TD&gt; &lt;TD align="right"&gt; 0.838 &lt;/TD&gt; &lt;TD align="right"&gt; 0.842 &lt;/TD&gt; &lt;TD align="right"&gt; 0.832 &lt;/TD&gt; &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; val &lt;/TD&gt; &lt;TD align="right"&gt; 0.808 &lt;/TD&gt; &lt;TD align="right"&gt; 0.797 &lt;/TD&gt; &lt;TD align="right"&gt; 0.853 &lt;/TD&gt; &lt;TD align="right"&gt; 0.825 &lt;/TD&gt; &lt;TD align="right"&gt; 0.825 &lt;/TD&gt; &lt;TD align="right"&gt; 0.808 &lt;/TD&gt; &lt;TD align="right"&gt; 0.802 &lt;/TD&gt; &lt;TD align="right"&gt; 0.797 &lt;/TD&gt; &lt;TD align="right"&gt; 0.785 &lt;/TD&gt; &lt;TD align="right"&gt; 0.808 &lt;/TD&gt; &lt;/TR&gt;
   &lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 1: Accuracy of individual classifiers on training and validation set.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The ada and gbm classifiers seems to do best in terms of accuracy, on both the training as well as the validation set, followed by the svm. However, since we have used the area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve as the optimized metric it might be more informative to drill down into how the classifiers perform in terms of &lt;span class="caps"&gt;ROC&lt;/span&gt;, specificity and&amp;nbsp;sensitivity. &lt;/p&gt;
&lt;figure&gt;
&lt;img src="/images/titanic/Roc.png" alt="Dot plot of ROC metrics for individual classifiers obtained from resamples created during cross-validation."/&gt;
&lt;figcaption  class="capCenter"&gt;Figure 4: Dot plot of &lt;span class="caps"&gt;ROC&lt;/span&gt; metrics for individual classifiers estimated from resampled data (10 repeats of 10-fold cross-validation). &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Figure 4 uses the resample results from cross-validation to display means and 95% confidence intervals for the shown metrics. We note that though gbm and ada had the best accuracy on the validation set, there are other models that seem to find a better trade-off between sensitivity and specificity, at least as estimated on the resampled data. More specifically, gbm, ada and svm show relatively high sensitivity, but low specificity. The generalized linear and additive models (glm, gam) seem to do better. Also, while the svm has high accuracy on the validation set and high sensitivity (recall) in cross-validation, i.e. is good at identifying the survivors, it performs worst amongst all classifiers in correctly identifying those who died&amp;nbsp;(specificity).&lt;/p&gt;
&lt;p&gt;Finally, let&amp;#8217;s create ensembles from the individual models and compare their &lt;span class="caps"&gt;ROC&lt;/span&gt; performance to the models on the validation set. Two ensembles are created with the help of Zach Mayer&amp;#8217;s &lt;a href="https://github.com/zachmayer/caretEnsemble"&gt;caretEnsemble&lt;/a&gt; package (itself based on a paper by &lt;a href="http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf"&gt;Caruana et al. 2004&lt;/a&gt;): the first employs a greedy forward selection of individual models to incrementally add those to the ensemble that minimize the ensemble&amp;#8217;s chosen error metric. The ensemble&amp;#8217;s predictions are then essentially a weighted average of the individual predictions. The second ensemble simply trains a new caret model of choice using the matrix of individual model predictions as features (in this case I use a generalized linear model), also known as a&amp;nbsp;&amp;#8220;stack&amp;#8221;.&lt;/p&gt;
&lt;figure&gt;
&lt;div style="display:table"&gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt; &lt;TH&gt;  &lt;/TH&gt; &lt;TH&gt; glmnet &lt;/TH&gt; &lt;TH&gt; gbm &lt;/TH&gt; &lt;TH&gt; LogitBoost &lt;/TH&gt; &lt;TH&gt; earth &lt;/TH&gt; &lt;TH&gt; blackboost &lt;/TH&gt; &lt;TH&gt; bayesglm &lt;/TH&gt; &lt;TH&gt; gamboost &lt;/TH&gt; &lt;TH&gt; svmRadial &lt;/TH&gt; &lt;TH&gt; ada &lt;/TH&gt;  &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; weight &lt;/TD&gt; &lt;TD align="right"&gt; 0.387 &lt;/TD&gt; &lt;TD align="right"&gt; 0.292 &lt;/TD&gt; &lt;TD align="right"&gt; 0.203 &lt;/TD&gt; &lt;TD align="right"&gt; 0.080 &lt;/TD&gt; &lt;TD align="right"&gt; 0.019 &lt;/TD&gt; &lt;TD align="right"&gt; 0.009 &lt;/TD&gt; &lt;TD align="right"&gt; 0.007 &lt;/TD&gt; &lt;TD align="right"&gt; 0.002 &lt;/TD&gt; &lt;TD align="right"&gt; 0.001 &lt;/TD&gt; &lt;/TR&gt;
   &lt;/TABLE&gt;

&lt;TABLE class="table"&gt;
&lt;TR&gt; &lt;TH&gt;  &lt;/TH&gt; &lt;TH&gt; earth &lt;/TH&gt; &lt;TH&gt; gamboost &lt;/TH&gt; &lt;TH&gt; blackboost &lt;/TH&gt; &lt;TH&gt; cforest &lt;/TH&gt; &lt;TH&gt; bayesglm &lt;/TH&gt; &lt;TH&gt; glmnet &lt;/TH&gt; &lt;TH&gt; svmRadial &lt;/TH&gt; &lt;TH&gt; rf &lt;/TH&gt; &lt;TH&gt; greedyEns &lt;/TH&gt; &lt;TH&gt; ada &lt;/TH&gt; &lt;TH&gt; gbm &lt;/TH&gt; &lt;TH&gt; linearEns &lt;/TH&gt;  &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.836 &lt;/TD&gt; &lt;TD align="right"&gt; 0.846 &lt;/TD&gt; &lt;TD align="right"&gt; 0.846 &lt;/TD&gt; &lt;TD align="right"&gt; 0.858 &lt;/TD&gt; &lt;TD align="right"&gt; 0.861 &lt;/TD&gt; &lt;TD align="right"&gt; 0.862 &lt;/TD&gt; &lt;TD align="right"&gt; 0.862 &lt;/TD&gt; &lt;TD align="right"&gt; 0.865 &lt;/TD&gt; &lt;TD align="right"&gt; 0.873 &lt;/TD&gt; &lt;TD align="right"&gt; 0.876 &lt;/TD&gt; &lt;TD align="right"&gt; 0.878 &lt;/TD&gt; &lt;TD align="right"&gt; 0.879 &lt;/TD&gt; &lt;/TR&gt;
   &lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 2: Top: classifier weights determined by the greedy ensemble. Bottom: &lt;span class="caps"&gt;ROC&lt;/span&gt; measured on validation set for individual and ensemble classifiers.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;On the unseen validation set we notice once again that ada and gbm perform best amongst the individual classifiers, not only in terms of accuracy as demonstrated above, but also in terms of the area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve. Both, however, are outperformed slightly by the stacked ensemble&amp;nbsp;(linearEns). &lt;/p&gt;
&lt;p&gt;Finally, let&amp;#8217;s compare the performances on the validation set to those obtained from cross-validated training on the whole training set. Table 3 summarises corresponding metrics for all&amp;nbsp;classifiers:&lt;/p&gt;
&lt;figure&gt;
&lt;div&gt;
&lt;TABLE class="table"&gt;
&lt;TR&gt; &lt;TH&gt;  &lt;/TH&gt; &lt;TH&gt; glmnet &lt;/TH&gt; &lt;TH&gt; rf &lt;/TH&gt; &lt;TH&gt; gbm &lt;/TH&gt; &lt;TH&gt; ada &lt;/TH&gt; &lt;TH&gt; svmRadial &lt;/TH&gt; &lt;TH&gt; cforest &lt;/TH&gt; &lt;TH&gt; blackboost &lt;/TH&gt; &lt;TH&gt; earth &lt;/TH&gt; &lt;TH&gt; gamboost &lt;/TH&gt; &lt;TH&gt; bayesglm &lt;/TH&gt; &lt;TH&gt; linearEns &lt;/TH&gt; &lt;TH&gt; greedyEns &lt;/TH&gt;  &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;/TD&gt; &lt;TD align="right"&gt; 0.871 &lt;/TD&gt; &lt;TD align="right"&gt; 0.875 &lt;/TD&gt; &lt;TD align="right"&gt; 0.877 &lt;/TD&gt; &lt;TD align="right"&gt; 0.875 &lt;/TD&gt; &lt;TD align="right"&gt; 0.864 &lt;/TD&gt; &lt;TD align="right"&gt; 0.871 &lt;/TD&gt; &lt;TD align="right"&gt; 0.866 &lt;/TD&gt; &lt;TD align="right"&gt; 0.869 &lt;/TD&gt; &lt;TD align="right"&gt; 0.873 &lt;/TD&gt; &lt;TD align="right"&gt; 0.870 &lt;/TD&gt; &lt;TD align="right"&gt; 0.880 &lt;/TD&gt; &lt;TD align="right"&gt; 0.878 &lt;/TD&gt; &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; Sens &lt;/TD&gt; &lt;TD align="right"&gt; 0.879 &lt;/TD&gt; &lt;TD align="right"&gt; 0.910 &lt;/TD&gt; &lt;TD align="right"&gt; 0.892 &lt;/TD&gt; &lt;TD align="right"&gt; 0.897 &lt;/TD&gt; &lt;TD align="right"&gt; 0.923 &lt;/TD&gt; &lt;TD align="right"&gt; 0.908 &lt;/TD&gt; &lt;TD align="right"&gt; 0.890 &lt;/TD&gt; &lt;TD align="right"&gt; 0.883 &lt;/TD&gt; &lt;TD align="right"&gt; 0.876 &lt;/TD&gt; &lt;TD align="right"&gt; 0.871 &lt;/TD&gt; &lt;TD align="right"&gt; 0.894 &lt;/TD&gt; &lt;TD align="right"&gt;  &lt;/TD&gt; &lt;/TR&gt;
  &lt;TR&gt; &lt;TD align="right"&gt; Spec &lt;/TD&gt; &lt;TD align="right"&gt; 0.750 &lt;/TD&gt; &lt;TD align="right"&gt; 0.699 &lt;/TD&gt; &lt;TD align="right"&gt; 0.743 &lt;/TD&gt; &lt;TD align="right"&gt; 0.739 &lt;/TD&gt; &lt;TD align="right"&gt; 0.678 &lt;/TD&gt; &lt;TD align="right"&gt; 0.701 &lt;/TD&gt; &lt;TD align="right"&gt; 0.723 &lt;/TD&gt; &lt;TD align="right"&gt; 0.721 &lt;/TD&gt; &lt;TD align="right"&gt; 0.740 &lt;/TD&gt; &lt;TD align="right"&gt; 0.752 &lt;/TD&gt; &lt;TD align="right"&gt; 0.733 &lt;/TD&gt; &lt;TD align="right"&gt;  &lt;/TD&gt; &lt;/TR&gt;
   &lt;/TABLE&gt;
&lt;/div&gt;
&lt;figcaption class="capCenter"&gt;Table 3: Area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve, sensitivity and specificity of all models estimated in 10 repeats of 10-fold cross-validation after training on the whole data set (sens and spec are not calculated automatically by the greedy ensemble) . &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The results seem to confirm our finding from predictions on the validation set. After training on the whole data set ada and gbm exhibit the best cross-validated &lt;span class="caps"&gt;ROC&lt;/span&gt; measures, but the ensemble classifiers do even&amp;nbsp;better.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;Based on an assessment of the area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve, on both a validation subset of the data as well as repeated cross-validation on the whole set, boosted classification trees (&lt;a href="http://dept.stat.lsa.umich.edu/~gmichail/ada_final.pdf"&gt;ada&lt;/a&gt; and &lt;a href="http://gradientboostedmodels.googlecode.com/git/gbm/inst/doc/gbm.pdf"&gt;gbm&lt;/a&gt;) seem to perform best amongst single classifiers on the titanic data set. Ensembles built using a range of different classifiers, in particular in the form of a stack, lead to a small but seemingly consistent improvement over the performance of individual classifiers. I therefore chose to submit the predictions of the generalized linear stack. Interestingly, this did not lead to my best submission score. The ensemble has an accuracy of 0.78947 on the public leaderboard, i.e. on the part of the test set used to score different submissions. In comparison, I&amp;#8217;ve also trained a single forest of conditional inference trees using the familyid information as an additional predictor, which obtained an accuracy score of 0.81818 and ended up much higher on the leaderboard. Now, kaggle leaderboard position in itself &lt;a href="http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/"&gt;doesn&amp;#8217;t always correlate well&lt;/a&gt; with final performance on the whole test set, essentially because of overfitting to the leaderboard if many submissions are made and models selected on the basis of achieved position. Nevertheless, before the end of the competition it might be worth comparing the above classifiers and ensembles with different formulas (combinations of predictors, including family identifiers). Another option is to perform the full training again with accuracy rather than &lt;span class="caps"&gt;AUC&lt;/span&gt; as the optimized metric, which is the one used to assess predictions by kaggle in this competition. However, as have commented many kagglers involved in past competitions, it is probably better to rely on one&amp;#8217;s own cross-validation scores, rather than potentially overinflated leaderboard scores, to predict a model&amp;#8217;s final&amp;nbsp;success.&lt;/p&gt;
&lt;script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt;</summary><category term="R"></category><category term="kaggle"></category><category term="titanic"></category><category term="report"></category><category term="classification"></category></entry><entry><title>Learning to perceive through equilibration</title><link href="https://buhrmann.github.io/piaget.html" rel="alternate"></link><updated>2014-07-01T00:00:00+02:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2014-07-01:piaget.html</id><summary type="html">&lt;p&gt;Our new paper on sensorimotor contingencies is out. It tackles what seems like a paradox in the sensorimotor approach: if understanding is required for perception, how can we learn to perceive something new, something we do not yet understand? We propose a Piagetian solution to this problem, according to which we learn to perceive by re-shaping pre-existing sensorimotor structures (the earliest of which are already present at birth) in coupling with dynamical regularities of the&amp;nbsp;world.&lt;/p&gt;
&lt;figure&gt;
&lt;a href="http://journal.frontiersin.org/Journal/10.3389/fnhum.2014.00551/full"&gt;&lt;img src="http://www.frontiersin.org/files/Articles/92421/fnhum-08-00551-HTML/image_m/fnhum-08-00551-g001.jpg" style="width: 400px"/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;Get it here: &lt;a href="http://journal.frontiersin.org/Journal/10.3389/fnhum.2014.00551/full"&gt;Frontiers in Cognition | Learning to perceive in the sensorimotor approach: Piaget’s theory of equilibration interpreted&amp;nbsp;dynamically&lt;/a&gt;&lt;/p&gt;</summary><category term="smcs"></category></entry><entry><title>A dynamical systems account of sensorimotor contingencies</title><link href="https://buhrmann.github.io/dynamical-smcs.html" rel="alternate"></link><updated>2013-05-31T13:25:00+02:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2013-05-31:dynamical-smcs.html</id><summary type="html">&lt;p&gt;We have published a new paper on sensorimotor contingencies (SMCs). It provides operational definitions for four different notions of SMCs that have not previously been distinguished. The paper illustrates these using a minimal cognition model and hypothesises about their link to personal-level concepts fundamental to the sensorimotor approach, such as the mastery of sensorimotor&amp;nbsp;skills.&lt;/p&gt;
&lt;figure&gt;
&lt;a href="http://journal.frontiersin.org/Journal/10.3389/fpsyg.2013.00285/full"&gt;&lt;img src="http://www.frontiersin.org/files/Articles/49706/fpsyg-04-00285-HTML/image_m/fpsyg-04-00285-g004.jpg" style="width: 400px"/&gt;&lt;/a&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a href="http://journal.frontiersin.org/Journal/10.3389/fpsyg.2013.00285/full"&gt;Frontiers in Cognition | A Dynamical Systems Account of Sensorimotor&amp;nbsp;Contingencies&lt;/a&gt;&lt;/p&gt;</summary><category term="smcs"></category></entry><entry><title>Robots perceive the world like humans</title><link href="https://buhrmann.github.io/science-daily.html" rel="alternate"></link><updated>2012-10-20T00:00:00+02:00</updated><author><name>Thomas Buhrmann</name></author><id>tag:buhrmann.github.io,2012-10-20:science-daily.html</id><summary type="html">&lt;p&gt;Science daily has covered our work on sensorimotor&amp;nbsp;contingencies:&lt;/p&gt;
&lt;blockquote&gt;Perceive first, act afterwards. The architecture of most of today’s robots is underpinned by this control strategy. The eSMCs project has set itself the aim of changing the paradigm and generating more dynamic computer models in which action is not a mere consequence of perception but an integral part of the perception process. It is about improving robot behavior by means of perception models closer to those of humans&amp;#8230;&lt;/blockquote&gt;

&lt;p&gt;Read the full article here: &lt;a href="http://www.sciencedaily.com/releases/2012/10/121018100131.htm"&gt;Science Daily: Robots that perceive the world like&amp;nbsp;humans&lt;/a&gt;&lt;/p&gt;</summary><category term="smcs"></category></entry></feed>